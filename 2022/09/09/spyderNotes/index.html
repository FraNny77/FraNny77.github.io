<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>spyderNotes | 是甜豆腐脑</title><meta name="keywords" content="爬虫"><meta name="author" content="是甜豆腐脑"><meta name="copyright" content="是甜豆腐脑"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="此篇博客记录爬虫基础知识即应用">
<meta property="og:type" content="article">
<meta property="og:title" content="spyderNotes">
<meta property="og:url" content="http://franny77.github.io/2022/09/09/spyderNotes/index.html">
<meta property="og:site_name" content="是甜豆腐脑">
<meta property="og:description" content="此篇博客记录爬虫基础知识即应用">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://cache.yisu.com/upload/admin/customer_case_img/2019-08-08/1565261709.jpg">
<meta property="article:published_time" content="2022-09-09T12:34:08.000Z">
<meta property="article:modified_time" content="2022-09-09T13:31:52.905Z">
<meta property="article:author" content="是甜豆腐脑">
<meta property="article:tag" content="爬虫">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://cache.yisu.com/upload/admin/customer_case_img/2019-08-08/1565261709.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://franny77.github.io/2022/09/09/spyderNotes/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: ture
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'spyderNotes',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-09-09 21:31:52'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 5.4.2"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/./img/tx.JPG" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">13</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">14</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 链接</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></li><li><a class="site-page child" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('http://cache.yisu.com/upload/admin/customer_case_img/2019-08-08/1565261709.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">是甜豆腐脑</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 链接</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></li><li><a class="site-page child" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">spyderNotes</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-09-09T12:34:08.000Z" title="发表于 2022-09-09 20:34:08">2022-09-09</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-09-09T13:31:52.905Z" title="更新于 2022-09-09 21:31:52">2022-09-09</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">4.3k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>18分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="spyderNotes"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1i54y1h75W?spm_id_from=333.1007.top_right_bar_window_custom_collection.content.click&vd_source=33f9e795549b35f78c7b08573b1fa296">参考文章</a></p>
<h1 id="第一章"><a href="#第一章" class="headerlink" title="第一章"></a>第一章</h1><h2 id="爬虫的基本原理"><a href="#爬虫的基本原理" class="headerlink" title="爬虫的基本原理"></a>爬虫的基本原理</h2><h4 id="1-1-http协议-amp-https协议"><a href="#1-1-http协议-amp-https协议" class="headerlink" title="1.1 http协议&amp;https协议"></a>1.1 http协议&amp;https协议</h4><p>http协议： 服务器与客户端进行数据交换的协议</p>
<ul>
<li>常用请求头信息：<br>User-Agent： 请求载体（浏览器）的身份标识<br>Connection： 请求完毕后，断开连接&#x2F;保持连接</li>
<li>常用响应头信息：<br>Cntent-Type： 服务器响应回客户端的数据类型<br>https协议： 安全的超文本传输协议（http协议）（s——security，进行了数据加密）</li>
<li>加密方式：<br>对称密钥加密： 同时传输密钥和密文<br>非对称密钥加密： 服务器传输密钥（密钥可能会被中间拦截，被恶意篡改），客户端返回相应密文<br>证书密钥加密： 非对称基础上，认证机构确认后给密钥签名（https采用）</li>
</ul>
<h4 id="1-2-URL和URI"><a href="#1-2-URL和URI" class="headerlink" title="1.2 URL和URI"></a>1.2 URL和URI</h4><p>URI (Uniform Resource Identifier) 即统一资源标志符<br>URL (Universal Resource Locator) 即统一资源定位符<br>URL是URI 的一个子集</p>
<p>也就是说每个URL都是URI，但不是每个URI都是URL<br>URI还包括一个子类叫作URN (Universal Resource Name) 即统一资源名称.</p>
<p><img src="https://img-blog.csdnimg.cn/ee92f85810b8484f81ac8519a5565d0f.png" alt="在这里插入图片描述"></p>
<h4 id="1-3-请求"><a href="#1-3-请求" class="headerlink" title="1.3 请求"></a>1.3 请求</h4><h5 id="1-3-1请求的方法"><a href="#1-3-1请求的方法" class="headerlink" title="1.3.1请求的方法"></a>1.3.1请求的方法</h5><p>常见的有: GET和POST</p>
<ul>
<li><p>GET请求：<br>在浏览器中直接输入URL并回车，便发起了一个GET请求，请求的参数会直接包含到URL里<br>例如:在百度中搜索Python, 这就是一个GET请求，链接为https :&#x2F;&#x2F;www. baidu. com&#x2F;s?wd&#x3D;Python<br>URL中包含了请求的参数信息，这里参数wd表示要搜寻的关键字</p>
</li>
<li><p>POST请求大多在表单提交时发起<br>例如:对于一个登录表单，输入用户名和密码后，点击“登录” 按钮这通常会发起一个 POST请求<br>其数据通常以表单的形式传输，而不会体现在URL中</p>
</li>
</ul>
<p>GET和POST请求方法有如下区别：</p>
<ul>
<li>GET请求中的参数包含在URL里面，数据可以在URL中看到</li>
<li>POST请求的URL不会包含这些数据，数据都是通过表单形式传输的，会包含在请求体中。</li>
<li>GET请求提交的数据最多只有1024 字节，而POST 请求没有限制</li>
</ul>
<h6 id="请求方法"><a href="#请求方法" class="headerlink" title="请求方法"></a>请求方法</h6><p><img src="https://img-blog.csdnimg.cn/07f668168669424eb60780a19c639676.png" alt="在这里插入图片描述"></p>
<h5 id="1-3-2-请求头"><a href="#1-3-2-请求头" class="headerlink" title="1.3.2 请求头"></a>1.3.2 请求头</h5><p>用来说明服务器要使用的附加信息，比较重要的信息有Cookie、 Referer、 User-Agent<br><img src="https://img-blog.csdnimg.cn/1024691a7b5a4e8282c8ffe0d93f5274.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/aef91e53fd3e4e708287cd41a4f8577c.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/2cc792d9b5b5438db5b0d3192c1ad0cc.png" alt="在这里插入图片描述"></p>
<h5 id="1-3-3-响应"><a href="#1-3-3-响应" class="headerlink" title="1.3.3 响应"></a>1.3.3 响应</h5><p>由服务端返回给客户端，可以分为三部分:</p>
<ol>
<li>响应状态码(Response Status Code)</li>
<li>响应头(Response Headers)</li>
<li>响应体(Response Body )</li>
</ol>
<p>1.响应状态码<br><img src="https://img-blog.csdnimg.cn/8aa9135f5b634d89ac0ee1458cf1f87f.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/7a52f22c37de40a292b26b9aa6692db5.png" alt="在这里插入图片描述"></p>
<p><img src="https://img-blog.csdnimg.cn/54e7a22526a24e988fa7efde1912b546.png" alt="在这里插入图片描述"></p>
<p><img src="https://img-blog.csdnimg.cn/00500c0e888f4696abb2a1f9bbf9a46d.png" alt="在这里插入图片描述"><br>2. 响应头包含了服务器对请求的应答信息，如Content -Type、Server、 Set -Cookie等<br>        简要说明一些常用的头信息:<br>        ●Date:  标识响应产生的时间<br>        ●Last-Modified:  指定资源的最后修改时间<br>        ●Content -Encoding: 指定响应内容的编码<br>        ●Server: 包含服务器的信息，比如名称、版本号等<br>        ●Content-Type:  文档类型，指定返回的数据类型是什么，如text&#x2F;html 代表返回HTML文档；appl icat ion&#x2F;x- javascr ipt则代表返回JavaScript 文件，image&#x2F; jpeg则代表返回图片。<br>        ●Set-Cookie:  设置Cook ies。 响应头中的Set-Cookie 告诉浏览器需要将此内容放在Cookies 中下次请求携带Cookies请求。<br>        ●Expires: 指定响应的过期时间，可以使代理服务器或浏览器将加载的内容更新到缓存中。<br>        如果再次访问时，就可以直接从缓存中加载，降低服务器负载，缩短加载时间<br>3. 响应体：响应的正文数据都在响应体中<br>比如:<br>请求网页时，它的响应体就是网页的HTML代码<br>请求一张图片时，它的响应体就是图片的二进制数据</p>
<h1 id="第二章"><a href="#第二章" class="headerlink" title="第二章"></a>第二章</h1><h2 id="Requests-模块"><a href="#Requests-模块" class="headerlink" title="Requests 模块"></a>Requests 模块</h2><h3 id="2-1-requests的主要方法"><a href="#2-1-requests的主要方法" class="headerlink" title="2.1 requests的主要方法"></a>2.1 requests的主要方法</h3><p>根据网页实际请求方式选择对应方法<br><img src="https://img-blog.csdnimg.cn/88ed7a7139914777b9be87bd3057a662.png" alt="在这里插入图片描述"></p>
<h3 id="2-2-get-方法"><a href="#2-2-get-方法" class="headerlink" title="2.2 get()方法"></a>2.2 get()方法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">requests.get(url,params,headers)</span><br></pre></td></tr></table></figure>

<p><img src="https://img-blog.csdnimg.cn/db5d9075f644446a9ffa02d05c88713a.png" alt="在这里插入图片描述"></p>
<h5 id="2-2-2-Response对象属性"><a href="#2-2-2-Response对象属性" class="headerlink" title="2.2.2 Response对象属性"></a>2.2.2 Response对象属性</h5><p><img src="https://img-blog.csdnimg.cn/2998c7067d904d5aafde9568c3286aa9.png" alt="在这里插入图片描述"></p>
<h4 id="实战编码"><a href="#实战编码" class="headerlink" title="实战编码"></a>实战编码</h4><h4 id="①爬取搜狗首页的页面数据"><a href="#①爬取搜狗首页的页面数据" class="headerlink" title="①爬取搜狗首页的页面数据"></a>①爬取搜狗首页的页面数据</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line">url = <span class="string">&quot;https://www.sogou.com/&quot;</span></span><br><span class="line">response = requests.get(url)</span><br><span class="line">page_text = response.text</span><br><span class="line">x = BeautifulSoup(page_text,<span class="string">&quot;lxml&quot;</span>)  <span class="comment"># 使用 lxml 解析器作为底层解析引擎</span></span><br><span class="line"><span class="built_in">print</span>(x.prettify()) <span class="comment"># 当爬取的页面内容只按一行显示，使用prettify()变成规整的 html 格式</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;./sogou.html&#x27;</span>, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">        fp.write(page_text)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;数据爬取结束&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h4 id="②网页采集器"><a href="#②网页采集器" class="headerlink" title="②网页采集器"></a>②网页采集器</h4><ul>
<li>UA: User-Agent (请求载体的身份标识)</li>
<li>UA检测: 门户网站的服务器会检测对应请求的载体身份标识，如果检测到的载体身份标识唯一正常浏览器说明该请求是一个正常的请求。但是，如果检测到请求的载体身份标识不是基于某一浏览器的，则表示该请求为不正常的请求（爬虫），则服务器端就很有可能拒绝该次请求</li>
<li>UA伪装: 让爬虫对应的请求载体身份标识伪装成某一浏览器</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">url = <span class="string">&#x27;https://www.sogou.com/web&#x27;</span></span><br><span class="line">header = &#123;<span class="string">&quot;User-Agent&quot;</span>:<span class="string">&quot;Mozilla/5.0&quot;</span>&#125;</span><br><span class="line">kw = <span class="built_in">input</span>(<span class="string">&quot;请输入关键字：&quot;</span>)</span><br><span class="line"><span class="comment"># 处理url携带参数：封装到字典中</span></span><br><span class="line">param = &#123;</span><br><span class="line">        <span class="string">&#x27;query&#x27;</span>: kw</span><br><span class="line">    &#125;</span><br><span class="line">response = requests.get(url,params=param,headers=header)</span><br><span class="line">page_text = response.text</span><br><span class="line">filename = kw + <span class="string">&#x27;.html&#x27;</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(filename, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">     fp.write(page_text)</span><br><span class="line"><span class="built_in">print</span>(filename, <span class="string">&#x27;保存成功&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h4 id="③破解百度翻译"><a href="#③破解百度翻译" class="headerlink" title="③破解百度翻译"></a>③破解百度翻译</h4><p>ajax请求在xhr中捕获</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line">post_url = <span class="string">&quot;https://fanyi.baidu.com/sug&quot;</span></span><br><span class="line"></span><br><span class="line">kw = <span class="built_in">input</span>()</span><br><span class="line">data = &#123;</span><br><span class="line"><span class="string">&quot;kw&quot;</span>:kw</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">header = &#123;</span><br><span class="line"><span class="string">&quot;User-Agent&quot;</span>:<span class="string">&quot;Mozilla/5.0&quot;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">response = requests.post(post_url,data=data,headers=header)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取响应数据,json方法返回的是obj（对象）（如果确认响应数据是json类型的，才可以使用json()）</span></span><br><span class="line"><span class="comment"># response.text</span></span><br><span class="line">dic_obj = response.json()</span><br><span class="line"><span class="built_in">print</span>(dic_obj)</span><br><span class="line"><span class="comment"># 进行持久化存储</span></span><br><span class="line">fileName = word + <span class="string">&#x27;.json&#x27;</span></span><br><span class="line">fp = <span class="built_in">open</span>(fileName, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">json.dump(dic_obj,fp=fp, ensure_ascii=<span class="literal">False</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;over&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h4 id="④豆瓣电影爬取"><a href="#④豆瓣电影爬取" class="headerlink" title="④豆瓣电影爬取"></a>④豆瓣电影爬取</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line">url = <span class="string">&quot;https://movie.douban.com/j/chart/top_list&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">type</span> = <span class="built_in">input</span>(<span class="string">&#x27;(纪录片——1；传记——2；犯罪——3；历史——4；动作——5；情色——6；歌舞——7；\n&#x27;</span></span><br><span class="line">                 <span class="string">&#x27;儿童——8；悬疑——10；剧情——11；灾难——12；爱情——13；音乐——14；\n&#x27;</span></span><br><span class="line">                 <span class="string">&#x27;冒险——15；奇幻——16；科幻——17；运动——18；惊悚——19；恐怖——20；\n&#x27;</span></span><br><span class="line">                 <span class="string">&#x27;战争——22；短片——23；喜剧——24；动画——25；同性——26；西部——27；\n&#x27;</span></span><br><span class="line">                 <span class="string">&#x27;家庭——28；武侠——29；古装——30；黑色电影——31)\n&#x27;</span></span><br><span class="line">                 <span class="string">&#x27;（PS:缺少9，21可能是用于反爬，防止循环数据时进行爬取)\n&#x27;</span></span><br><span class="line">                 <span class="string">&#x27;enter a type:&#x27;</span>)</span><br><span class="line">                 </span><br><span class="line">start = <span class="built_in">input</span>(<span class="string">&#x27;enter start(first one corresponds with number 0):&#x27;</span>)</span><br><span class="line">limit = <span class="built_in">input</span>(<span class="string">&#x27;enter the amount you want:&#x27;</span>)</span><br><span class="line">param = &#123;</span><br><span class="line">    <span class="string">&#x27;type&#x27;</span>: <span class="built_in">type</span>,  <span class="comment"># 可以进行修改，喜剧：24, 动作片:5 爱情片:13 etc.</span></span><br><span class="line">    <span class="string">&#x27;interval_id&#x27;</span>: <span class="string">&#x27;100:90&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;action&#x27;</span>: <span class="string">&#x27;&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;start&#x27;</span>: start,  <span class="comment"># 从库中的第几部电影去取   0对应第一部电影</span></span><br><span class="line">    <span class="string">&#x27;limit&#x27;</span>: limit,  <span class="comment"># 一次取出的个数</span></span><br><span class="line">&#125;</span><br><span class="line">header = &#123;</span><br><span class="line">	<span class="string">&quot;User-Agent&quot;</span>:<span class="string">&quot;Mozila/5.0&quot;</span></span><br><span class="line">&#125;</span><br><span class="line">response = requests.get(url,params=param,headers =header)</span><br><span class="line">list_data = response.json()</span><br><span class="line">fp = <span class="built_in">open</span>(<span class="string">&#x27;./douban.json&#x27;</span>, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">json.dump(list_data, fp=fp, ensure_ascii=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;over&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h1 id="第三章"><a href="#第三章" class="headerlink" title="第三章"></a>第三章</h1><h2 id="数据解析"><a href="#数据解析" class="headerlink" title="数据解析"></a>数据解析</h2><h5 id="编码流程："><a href="#编码流程：" class="headerlink" title="编码流程："></a>编码流程：</h5><ol>
<li>指定url</li>
<li>发起请求</li>
<li>获取响应数据</li>
<li>数据解析</li>
<li>持久化存储</li>
</ol>
<h5 id="数据解析分类："><a href="#数据解析分类：" class="headerlink" title="数据解析分类："></a>数据解析分类：</h5><ul>
<li>正则</li>
<li>bs4</li>
<li>xpath（重点，因为通用性强）</li>
</ul>
<h5 id="数据解析原理概述："><a href="#数据解析原理概述：" class="headerlink" title="数据解析原理概述："></a>数据解析原理概述：</h5><p>解析的局部文本内容都会在标签之间或标签对应的属性中进行存储</p>
<ol>
<li>进行指定标签的定位</li>
<li>标签或者标签对应的属性中存储的数据值进行提取（解析）</li>
</ol>
<h4 id="3-1-正则表达式"><a href="#3-1-正则表达式" class="headerlink" title="3.1 正则表达式"></a>3.1 正则表达式</h4><p>Regular Expression,正则表达式，一种使用表达式的方式对字符串进行匹配的语法规则. </p>
<p>我们抓取到的网页源代码本质上就是一个超长的字符串，想从里面提取内容.用正则再合适不过了.</p>
<p>正则的优点:速度快，效率高，准确性高<br>正则的语法:使用元字符进行排列组合用来匹配字符串<br>在线测试正则表达式<a target="_blank" rel="noopener" href="https://tool.oschina.net/regex/">https://tool.oschina.net/regex/</a></p>
<p>元字符:具有固定含义的特殊符号</p>
<h5 id="常用元字符"><a href="#常用元字符" class="headerlink" title="常用元字符"></a>常用元字符</h5><p><img src="https://img-blog.csdnimg.cn/4bbd6213ca244465bfcad354a15dcc52.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/1273e80b5a934f358c6a30a9e5b7e2d4.png" alt="在这里插入图片描述"></p>
<h5 id="经典正则表达式"><a href="#经典正则表达式" class="headerlink" title="经典正则表达式"></a>经典正则表达式</h5><p><img src="https://img-blog.csdnimg.cn/9d918e2331e24fb2864696b60672d952.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/e89c251141534d6a98ccfcac179ba28d.png" alt="在这里插入图片描述"></p>
<h5 id="贪婪匹配和惰性匹配"><a href="#贪婪匹配和惰性匹配" class="headerlink" title="贪婪匹配和惰性匹配"></a>贪婪匹配和惰性匹配</h5><ol>
<li>.*  贪婪匹配</li>
<li>.*?  惰性匹配（？控制 * 尽可能少地匹配）运用了回溯</li>
</ol>
<h5 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h5><p><img src="https://img-blog.csdnimg.cn/9b2ebd5625824195b2719854aee24a47.png" alt="在这里插入图片描述"></p>
<h4 id="3-2-Re库"><a href="#3-2-Re库" class="headerlink" title="3.2 Re库"></a>3.2 Re库</h4><p><strong>raw string类型</strong>(原生字符串类型): 不包含对转义字符再次转义的字符串<br>re库采用raw string类型表示正则表达式,表示为:  r’text’</p>
<h5 id="3-2-1-Match-对象"><a href="#3-2-1-Match-对象" class="headerlink" title="3.2.1 Match 对象"></a>3.2.1 Match 对象</h5><p>Match对象是一次匹配的结果，包含匹配的很多信息<br><strong>属性：</strong><br><img src="https://img-blog.csdnimg.cn/f2ae14e0550e441496f940a31edeafec.png" alt="在这里插入图片描述"><br><strong>方法：</strong><br><img src="https://img-blog.csdnimg.cn/deed85f73af84afb9fbf33327fa430f5.png" alt="在这里插入图片描述"></p>
<h5 id="3-2-2-Re库主要功能函数"><a href="#3-2-2-Re库主要功能函数" class="headerlink" title="3.2.2 Re库主要功能函数"></a>3.2.2 Re库主要功能函数</h5><p><img src="https://img-blog.csdnimg.cn/45ed7952e3d340059ab9769b86dc0f6b.png" alt="在这里插入图片描述"><br><strong>参数：</strong><br><img src="https://img-blog.csdnimg.cn/6bce5abd0e7c4339aab153c9128771ae.png" alt="在这里插入图片描述"></p>
<p><img src="https://img-blog.csdnimg.cn/f88bc2a939b34e95936481c3466a5996.png" alt="在这里插入图片描述"><br><strong>例：</strong></p>
<p>1. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">for</span> m <span class="keyword">in</span> re.finditer(<span class="string">r&#x27;[1-9]\d&#123;5&#125;&#x27;</span>,<span class="string">&#x27;BIT100081 TSU100084&#x27;</span>)</span><br><span class="line">	<span class="keyword">if</span> m:</span><br><span class="line">		<span class="built_in">print</span>(m.group())</span><br></pre></td></tr></table></figure>
<p>2.<br><img src="https://img-blog.csdnimg.cn/24f7956010644f5ead16d3d1388ecbab.png" alt="在这里插入图片描述"></p>
<p><img src="https://img-blog.csdnimg.cn/4a471a962c3e4130a78145c4905f0bdd.png" alt="在这里插入图片描述"></p>
<h5 id="3-2-3-预加载正则表达式"><a href="#3-2-3-预加载正则表达式" class="headerlink" title="3.2.3 预加载正则表达式"></a>3.2.3 预加载正则表达式</h5><p><img src="https://img-blog.csdnimg.cn/678edc3060104a0aa6542626b914ffc2.png" alt="在这里插入图片描述"></p>
<h5 id="3-2-4-内容获取"><a href="#3-2-4-内容获取" class="headerlink" title="3.2.4 内容获取"></a>3.2.4 内容获取</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">s = <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">	&lt;div class= &#x27;jay&#x27;&gt;&lt;span id= &#x27;1&#x27;&gt;郭麒麟&lt;/span&gt;&lt;/div&gt;</span></span><br><span class="line"><span class="string">	&lt;div class=&#x27;jj&#x27;&gt;&lt;span id= &#x27;2&#x27;&gt;宋铁&lt;/span&gt;&lt;/div&gt;</span></span><br><span class="line"><span class="string">	&lt;div class=&#x27;jolin&#x27;&gt;&lt;span id=&#x27;3 &#x27;&gt;大聪明&lt;/span&gt;&lt;/div&gt;</span></span><br><span class="line"><span class="string">	&lt;div class= &#x27; sylar&#x27;&gt;&lt;span id= &#x27; 4‘&gt;范思哲&lt;/span&gt;&lt;/div&gt;</span></span><br><span class="line"><span class="string">	&lt;div class=&#x27;tory &#x27;&gt;&lt;span id=&#x27; 5”&gt;胡说八道&lt;/span&gt;&lt;/div&gt;</span></span><br><span class="line"><span class="string">	&#x27;&#x27;&#x27;</span></span><br><span class="line">reg = re.<span class="built_in">compile</span>(<span class="string">r&quot;&lt;div class=&#x27;.*?&#x27;&gt;&lt;span id= &#x27;(?P&lt;id&gt;.*?)&#x27;&gt;(?P&lt;name&gt;.*?)&lt;/span&gt;&lt;/div&gt;&quot;</span>,flag=<span class="string">&quot;re.S&quot;</span>) </span><br><span class="line"><span class="comment"># re.S 使得 . 可以匹配换行符</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># (?P&lt;分组名字&gt;正则)  可以单独从正则匹配的内容中提取到XX名字的内容</span></span><br><span class="line"><span class="keyword">for</span> it <span class="keyword">in</span> reg.finditer(s):</span><br><span class="line">	<span class="built_in">print</span>(it.group(<span class="string">&quot;name&quot;</span>)) <span class="comment"># 可以获取名字的内容</span></span><br><span class="line">	<span class="built_in">print</span>(it.group(<span class="string">&quot;id&quot;</span>))</span><br><span class="line">	</span><br></pre></td></tr></table></figure>

<h4 id="3-3-案例"><a href="#3-3-案例" class="headerlink" title="3.3 案例"></a>3.3 案例</h4><h5 id="①爬取豆瓣top250电影排行"><a href="#①爬取豆瓣top250电影排行" class="headerlink" title="①爬取豆瓣top250电影排行"></a>①爬取豆瓣top250电影排行</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"></span><br><span class="line">url = <span class="string">&quot;https://movie.douban.com/top250&quot;</span></span><br><span class="line">headers = &#123;</span><br><span class="line">	<span class="string">&quot;User-Agent&quot;</span>:<span class="string">&quot;Mozilla/5.0&quot;</span></span><br><span class="line">&#125;</span><br><span class="line">response = requests.get(url,headers=headers)</span><br><span class="line">page_content = response.text</span><br><span class="line">reg = re.<span class="built_in">compile</span>(<span class="string">r&#x27;&lt;li&gt;.*?&lt;div class=&quot;item&quot;&gt;.*?&lt;span class=&quot;title&quot;&gt;(?P&lt;name&gt;.*?)&lt;/span&gt;.*?&lt;br&gt;(?P&lt;year&gt;.*?)&amp;nbsp.*?&lt;span class=&quot;rating_num&quot; property=&quot;v:average&quot;&gt;(?P&lt;score&gt;.*?)&lt;/span&gt;.*?&lt;span&gt;(?P&lt;num&gt;.*?)&lt;/span&gt;&#x27;</span>,re.S)</span><br><span class="line">f = <span class="built_in">open</span>(<span class="string">&quot;data.csv&quot;</span>,<span class="string">&quot;w&quot;</span>)</span><br><span class="line">csvwriter = csv.writer(f)</span><br><span class="line"><span class="keyword">for</span> it <span class="keyword">in</span> reg.finditer(page_content):</span><br><span class="line">	<span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">	print(it.group(&quot;name&quot;))</span></span><br><span class="line"><span class="string">	print(it.group(&quot;year&quot;).strip())</span></span><br><span class="line"><span class="string">	print(it.group(&quot;score&quot;))</span></span><br><span class="line"><span class="string">	print(it.group(&quot;num&quot;))</span></span><br><span class="line"><span class="string">	&#x27;&#x27;&#x27;</span></span><br><span class="line">	dic = it.groupdict()</span><br><span class="line">	dic[<span class="string">&#x27;year&#x27;</span>] = dic[<span class="string">&#x27;year&#x27;</span>].strip()</span><br><span class="line">	csvwriter.writerow(dic.values())</span><br><span class="line">f.close()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;over&quot;</span>)</span><br></pre></td></tr></table></figure>
<h5 id="②爬取电影天堂"><a href="#②爬取电影天堂" class="headerlink" title="②爬取电影天堂"></a>②爬取电影天堂</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"></span><br><span class="line">domain = <span class="string">&quot;https://dytt89.com&quot;</span></span><br><span class="line">headers = &#123;</span><br><span class="line">	<span class="string">&quot;User-Agent&quot;</span>:<span class="string">&quot;Mozilla/5.0&quot;</span></span><br><span class="line">&#125;</span><br><span class="line">response = requests.get(domain,verify=<span class="literal">False</span>) <span class="comment"># verify =False 去除安全验证</span></span><br><span class="line">response.encoding = response.apparent_encoding <span class="comment"># 防止乱码</span></span><br><span class="line">reg1 = re.<span class="built_in">compile</span>(<span class="string">r&#x27;2022必看热片.*?&lt;ul&gt;(?P&lt;ul&gt;.*?)&lt;/ul&gt;&#x27;</span>,re.S)</span><br><span class="line">res1 = reg1.finditer(response.text)</span><br><span class="line"></span><br><span class="line">hrefs = []</span><br><span class="line"><span class="keyword">for</span> it <span class="keyword">in</span> res1:</span><br><span class="line">	ul = it.group(<span class="string">&#x27;ul&#x27;</span>)</span><br><span class="line">	reg2 = re.<span class="built_in">compile</span>(<span class="string">r&#x27;&lt;a href=(?P&lt;href&gt;.*?)&#x27;</span>,re.S)</span><br><span class="line">	res2 = reg2.finditer(ul)</span><br><span class="line">	<span class="keyword">for</span> itt <span class="keyword">in</span> res:</span><br><span class="line">		hrefs.append(itt.group(<span class="string">&#x27;href&#x27;</span>))</span><br><span class="line">links = []</span><br><span class="line"><span class="keyword">for</span> href <span class="keyword">in</span> hrefs:</span><br><span class="line">	url = domain+href</span><br><span class="line">	resp = requests.get(url,verify=<span class="literal">False</span>)</span><br><span class="line">	resp.encoding = resp.apparent_encoding</span><br><span class="line">	reg3 = re.<span class="built_in">compile</span>(<span class="string">r&#x27;◎片　　名(?P&lt;name&gt;.*?)&lt;br /&gt;&lt;td style=&quot;WORD-WRAP: break-word&quot; bgcolor=&quot;#fdfddf&quot;&gt;&lt;a href=&quot;(?P&lt;link&gt;.*?)&quot;&gt;&#x27;</span>,re.S)</span><br><span class="line">	res3 = reg3.finditer(resp)</span><br><span class="line">	<span class="keyword">for</span> it <span class="keyword">in</span> res3:</span><br><span class="line">		links.append([it.group(<span class="string">&#x27;name&#x27;</span>),it.group(<span class="string">&#x27;link&#x27;</span>)])</span><br></pre></td></tr></table></figure>
<h4 id="3-4-Bs4解析基础"><a href="#3-4-Bs4解析基础" class="headerlink" title="3.4 Bs4解析基础"></a>3.4 Bs4解析基础</h4><h5 id="3-4-1数据解析的原理："><a href="#3-4-1数据解析的原理：" class="headerlink" title="3.4.1数据解析的原理："></a>3.4.1数据解析的原理：</h5><ol>
<li>标签定位</li>
<li>提取标签、标签属性中存储的数据值</li>
</ol>
<h5 id="3-4-2bs4数据解析的原理："><a href="#3-4-2bs4数据解析的原理：" class="headerlink" title="3.4.2bs4数据解析的原理："></a>3.4.2bs4数据解析的原理：</h5><ol>
<li>实例化一个BeautifulSoup对象，并且将页面源码数据及加载到该对象中</li>
<li>通过调用BeautifulSoup对象中相关的属性或者方法进行签定位和数据解析</li>
</ol>
<h5 id="3-4-3-如何实例化BeautifulSoup对象："><a href="#3-4-3-如何实例化BeautifulSoup对象：" class="headerlink" title="3.4.3 如何实例化BeautifulSoup对象："></a>3.4.3 如何实例化BeautifulSoup对象：</h5><p>from bs4 import BeautifulSoup<br>对象的实例化：</p>
<ol>
<li>将本地的html文档中的数据加载到该对象中<br>fp &#x3D; open(‘.&#x2F;test.html’, ‘r’, encoding&#x3D;’utf-8’)<br>soup &#x3D; BeautifulSoup(fp, ‘lxml’)</li>
<li>将互联网上获取的页面源码加载到对象中<br>page_text &#x3D; requests.text<br>soup &#x3D; BeautifulSoup(page_text, ‘lxml’)</li>
</ol>
<h5 id="3-4-4-提供的用于数据解析的方法和属性："><a href="#3-4-4-提供的用于数据解析的方法和属性：" class="headerlink" title="3.4.4 提供的用于数据解析的方法和属性："></a>3.4.4 提供的用于数据解析的方法和属性：</h5><p>soup.tagName : 返回的是文档中第一次出现的 tagName 对应的标签<br>soup.find():</p>
<ul>
<li>find(‘tagName’):  等同于soup.div</li>
<li>属性定位：<br>soup.find(‘div’, class_&#x2F;id&#x2F;attr&#x3D;’song’)<br>soup.find_all(‘tagName’): 返回符合要求的所有标签（列表）</li>
</ul>
<h5 id="3-4-5-select"><a href="#3-4-5-select" class="headerlink" title="3.4.5 select:"></a>3.4.5 select:</h5><p>select(‘某种选择器（id，class，标签。。。选择器）’): 返回的是一个列表</p>
<p>层级选择器：</p>
<ul>
<li>soup.select(‘.tang &gt; ul &gt; li &gt; a’)：&gt;表示的是一个层级</li>
<li>soup.select(‘.tang &gt; ul a’)：空格表示的是多个层级</li>
</ul>
<h5 id="3-4-6-获取标签之间的文本数据："><a href="#3-4-6-获取标签之间的文本数据：" class="headerlink" title="3.4.6 获取标签之间的文本数据："></a>3.4.6 获取标签之间的文本数据：</h5><p>soup.a.text&#x2F;string&#x2F;get_text()   : 两个属性一个方法</p>
<ul>
<li>text&#x2F;get_text(): 可以获取某一个标签中所有的文本内容（不属于直系的也可以）</li>
<li>string：只可以获取该标签下面直系的文本内容</li>
</ul>
<h5 id="3-4-7-获取标签中属性值："><a href="#3-4-7-获取标签中属性值：" class="headerlink" title="3.4.7 获取标签中属性值："></a>3.4.7 获取标签中属性值：</h5><p>soup.a[‘href’]</p>
<p>各种用法的例子代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="comment"># 将本地的html文档中的数据加载到该对象中</span></span><br><span class="line">fp = <span class="built_in">open</span>(<span class="string">&#x27;./test.html&#x27;</span>, <span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"><span class="comment"># 第一个参数为文件路径，第二个为固定的lxml解析器</span></span><br><span class="line">soup = BeautifulSoup(fp, <span class="string">&#x27;lxml&#x27;</span>)</span><br><span class="line"><span class="comment"># print(soup)</span></span><br><span class="line"><span class="built_in">print</span>(soup.a)  <span class="comment"># soup.tagName 返回的是html中第一次出现的tagName标签</span></span><br><span class="line"><span class="built_in">print</span>(soup.div)</span><br><span class="line"><span class="comment"># find(&#x27;tagName&#x27;):等同于soup.div</span></span><br><span class="line"><span class="built_in">print</span>(soup.find(<span class="string">&#x27;div&#x27;</span>))  <span class="comment"># print(soup.div)</span></span><br><span class="line"><span class="built_in">print</span>(soup.find(<span class="string">&#x27;div&#x27;</span>, class_=<span class="string">&#x27;song&#x27;</span>))  <span class="comment"># 注意&#x27;class&#x27;为关键字，加上下划线&#x27;class_&#x27;为参数名称</span></span><br><span class="line"><span class="built_in">print</span>(soup.find_all(<span class="string">&#x27;a&#x27;</span>))  <span class="comment"># 返回是一个列表类型</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(soup.select(<span class="string">&#x27;.tang&#x27;</span>))  <span class="comment"># &#x27;.&#x27;表示class  select返回列表</span></span><br><span class="line"> <span class="comment"># 1.层级选择器&#x27;&gt;&#x27;表示一个层级  2.bs4支持属性定位，不支持索引定位 3.返回的是列表</span></span><br><span class="line"><span class="built_in">print</span>(soup.select(<span class="string">&#x27;.tang &gt; ul &gt; li &gt; a&#x27;</span>)[<span class="number">0</span>]) </span><br><span class="line"><span class="built_in">print</span>(soup.select(<span class="string">&#x27;.tang &gt; ul &gt; li &gt; a&#x27;</span>)[<span class="number">0</span>].text)</span><br><span class="line"><span class="built_in">print</span>(soup.select(<span class="string">&#x27;.tang &gt; ul &gt; li &gt; a&#x27;</span>)[<span class="number">0</span>].string)</span><br><span class="line"><span class="built_in">print</span>(soup.select(<span class="string">&#x27;.tang &gt; ul &gt; li &gt; a&#x27;</span>)[<span class="number">0</span>].get_text())</span><br><span class="line"><span class="built_in">print</span>(soup.select(<span class="string">&#x27;.tang &gt; ul &gt; li &gt; a&#x27;</span>)[<span class="number">0</span>][<span class="string">&#x27;href&#x27;</span>])</span><br></pre></td></tr></table></figure>
<p>bs4解析案例实战 1</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line">f = <span class="built_in">open</span>(<span class="string">&quot;菜价.csv&quot;</span>,<span class="string">&quot;w&quot;</span>)</span><br><span class="line">csvwriter = csv.writer(f)</span><br><span class="line"></span><br><span class="line">url = <span class="string">&quot;https://www.xinfadi.com.cn/marketanalysis/0/list/1.shtml&quot;</span></span><br><span class="line">response = requests.get(url)</span><br><span class="line">page = BeautifulSoup(response.text,<span class="string">&quot;lxml&quot;</span>)</span><br><span class="line"></span><br><span class="line">table = page.find(<span class="string">&quot;table&quot;</span>,class_=<span class="string">&quot;hq_table&quot;</span>)</span><br><span class="line"></span><br><span class="line">trs = table.find_all(<span class="string">&quot;tr&quot;</span>)[<span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> tr <span class="keyword">in</span> trs:</span><br><span class="line">	tds = tr.find_all(<span class="string">&quot;td&quot;</span>)</span><br><span class="line">	name = tds[<span class="number">0</span>].text</span><br><span class="line">	low = tds[<span class="number">1</span>].text</span><br><span class="line">	avg = tds[<span class="number">2</span>].text</span><br><span class="line">	high = tds[<span class="number">3</span>].text</span><br><span class="line">	guige = tds[<span class="number">4</span>].text</span><br><span class="line">	dangwei = tds[<span class="number">5</span>].text</span><br><span class="line">	date = tds[<span class="number">6</span>].text</span><br><span class="line">	csvwriter.writerow([name,low,avg,high,guige,dangwei,date])</span><br><span class="line">f.close()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;over!&quot;</span>)</span><br></pre></td></tr></table></figure>

<h5 id="3-4-8-标签树的遍历"><a href="#3-4-8-标签树的遍历" class="headerlink" title="3.4.8 标签树的遍历"></a>3.4.8 标签树的遍历</h5><p><img src="https://img-blog.csdnimg.cn/e2b5b63c110c4559805cf42d50716f3e.png" alt="在这里插入图片描述"></p>
<h6 id="3-4-8-1-标签树的下行遍历"><a href="#3-4-8-1-标签树的下行遍历" class="headerlink" title="3.4.8.1 标签树的下行遍历"></a>3.4.8.1 标签树的下行遍历</h6><p><img src="https://img-blog.csdnimg.cn/c76d9bcea2944195a7e9d7f4c4b412c4.png" alt="在这里插入图片描述"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> lxml</span><br><span class="line"></span><br><span class="line">url = <span class="string">&quot;https://www.baidu.com&quot;</span></span><br><span class="line">response = requests.get(url)</span><br><span class="line">soup = BeautifuSoup(response.text,<span class="string">&quot;lxml&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> child <span class="keyword">in</span> soup.body.children: <span class="comment"># 遍历儿子节点</span></span><br><span class="line">	<span class="built_in">print</span>(child)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> child <span class="keyword">in</span> soup.body.descendants: <span class="comment">#遍历子孙节点</span></span><br><span class="line">	<span class="built_in">print</span>(child)</span><br><span class="line">	</span><br></pre></td></tr></table></figure>

<h6 id="3-4-8-2-标签树的上行遍历"><a href="#3-4-8-2-标签树的上行遍历" class="headerlink" title="3.4.8.2 标签树的上行遍历"></a>3.4.8.2 标签树的上行遍历</h6><p><img src="https://img-blog.csdnimg.cn/5c0a0e6f3154436c8ab892b8a1b7535e.png" alt="在这里插入图片描述"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> lxml</span><br><span class="line"></span><br><span class="line">url = <span class="string">&quot;https://www.baidu.com&quot;</span></span><br><span class="line">response = requests.get(url)</span><br><span class="line">soup = BeautifuSoup(response.text,<span class="string">&quot;lxml&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> parent <span class="keyword">in</span> soup.a.parents:</span><br><span class="line">	<span class="keyword">if</span> parent <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">		<span class="built_in">print</span>(parent)</span><br><span class="line">	<span class="keyword">else</span>:</span><br><span class="line">		<span class="built_in">print</span>(parent.name)</span><br></pre></td></tr></table></figure>
<p>3.4.8.2 标签树的平行遍历<br><img src="https://img-blog.csdnimg.cn/52ac91afdc5c4979872a064143f43a1c.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/b5277fe8adfc4106995c72f604d55d00.png" alt="在这里插入图片描述"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> lxml</span><br><span class="line"></span><br><span class="line">url = <span class="string">&quot;https://www.baidu.com&quot;</span></span><br><span class="line">response = requests.get(url)</span><br><span class="line">soup = BeautifuSoup(response.text,<span class="string">&quot;lxml&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> sibling <span class="keyword">in</span> soup.a.next_sibling: <span class="comment">#遍历后续节点</span></span><br><span class="line">	<span class="built_in">print</span>(sibling)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> sibling <span class="keyword">in</span> soup.a.previous_sibling: <span class="comment"># 遍历前续节点</span></span><br><span class="line">	<span class="built_in">print</span>(sibling)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>bs4解析案例实战 2</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 爬取优美图库</span></span><br><span class="line"><span class="comment"># 1.拿到主页面的源代码。然后提取到子页面的链接地址，href </span></span><br><span class="line"><span class="comment"># 2.通过href拿到子页面的内容。从子页面中找到图片的下载地址img -&gt; src</span></span><br><span class="line"><span class="comment"># 3.下载图片</span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> lxml</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line">url = <span class="string">&quot;https://umei.cc&quot;</span></span><br><span class="line">headers = &#123;</span><br><span class="line">	<span class="string">&quot;User-Agent&quot;</span>:<span class="string">&quot;Mozilla/5.0&quot;</span></span><br><span class="line">&#125;</span><br><span class="line">response = requests.get(url,headers=headers)</span><br><span class="line">response.encoding = response.apparent_encoding</span><br><span class="line"></span><br><span class="line">soup = BeautifulSoup(response.text,<span class="string">&quot;lxml&quot;</span>)</span><br><span class="line">alist = soup.find(<span class="string">&quot;div&quot;</span>,class_=<span class="string">&quot;TypeList&quot;</span>).find_all(<span class="string">&#x27;a&#x27;</span>)</span><br><span class="line"></span><br><span class="line">links = []</span><br><span class="line"><span class="keyword">for</span> a <span class="keyword">in</span> alist:</span><br><span class="line">	href = a.get(<span class="string">&#x27;href&#x27;</span>)</span><br><span class="line">	res2 = requests.get(href,headers=headers)</span><br><span class="line">	res2.encoding = res2.apparent_encoding</span><br><span class="line">	soup2 = BeautifulSoup(res2.text,<span class="string">&quot;lxml&quot;</span>)</span><br><span class="line">	img = soup2.find(<span class="string">&quot;p&quot;</span>,align=<span class="string">&quot;center&quot;</span>).find(<span class="string">&quot;img&quot;</span>)</span><br><span class="line">	src = img.get(<span class="string">&quot;src&quot;</span>)</span><br><span class="line">	<span class="comment"># 下载图片</span></span><br><span class="line">	img_resp = requests.get(src)</span><br><span class="line">	img_name = src.split(<span class="string">&quot;/&quot;</span>)[-<span class="number">1</span>]</span><br><span class="line">	<span class="keyword">with</span> <span class="built_in">open</span>(img_name,<span class="string">&quot;wb&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">		f.write(img_resp.content) <span class="comment"># 返回字节</span></span><br><span class="line">	<span class="built_in">print</span>(img_name,<span class="string">&quot;over&quot;</span>)</span><br><span class="line">	time.sleep(<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;all_over&quot;</span>)	</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="3-5-xpath解析基础"><a href="#3-5-xpath解析基础" class="headerlink" title="3.5 xpath解析基础"></a>3.5 xpath解析基础</h4><p><strong>xpath解析</strong>：最常用且最便捷高效的一种解析方式，同时也是通用性最强的一种方式。</p>
<p><strong>xpath解析原理</strong>：</p>
<ol>
<li>实例化一个etree的对象，且需要将被解析的页面源码数据加载到该对象中。</li>
<li>调用etree对象中的xpath方法结合着xpath表达式实现标签的定位和内容的捕获。</li>
</ol>
<p><strong>如何实例化一个etree对象</strong>：<br>from lxml import etree</p>
<ol>
<li><p>将本地的html文档中的源码数据加载到etree对象中：<br>etree.parse(filePath)</p>
</li>
<li><p>可以将从互联网上获取的源码数据加载到该对象中:<br>etree.HTML(‘page_text’)</p>
</li>
<li><p>xpath(‘xpath表达式’):<br> - &#x2F;：表示的是从根节点开始定位。表示的是一个层级<br> - &#x2F;&#x2F;：表示的是多个层级；可以表示从任意位置开始定位。<br> - 属性定位：&#x2F;&#x2F;div[@class&#x3D;”song”] 语法：tag[@attrName&#x3D;”attrValue”]<br> - 索引定位：&#x2F;&#x2F;div[@class&#x3D;”song”]&#x2F;p[3] 索引是从1开始的<br> - 取文本：<br>     &#x2F;text() 获取的是标签中的直系文本内容<br>     &#x2F;&#x2F;text() 获取标签中非直系文本的内容（所有的文本内容）<br> - 取属性：<br> &#x2F;attrName &#x3D;&#x3D;&gt;img&#x2F;src</p>
</li>
</ol>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="http://FraNny77.github.io">是甜豆腐脑</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://franny77.github.io/2022/09/09/spyderNotes/">http://franny77.github.io/2022/09/09/spyderNotes/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://FraNny77.github.io" target="_blank">是甜豆腐脑</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E7%88%AC%E8%99%AB/">爬虫</a></div><div class="post_share"><div class="social-share" data-image="http://cache.yisu.com/upload/admin/customer_case_img/2019-08-08/1565261709.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2022/07/13/Regression/"><img class="next-cover" src="http://imgoss.cnu.cc/2204/27/a4383d0f926b346eb1fe8ff128e5b703.jpg?x-oss-process=style/content" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Regression</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/./img/tx.JPG" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">是甜豆腐脑</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">13</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">14</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/FraNny77.io"><i class="fab fa-github"></i><span>Gituhub</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/FraNny77" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:2556725169@qq.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="https://blog.csdn.net/FraNny13" target="_blank" title="Blog"><i class="fab fa-algolia"></i></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E4%B8%80%E7%AB%A0"><span class="toc-text">第一章</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%88%AC%E8%99%AB%E7%9A%84%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86"><span class="toc-text">爬虫的基本原理</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-1-http%E5%8D%8F%E8%AE%AE-amp-https%E5%8D%8F%E8%AE%AE"><span class="toc-text">1.1 http协议&amp;https协议</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-2-URL%E5%92%8CURI"><span class="toc-text">1.2 URL和URI</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-3-%E8%AF%B7%E6%B1%82"><span class="toc-text">1.3 请求</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1-3-1%E8%AF%B7%E6%B1%82%E7%9A%84%E6%96%B9%E6%B3%95"><span class="toc-text">1.3.1请求的方法</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#%E8%AF%B7%E6%B1%82%E6%96%B9%E6%B3%95"><span class="toc-text">请求方法</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#1-3-2-%E8%AF%B7%E6%B1%82%E5%A4%B4"><span class="toc-text">1.3.2 请求头</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#1-3-3-%E5%93%8D%E5%BA%94"><span class="toc-text">1.3.3 响应</span></a></li></ol></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E4%BA%8C%E7%AB%A0"><span class="toc-text">第二章</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Requests-%E6%A8%A1%E5%9D%97"><span class="toc-text">Requests 模块</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-requests%E7%9A%84%E4%B8%BB%E8%A6%81%E6%96%B9%E6%B3%95"><span class="toc-text">2.1 requests的主要方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-get-%E6%96%B9%E6%B3%95"><span class="toc-text">2.2 get()方法</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#2-2-2-Response%E5%AF%B9%E8%B1%A1%E5%B1%9E%E6%80%A7"><span class="toc-text">2.2.2 Response对象属性</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9E%E6%88%98%E7%BC%96%E7%A0%81"><span class="toc-text">实战编码</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E2%91%A0%E7%88%AC%E5%8F%96%E6%90%9C%E7%8B%97%E9%A6%96%E9%A1%B5%E7%9A%84%E9%A1%B5%E9%9D%A2%E6%95%B0%E6%8D%AE"><span class="toc-text">①爬取搜狗首页的页面数据</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E2%91%A1%E7%BD%91%E9%A1%B5%E9%87%87%E9%9B%86%E5%99%A8"><span class="toc-text">②网页采集器</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E2%91%A2%E7%A0%B4%E8%A7%A3%E7%99%BE%E5%BA%A6%E7%BF%BB%E8%AF%91"><span class="toc-text">③破解百度翻译</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E2%91%A3%E8%B1%86%E7%93%A3%E7%94%B5%E5%BD%B1%E7%88%AC%E5%8F%96"><span class="toc-text">④豆瓣电影爬取</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E4%B8%89%E7%AB%A0"><span class="toc-text">第三章</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E8%A7%A3%E6%9E%90"><span class="toc-text">数据解析</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%BC%96%E7%A0%81%E6%B5%81%E7%A8%8B%EF%BC%9A"><span class="toc-text">编码流程：</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E8%A7%A3%E6%9E%90%E5%88%86%E7%B1%BB%EF%BC%9A"><span class="toc-text">数据解析分类：</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E8%A7%A3%E6%9E%90%E5%8E%9F%E7%90%86%E6%A6%82%E8%BF%B0%EF%BC%9A"><span class="toc-text">数据解析原理概述：</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F"><span class="toc-text">3.1 正则表达式</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%B8%B8%E7%94%A8%E5%85%83%E5%AD%97%E7%AC%A6"><span class="toc-text">常用元字符</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%BB%8F%E5%85%B8%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F"><span class="toc-text">经典正则表达式</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%B4%AA%E5%A9%AA%E5%8C%B9%E9%85%8D%E5%92%8C%E6%83%B0%E6%80%A7%E5%8C%B9%E9%85%8D"><span class="toc-text">贪婪匹配和惰性匹配</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%A1%88%E4%BE%8B"><span class="toc-text">案例</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-Re%E5%BA%93"><span class="toc-text">3.2 Re库</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#3-2-1-Match-%E5%AF%B9%E8%B1%A1"><span class="toc-text">3.2.1 Match 对象</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-2-2-Re%E5%BA%93%E4%B8%BB%E8%A6%81%E5%8A%9F%E8%83%BD%E5%87%BD%E6%95%B0"><span class="toc-text">3.2.2 Re库主要功能函数</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-2-3-%E9%A2%84%E5%8A%A0%E8%BD%BD%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F"><span class="toc-text">3.2.3 预加载正则表达式</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-2-4-%E5%86%85%E5%AE%B9%E8%8E%B7%E5%8F%96"><span class="toc-text">3.2.4 内容获取</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-3-%E6%A1%88%E4%BE%8B"><span class="toc-text">3.3 案例</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E2%91%A0%E7%88%AC%E5%8F%96%E8%B1%86%E7%93%A3top250%E7%94%B5%E5%BD%B1%E6%8E%92%E8%A1%8C"><span class="toc-text">①爬取豆瓣top250电影排行</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E2%91%A1%E7%88%AC%E5%8F%96%E7%94%B5%E5%BD%B1%E5%A4%A9%E5%A0%82"><span class="toc-text">②爬取电影天堂</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-4-Bs4%E8%A7%A3%E6%9E%90%E5%9F%BA%E7%A1%80"><span class="toc-text">3.4 Bs4解析基础</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#3-4-1%E6%95%B0%E6%8D%AE%E8%A7%A3%E6%9E%90%E7%9A%84%E5%8E%9F%E7%90%86%EF%BC%9A"><span class="toc-text">3.4.1数据解析的原理：</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-4-2bs4%E6%95%B0%E6%8D%AE%E8%A7%A3%E6%9E%90%E7%9A%84%E5%8E%9F%E7%90%86%EF%BC%9A"><span class="toc-text">3.4.2bs4数据解析的原理：</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-4-3-%E5%A6%82%E4%BD%95%E5%AE%9E%E4%BE%8B%E5%8C%96BeautifulSoup%E5%AF%B9%E8%B1%A1%EF%BC%9A"><span class="toc-text">3.4.3 如何实例化BeautifulSoup对象：</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-4-4-%E6%8F%90%E4%BE%9B%E7%9A%84%E7%94%A8%E4%BA%8E%E6%95%B0%E6%8D%AE%E8%A7%A3%E6%9E%90%E7%9A%84%E6%96%B9%E6%B3%95%E5%92%8C%E5%B1%9E%E6%80%A7%EF%BC%9A"><span class="toc-text">3.4.4 提供的用于数据解析的方法和属性：</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-4-5-select"><span class="toc-text">3.4.5 select:</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-4-6-%E8%8E%B7%E5%8F%96%E6%A0%87%E7%AD%BE%E4%B9%8B%E9%97%B4%E7%9A%84%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%EF%BC%9A"><span class="toc-text">3.4.6 获取标签之间的文本数据：</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-4-7-%E8%8E%B7%E5%8F%96%E6%A0%87%E7%AD%BE%E4%B8%AD%E5%B1%9E%E6%80%A7%E5%80%BC%EF%BC%9A"><span class="toc-text">3.4.7 获取标签中属性值：</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-4-8-%E6%A0%87%E7%AD%BE%E6%A0%91%E7%9A%84%E9%81%8D%E5%8E%86"><span class="toc-text">3.4.8 标签树的遍历</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#3-4-8-1-%E6%A0%87%E7%AD%BE%E6%A0%91%E7%9A%84%E4%B8%8B%E8%A1%8C%E9%81%8D%E5%8E%86"><span class="toc-text">3.4.8.1 标签树的下行遍历</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#3-4-8-2-%E6%A0%87%E7%AD%BE%E6%A0%91%E7%9A%84%E4%B8%8A%E8%A1%8C%E9%81%8D%E5%8E%86"><span class="toc-text">3.4.8.2 标签树的上行遍历</span></a></li></ol></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-5-xpath%E8%A7%A3%E6%9E%90%E5%9F%BA%E7%A1%80"><span class="toc-text">3.5 xpath解析基础</span></a></li></ol></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2022/09/09/spyderNotes/" title="spyderNotes"><img src="http://cache.yisu.com/upload/admin/customer_case_img/2019-08-08/1565261709.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="spyderNotes"/></a><div class="content"><a class="title" href="/2022/09/09/spyderNotes/" title="spyderNotes">spyderNotes</a><time datetime="2022-09-09T12:34:08.000Z" title="发表于 2022-09-09 20:34:08">2022-09-09</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/07/13/Regression/" title="Regression"><img src="http://imgoss.cnu.cc/2204/27/a4383d0f926b346eb1fe8ff128e5b703.jpg?x-oss-process=style/content" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Regression"/></a><div class="content"><a class="title" href="/2022/07/13/Regression/" title="Regression">Regression</a><time datetime="2022-07-13T07:21:44.000Z" title="发表于 2022-07-13 15:21:44">2022-07-13</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2022/06/19/DataScience/" title="DataScience">DataScience</a><time datetime="2022-06-19T12:32:04.000Z" title="发表于 2022-06-19 20:32:04">2022-06-19</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2022/06/09/managerment/" title="managerment">managerment</a><time datetime="2022-06-09T12:05:42.000Z" title="发表于 2022-06-09 20:05:42">2022-06-09</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/06/09/JavaNotes/" title="JavaNotes"><img src="https://tse2-mm.cn.bing.net/th/id/OIP-C.WaCOgSUgMm-RNN1PhMBPWgHaEK?pid=ImgDet&amp;rs=1" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="JavaNotes"/></a><div class="content"><a class="title" href="/2022/06/09/JavaNotes/" title="JavaNotes">JavaNotes</a><time datetime="2022-06-09T06:22:52.000Z" title="发表于 2022-06-09 14:22:52">2022-06-09</time></div></div></div></div></div></div></main><footer id="footer" style="background: #000000"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2022 By 是甜豆腐脑</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">试着和曾经仰望的人并肩</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>