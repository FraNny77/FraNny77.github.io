<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>spyderNotes | 是甜豆腐脑</title><meta name="keywords" content="爬虫"><meta name="author" content="是甜豆腐脑"><meta name="copyright" content="是甜豆腐脑"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="此篇博客记录爬虫基础知识即应用">
<meta property="og:type" content="article">
<meta property="og:title" content="spyderNotes">
<meta property="og:url" content="http://franny77.github.io/2022/09/09/spiderNotes/index.html">
<meta property="og:site_name" content="是甜豆腐脑">
<meta property="og:description" content="此篇博客记录爬虫基础知识即应用">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://cache.yisu.com/upload/admin/customer_case_img/2019-08-08/1565261709.jpg">
<meta property="article:published_time" content="2022-09-09T12:34:08.000Z">
<meta property="article:modified_time" content="2022-09-20T14:28:51.209Z">
<meta property="article:author" content="是甜豆腐脑">
<meta property="article:tag" content="爬虫">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://cache.yisu.com/upload/admin/customer_case_img/2019-08-08/1565261709.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://franny77.github.io/2022/09/09/spiderNotes/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: ture
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'spyderNotes',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-09-20 22:28:51'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 5.4.2"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/./img/tx.JPG" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">19</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">18</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 链接</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></li><li><a class="site-page child" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('http://cache.yisu.com/upload/admin/customer_case_img/2019-08-08/1565261709.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">是甜豆腐脑</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 链接</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></li><li><a class="site-page child" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">spyderNotes</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-09-09T12:34:08.000Z" title="发表于 2022-09-09 20:34:08">2022-09-09</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-09-20T14:28:51.209Z" title="更新于 2022-09-20 22:28:51">2022-09-20</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">17.9k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>79分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="spyderNotes"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1i54y1h75W?spm_id_from=333.1007.top_right_bar_window_custom_collection.content.click&vd_source=33f9e795549b35f78c7b08573b1fa296">参考文章</a></p>
<h1 id="第一章"><a href="#第一章" class="headerlink" title="第一章"></a>第一章</h1><h2 id="爬虫的基本原理"><a href="#爬虫的基本原理" class="headerlink" title="爬虫的基本原理"></a>爬虫的基本原理</h2><h4 id="1-1-http协议-amp-https协议"><a href="#1-1-http协议-amp-https协议" class="headerlink" title="1.1 http协议&amp;https协议"></a>1.1 http协议&amp;https协议</h4><p>http协议： 服务器与客户端进行数据交换的协议</p>
<ul>
<li>常用请求头信息：<br>User-Agent： 请求载体（浏览器）的身份标识<br>Connection： 请求完毕后，断开连接&#x2F;保持连接</li>
<li>常用响应头信息：<br>Cntent-Type： 服务器响应回客户端的数据类型<br>https协议： 安全的超文本传输协议（http协议）（s——security，进行了数据加密）</li>
<li>加密方式：<br>对称密钥加密： 同时传输密钥和密文<br>非对称密钥加密： 服务器传输密钥（密钥可能会被中间拦截，被恶意篡改），客户端返回相应密文<br>证书密钥加密： 非对称基础上，认证机构确认后给密钥签名（https采用）</li>
</ul>
<h4 id="1-2-URL和URI"><a href="#1-2-URL和URI" class="headerlink" title="1.2 URL和URI"></a>1.2 URL和URI</h4><p>URI (Uniform Resource Identifier) 即统一资源标志符<br>URL (Universal Resource Locator) 即统一资源定位符<br>URL是URI 的一个子集</p>
<p>也就是说每个URL都是URI，但不是每个URI都是URL<br>URI还包括一个子类叫作URN (Universal Resource Name) 即统一资源名称.</p>
<p><img src="https://img-blog.csdnimg.cn/ee92f85810b8484f81ac8519a5565d0f.png" alt="在这里插入图片描述"></p>
<h4 id="1-3-请求"><a href="#1-3-请求" class="headerlink" title="1.3 请求"></a>1.3 请求</h4><h5 id="1-3-1请求的方法"><a href="#1-3-1请求的方法" class="headerlink" title="1.3.1请求的方法"></a>1.3.1请求的方法</h5><p>常见的有: GET和POST</p>
<ul>
<li><p>GET请求：<br>在浏览器中直接输入URL并回车，便发起了一个GET请求，请求的参数会直接包含到URL里<br>例如:在百度中搜索Python, 这就是一个GET请求，链接为https :&#x2F;&#x2F;www. baidu. com&#x2F;s?wd&#x3D;Python<br>URL中包含了请求的参数信息，这里参数wd表示要搜寻的关键字</p>
</li>
<li><p>POST请求大多在表单提交时发起<br>例如:对于一个登录表单，输入用户名和密码后，点击“登录” 按钮这通常会发起一个 POST请求<br>其数据通常以表单的形式传输，而不会体现在URL中</p>
</li>
</ul>
<p>GET和POST请求方法有如下区别：</p>
<ul>
<li>GET请求中的参数包含在URL里面，数据可以在URL中看到</li>
<li>POST请求的URL不会包含这些数据，数据都是通过表单形式传输的，会包含在请求体中。</li>
<li>GET请求提交的数据最多只有1024 字节，而POST 请求没有限制</li>
</ul>
<h6 id="请求方法"><a href="#请求方法" class="headerlink" title="请求方法"></a>请求方法</h6><p><img src="https://img-blog.csdnimg.cn/07f668168669424eb60780a19c639676.png" alt="在这里插入图片描述"></p>
<h5 id="1-3-2-请求头"><a href="#1-3-2-请求头" class="headerlink" title="1.3.2 请求头"></a>1.3.2 请求头</h5><p>用来说明服务器要使用的附加信息，比较重要的信息有Cookie、 Referer、 User-Agent<br><img src="https://img-blog.csdnimg.cn/1024691a7b5a4e8282c8ffe0d93f5274.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/aef91e53fd3e4e708287cd41a4f8577c.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/2cc792d9b5b5438db5b0d3192c1ad0cc.png" alt="在这里插入图片描述"></p>
<h5 id="1-3-3-响应"><a href="#1-3-3-响应" class="headerlink" title="1.3.3 响应"></a>1.3.3 响应</h5><p>由服务端返回给客户端，可以分为三部分:</p>
<ol>
<li>响应状态码(Response Status Code)</li>
<li>响应头(Response Headers)</li>
<li>响应体(Response Body )</li>
</ol>
<p>1.响应状态码<br><img src="https://img-blog.csdnimg.cn/8aa9135f5b634d89ac0ee1458cf1f87f.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/7a52f22c37de40a292b26b9aa6692db5.png" alt="在这里插入图片描述"></p>
<p><img src="https://img-blog.csdnimg.cn/54e7a22526a24e988fa7efde1912b546.png" alt="在这里插入图片描述"></p>
<p><img src="https://img-blog.csdnimg.cn/00500c0e888f4696abb2a1f9bbf9a46d.png" alt="在这里插入图片描述"><br>2. 响应头包含了服务器对请求的应答信息，如Content -Type、Server、 Set -Cookie等<br>        简要说明一些常用的头信息:<br>        ●Date:  标识响应产生的时间<br>        ●Last-Modified:  指定资源的最后修改时间<br>        ●Content -Encoding: 指定响应内容的编码<br>        ●Server: 包含服务器的信息，比如名称、版本号等<br>        ●Content-Type:  文档类型，指定返回的数据类型是什么，如text&#x2F;html 代表返回HTML文档；application&#x2F;x- javascript则代表返回JavaScript 文件，image&#x2F; jpeg则代表返回图片。<br>        ●Set-Cookie:  设置Cook ies。 响应头中的Set-Cookie 告诉浏览器需要将此内容放在Cookies 中下次请求携带Cookies请求。<br>        ●Expires: 指定响应的过期时间，可以使代理服务器或浏览器将加载的内容更新到缓存中。<br>        如果再次访问时，就可以直接从缓存中加载，降低服务器负载，缩短加载时间<br>3. 响应体：响应的正文数据都在响应体中<br>比如:<br>请求网页时，它的响应体就是网页的HTML代码<br>请求一张图片时，它的响应体就是图片的二进制数据</p>
<h1 id="第二章"><a href="#第二章" class="headerlink" title="第二章"></a>第二章</h1><h2 id="Requests-模块"><a href="#Requests-模块" class="headerlink" title="Requests 模块"></a>Requests 模块</h2><h3 id="2-1-requests的主要方法"><a href="#2-1-requests的主要方法" class="headerlink" title="2.1 requests的主要方法"></a>2.1 requests的主要方法</h3><p>根据网页实际请求方式选择对应方法<br><img src="https://img-blog.csdnimg.cn/88ed7a7139914777b9be87bd3057a662.png" alt="在这里插入图片描述"></p>
<h3 id="2-2-get-方法"><a href="#2-2-get-方法" class="headerlink" title="2.2 get()方法"></a>2.2 get()方法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">requests.get(url,params,headers)</span><br></pre></td></tr></table></figure>

<p><img src="https://img-blog.csdnimg.cn/db5d9075f644446a9ffa02d05c88713a.png" alt="在这里插入图片描述"></p>
<h5 id="2-2-2-Response对象属性"><a href="#2-2-2-Response对象属性" class="headerlink" title="2.2.2 Response对象属性"></a>2.2.2 Response对象属性</h5><p><img src="https://img-blog.csdnimg.cn/2998c7067d904d5aafde9568c3286aa9.png" alt="在这里插入图片描述"></p>
<h4 id="实战编码"><a href="#实战编码" class="headerlink" title="实战编码"></a>实战编码</h4><h4 id="①爬取搜狗首页的页面数据"><a href="#①爬取搜狗首页的页面数据" class="headerlink" title="①爬取搜狗首页的页面数据"></a>①爬取搜狗首页的页面数据</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line">url = <span class="string">&quot;https://www.sogou.com/&quot;</span></span><br><span class="line">response = requests.get(url)</span><br><span class="line">page_text = response.text</span><br><span class="line">x = BeautifulSoup(page_text,<span class="string">&quot;lxml&quot;</span>)  <span class="comment"># 使用 lxml 解析器作为底层解析引擎</span></span><br><span class="line"><span class="built_in">print</span>(x.prettify()) <span class="comment"># 当爬取的页面内容只按一行显示，使用prettify()变成规整的 html 格式</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;./sogou.html&#x27;</span>, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">	fp.write(page_text)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;数据爬取结束&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h4 id="②网页采集器"><a href="#②网页采集器" class="headerlink" title="②网页采集器"></a>②网页采集器</h4><ul>
<li>UA: User-Agent (请求载体的身份标识)</li>
<li>UA检测: 门户网站的服务器会检测对应请求的载体身份标识，如果检测到的载体身份标识唯一正常浏览器说明该请求是一个正常的请求。但是，如果检测到请求的载体身份标识不是基于某一浏览器的，则表示该请求为不正常的请求（爬虫），则服务器端就很有可能拒绝该次请求</li>
<li>UA伪装: 让爬虫对应的请求载体身份标识伪装成某一浏览器</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">url = <span class="string">&#x27;https://www.sogou.com/web&#x27;</span></span><br><span class="line">header = &#123;<span class="string">&quot;User-Agent&quot;</span>:<span class="string">&quot;Mozilla/5.0&quot;</span>&#125;</span><br><span class="line">kw = <span class="built_in">input</span>(<span class="string">&quot;请输入关键字：&quot;</span>)</span><br><span class="line"><span class="comment"># 处理url携带参数：封装到字典中</span></span><br><span class="line">param = &#123;</span><br><span class="line">        <span class="string">&#x27;query&#x27;</span>: kw</span><br><span class="line">    &#125;</span><br><span class="line">response = requests.get(url,params=param,headers=header)</span><br><span class="line">page_text = response.text</span><br><span class="line">filename = kw + <span class="string">&#x27;.html&#x27;</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(filename, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">     fp.write(page_text)</span><br><span class="line"><span class="built_in">print</span>(filename, <span class="string">&#x27;保存成功&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h4 id="③破解百度翻译"><a href="#③破解百度翻译" class="headerlink" title="③破解百度翻译"></a>③破解百度翻译</h4><p>ajax请求在xhr中捕获</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line">post_url = <span class="string">&quot;https://fanyi.baidu.com/sug&quot;</span></span><br><span class="line"></span><br><span class="line">kw = <span class="built_in">input</span>()</span><br><span class="line">data = &#123;</span><br><span class="line"><span class="string">&quot;kw&quot;</span>:kw</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">header = &#123;</span><br><span class="line"><span class="string">&quot;User-Agent&quot;</span>:<span class="string">&quot;Mozilla/5.0&quot;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">response = requests.post(post_url,data=data,headers=header)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取响应数据,json方法返回的是obj（对象）（如果确认响应数据是json类型的，才可以使用json()）</span></span><br><span class="line"><span class="comment"># response.text</span></span><br><span class="line">dic_obj = response.json()</span><br><span class="line"><span class="built_in">print</span>(dic_obj)</span><br><span class="line"><span class="comment"># 进行持久化存储</span></span><br><span class="line">fileName = word + <span class="string">&#x27;.json&#x27;</span></span><br><span class="line">fp = <span class="built_in">open</span>(fileName, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">json.dump(dic_obj,fp=fp, ensure_ascii=<span class="literal">False</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;over&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h4 id="④豆瓣电影爬取"><a href="#④豆瓣电影爬取" class="headerlink" title="④豆瓣电影爬取"></a>④豆瓣电影爬取</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line">url = <span class="string">&quot;https://movie.douban.com/j/chart/top_list&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">type</span> = <span class="built_in">input</span>(<span class="string">&#x27;(纪录片——1；传记——2；犯罪——3；历史——4；动作——5；情色——6；歌舞——7；\n&#x27;</span></span><br><span class="line">                 <span class="string">&#x27;儿童——8；悬疑——10；剧情——11；灾难——12；爱情——13；音乐——14；\n&#x27;</span></span><br><span class="line">                 <span class="string">&#x27;冒险——15；奇幻——16；科幻——17；运动——18；惊悚——19；恐怖——20；\n&#x27;</span></span><br><span class="line">                 <span class="string">&#x27;战争——22；短片——23；喜剧——24；动画——25；同性——26；西部——27；\n&#x27;</span></span><br><span class="line">                 <span class="string">&#x27;家庭——28；武侠——29；古装——30；黑色电影——31)\n&#x27;</span></span><br><span class="line">                 <span class="string">&#x27;（PS:缺少9，21可能是用于反爬，防止循环数据时进行爬取)\n&#x27;</span></span><br><span class="line">                 <span class="string">&#x27;enter a type:&#x27;</span>)</span><br><span class="line">                 </span><br><span class="line">start = <span class="built_in">input</span>(<span class="string">&#x27;enter start(first one corresponds with number 0):&#x27;</span>)</span><br><span class="line">limit = <span class="built_in">input</span>(<span class="string">&#x27;enter the amount you want:&#x27;</span>)</span><br><span class="line">param = &#123;</span><br><span class="line">    <span class="string">&#x27;type&#x27;</span>: <span class="built_in">type</span>,  <span class="comment"># 可以进行修改，喜剧：24, 动作片:5 爱情片:13 etc.</span></span><br><span class="line">    <span class="string">&#x27;interval_id&#x27;</span>: <span class="string">&#x27;100:90&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;action&#x27;</span>: <span class="string">&#x27;&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;start&#x27;</span>: start,  <span class="comment"># 从库中的第几部电影去取   0对应第一部电影</span></span><br><span class="line">    <span class="string">&#x27;limit&#x27;</span>: limit,  <span class="comment"># 一次取出的个数</span></span><br><span class="line">&#125;</span><br><span class="line">header = &#123;</span><br><span class="line">	<span class="string">&quot;User-Agent&quot;</span>:<span class="string">&quot;Mozila/5.0&quot;</span></span><br><span class="line">&#125;</span><br><span class="line">response = requests.get(url,params=param,headers =header)</span><br><span class="line">list_data = response.json()</span><br><span class="line">fp = <span class="built_in">open</span>(<span class="string">&#x27;./douban.json&#x27;</span>, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">json.dump(list_data, fp=fp, ensure_ascii=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;over&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h1 id="第三章"><a href="#第三章" class="headerlink" title="第三章"></a>第三章</h1><h2 id="数据解析"><a href="#数据解析" class="headerlink" title="数据解析"></a>数据解析</h2><h5 id="编码流程："><a href="#编码流程：" class="headerlink" title="编码流程："></a>编码流程：</h5><ol>
<li>指定url</li>
<li>发起请求</li>
<li>获取响应数据</li>
<li>数据解析</li>
<li>持久化存储</li>
</ol>
<h5 id="数据解析分类："><a href="#数据解析分类：" class="headerlink" title="数据解析分类："></a>数据解析分类：</h5><ul>
<li>正则</li>
<li>bs4</li>
<li>xpath（重点，因为通用性强）</li>
</ul>
<h5 id="数据解析原理概述："><a href="#数据解析原理概述：" class="headerlink" title="数据解析原理概述："></a>数据解析原理概述：</h5><p>解析的局部文本内容都会在标签之间或标签对应的属性中进行存储</p>
<ol>
<li>进行指定标签的定位</li>
<li>标签或者标签对应的属性中存储的数据值进行提取（解析）</li>
</ol>
<h4 id="3-1-正则表达式"><a href="#3-1-正则表达式" class="headerlink" title="3.1 正则表达式"></a>3.1 正则表达式</h4><p>Regular Expression,正则表达式，一种使用表达式的方式对字符串进行匹配的语法规则. </p>
<p>我们抓取到的网页源代码本质上就是一个超长的字符串，想从里面提取内容.用正则再合适不过了.</p>
<p>正则的优点:速度快，效率高，准确性高<br>正则的语法:使用元字符进行排列组合用来匹配字符串<br>在线测试正则表达式<a target="_blank" rel="noopener" href="https://tool.oschina.net/regex/">https://tool.oschina.net/regex/</a></p>
<p>元字符:具有固定含义的特殊符号</p>
<h5 id="常用元字符"><a href="#常用元字符" class="headerlink" title="常用元字符"></a>常用元字符</h5><p><img src="https://img-blog.csdnimg.cn/4bbd6213ca244465bfcad354a15dcc52.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/1273e80b5a934f358c6a30a9e5b7e2d4.png" alt="在这里插入图片描述"></p>
<h5 id="经典正则表达式"><a href="#经典正则表达式" class="headerlink" title="经典正则表达式"></a>经典正则表达式</h5><p><img src="https://img-blog.csdnimg.cn/9d918e2331e24fb2864696b60672d952.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/e89c251141534d6a98ccfcac179ba28d.png" alt="在这里插入图片描述"></p>
<h5 id="贪婪匹配和惰性匹配"><a href="#贪婪匹配和惰性匹配" class="headerlink" title="贪婪匹配和惰性匹配"></a>贪婪匹配和惰性匹配</h5><ol>
<li>.*  贪婪匹配</li>
<li>.*?  惰性匹配（？控制 * 尽可能少地匹配）运用了回溯</li>
</ol>
<h5 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h5><p><img src="https://img-blog.csdnimg.cn/9b2ebd5625824195b2719854aee24a47.png" alt="在这里插入图片描述"></p>
<h4 id="3-2-Re库"><a href="#3-2-Re库" class="headerlink" title="3.2 Re库"></a>3.2 Re库</h4><p><strong>raw string类型</strong>(原生字符串类型): 不包含对转义字符再次转义的字符串<br>re库采用raw string类型表示正则表达式,表示为:  r’text’</p>
<h5 id="3-2-1-Match-对象"><a href="#3-2-1-Match-对象" class="headerlink" title="3.2.1 Match 对象"></a>3.2.1 Match 对象</h5><p>Match对象是一次匹配的结果，包含匹配的很多信息<br><strong>属性：</strong><br><img src="https://img-blog.csdnimg.cn/f2ae14e0550e441496f940a31edeafec.png" alt="在这里插入图片描述"><br><strong>方法：</strong><br><img src="https://img-blog.csdnimg.cn/deed85f73af84afb9fbf33327fa430f5.png" alt="在这里插入图片描述"></p>
<h5 id="3-2-2-Re库主要功能函数"><a href="#3-2-2-Re库主要功能函数" class="headerlink" title="3.2.2 Re库主要功能函数"></a>3.2.2 Re库主要功能函数</h5><p><img src="https://img-blog.csdnimg.cn/45ed7952e3d340059ab9769b86dc0f6b.png" alt="在这里插入图片描述"><br><strong>参数：</strong><br><img src="https://img-blog.csdnimg.cn/6bce5abd0e7c4339aab153c9128771ae.png" alt="在这里插入图片描述"></p>
<p><img src="https://img-blog.csdnimg.cn/f88bc2a939b34e95936481c3466a5996.png" alt="在这里插入图片描述"><br><strong>例：</strong></p>
<p>1. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">for</span> m <span class="keyword">in</span> re.finditer(<span class="string">r&#x27;[1-9]\d&#123;5&#125;&#x27;</span>,<span class="string">&#x27;BIT100081 TSU100084&#x27;</span>)</span><br><span class="line">	<span class="keyword">if</span> m:</span><br><span class="line">		<span class="built_in">print</span>(m.group())</span><br></pre></td></tr></table></figure>
<p>2.<br><img src="https://img-blog.csdnimg.cn/24f7956010644f5ead16d3d1388ecbab.png" alt="在这里插入图片描述"></p>
<p><img src="https://img-blog.csdnimg.cn/4a471a962c3e4130a78145c4905f0bdd.png" alt="在这里插入图片描述"></p>
<h5 id="3-2-3-预加载正则表达式"><a href="#3-2-3-预加载正则表达式" class="headerlink" title="3.2.3 预加载正则表达式"></a>3.2.3 预加载正则表达式</h5><p><img src="https://img-blog.csdnimg.cn/678edc3060104a0aa6542626b914ffc2.png" alt="在这里插入图片描述"></p>
<h5 id="3-2-4-内容获取"><a href="#3-2-4-内容获取" class="headerlink" title="3.2.4 内容获取"></a>3.2.4 内容获取</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">s = <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">	&lt;div class= &#x27;jay&#x27;&gt;&lt;span id= &#x27;1&#x27;&gt;郭麒麟&lt;/span&gt;&lt;/div&gt;</span></span><br><span class="line"><span class="string">	&lt;div class=&#x27;jj&#x27;&gt;&lt;span id= &#x27;2&#x27;&gt;宋铁&lt;/span&gt;&lt;/div&gt;</span></span><br><span class="line"><span class="string">	&lt;div class=&#x27;jolin&#x27;&gt;&lt;span id=&#x27;3 &#x27;&gt;大聪明&lt;/span&gt;&lt;/div&gt;</span></span><br><span class="line"><span class="string">	&lt;div class= &#x27; sylar&#x27;&gt;&lt;span id= &#x27; 4‘&gt;范思哲&lt;/span&gt;&lt;/div&gt;</span></span><br><span class="line"><span class="string">	&lt;div class=&#x27;tory &#x27;&gt;&lt;span id=&#x27; 5”&gt;胡说八道&lt;/span&gt;&lt;/div&gt;</span></span><br><span class="line"><span class="string">	&#x27;&#x27;&#x27;</span></span><br><span class="line">reg = re.<span class="built_in">compile</span>(<span class="string">r&quot;&lt;div class=&#x27;.*?&#x27;&gt;&lt;span id= &#x27;(?P&lt;id&gt;.*?)&#x27;&gt;(?P&lt;name&gt;.*?)&lt;/span&gt;&lt;/div&gt;&quot;</span>,flag=<span class="string">&quot;re.S&quot;</span>) </span><br><span class="line"><span class="comment"># re.S 使得 . 可以匹配换行符</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># (?P&lt;分组名字&gt;正则)  可以单独从正则匹配的内容中提取到XX名字的内容</span></span><br><span class="line"><span class="keyword">for</span> it <span class="keyword">in</span> reg.finditer(s):</span><br><span class="line">	<span class="built_in">print</span>(it.group(<span class="string">&quot;name&quot;</span>)) <span class="comment"># 可以获取名字的内容</span></span><br><span class="line">	<span class="built_in">print</span>(it.group(<span class="string">&quot;id&quot;</span>))</span><br><span class="line">	</span><br></pre></td></tr></table></figure>

<h4 id="3-3-案例"><a href="#3-3-案例" class="headerlink" title="3.3 案例"></a>3.3 案例</h4><h5 id="①爬取豆瓣top250电影排行"><a href="#①爬取豆瓣top250电影排行" class="headerlink" title="①爬取豆瓣top250电影排行"></a>①爬取豆瓣top250电影排行</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"></span><br><span class="line">url = <span class="string">&quot;https://movie.douban.com/top250&quot;</span></span><br><span class="line">headers = &#123;</span><br><span class="line">	<span class="string">&quot;User-Agent&quot;</span>:<span class="string">&quot;Mozilla/5.0&quot;</span></span><br><span class="line">&#125;</span><br><span class="line">response = requests.get(url,headers=headers)</span><br><span class="line">page_content = response.text</span><br><span class="line">reg = re.<span class="built_in">compile</span>(<span class="string">r&#x27;&lt;li&gt;.*?&lt;div class=&quot;item&quot;&gt;.*?&lt;span class=&quot;title&quot;&gt;(?P&lt;name&gt;.*?)&lt;/span&gt;.*?&lt;br&gt;(?P&lt;year&gt;.*?)&amp;nbsp.*?&lt;span class=&quot;rating_num&quot; property=&quot;v:average&quot;&gt;(?P&lt;score&gt;.*?)&lt;/span&gt;.*?&lt;span&gt;(?P&lt;num&gt;.*?)&lt;/span&gt;&#x27;</span>,re.S)</span><br><span class="line">f = <span class="built_in">open</span>(<span class="string">&quot;data.csv&quot;</span>,<span class="string">&quot;w&quot;</span>)</span><br><span class="line">csvwriter = csv.writer(f)</span><br><span class="line"><span class="keyword">for</span> it <span class="keyword">in</span> reg.finditer(page_content):</span><br><span class="line">	<span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">	print(it.group(&quot;name&quot;))</span></span><br><span class="line"><span class="string">	print(it.group(&quot;year&quot;).strip())</span></span><br><span class="line"><span class="string">	print(it.group(&quot;score&quot;))</span></span><br><span class="line"><span class="string">	print(it.group(&quot;num&quot;))</span></span><br><span class="line"><span class="string">	&#x27;&#x27;&#x27;</span></span><br><span class="line">	dic = it.groupdict()</span><br><span class="line">	dic[<span class="string">&#x27;year&#x27;</span>] = dic[<span class="string">&#x27;year&#x27;</span>].strip()</span><br><span class="line">	csvwriter.writerow(dic.values())</span><br><span class="line">f.close()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;over&quot;</span>)</span><br></pre></td></tr></table></figure>
<h5 id="②爬取电影天堂"><a href="#②爬取电影天堂" class="headerlink" title="②爬取电影天堂"></a>②爬取电影天堂</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"></span><br><span class="line">domain = <span class="string">&quot;https://dytt89.com&quot;</span></span><br><span class="line">headers = &#123;</span><br><span class="line">	<span class="string">&quot;User-Agent&quot;</span>:<span class="string">&quot;Mozilla/5.0&quot;</span></span><br><span class="line">&#125;</span><br><span class="line">response = requests.get(domain,verify=<span class="literal">False</span>) <span class="comment"># verify =False 去除安全验证</span></span><br><span class="line">response.encoding = response.apparent_encoding <span class="comment"># 防止乱码</span></span><br><span class="line">reg1 = re.<span class="built_in">compile</span>(<span class="string">r&#x27;2022必看热片.*?&lt;ul&gt;(?P&lt;ul&gt;.*?)&lt;/ul&gt;&#x27;</span>,re.S)</span><br><span class="line">res1 = reg1.finditer(response.text)</span><br><span class="line"></span><br><span class="line">hrefs = []</span><br><span class="line"><span class="keyword">for</span> it <span class="keyword">in</span> res1:</span><br><span class="line">	ul = it.group(<span class="string">&#x27;ul&#x27;</span>)</span><br><span class="line">	reg2 = re.<span class="built_in">compile</span>(<span class="string">r&#x27;&lt;a href=(?P&lt;href&gt;.*?)&#x27;</span>,re.S)</span><br><span class="line">	res2 = reg2.finditer(ul)</span><br><span class="line">	<span class="keyword">for</span> itt <span class="keyword">in</span> res:</span><br><span class="line">		hrefs.append(itt.group(<span class="string">&#x27;href&#x27;</span>))</span><br><span class="line">links = []</span><br><span class="line"><span class="keyword">for</span> href <span class="keyword">in</span> hrefs:</span><br><span class="line">	url = domain+href</span><br><span class="line">	resp = requests.get(url,verify=<span class="literal">False</span>)</span><br><span class="line">	resp.encoding = resp.apparent_encoding</span><br><span class="line">	reg3 = re.<span class="built_in">compile</span>(<span class="string">r&#x27;◎片　　名(?P&lt;name&gt;.*?)&lt;br /&gt;&lt;td style=&quot;WORD-WRAP: break-word&quot; bgcolor=&quot;#fdfddf&quot;&gt;&lt;a href=&quot;(?P&lt;link&gt;.*?)&quot;&gt;&#x27;</span>,re.S)</span><br><span class="line">	res3 = reg3.finditer(resp)</span><br><span class="line">	<span class="keyword">for</span> it <span class="keyword">in</span> res3:</span><br><span class="line">		links.append([it.group(<span class="string">&#x27;name&#x27;</span>),it.group(<span class="string">&#x27;link&#x27;</span>)])</span><br></pre></td></tr></table></figure>
<h4 id="3-4-Bs4解析基础"><a href="#3-4-Bs4解析基础" class="headerlink" title="3.4 Bs4解析基础"></a>3.4 Bs4解析基础</h4><h5 id="3-4-1数据解析的原理："><a href="#3-4-1数据解析的原理：" class="headerlink" title="3.4.1数据解析的原理："></a>3.4.1数据解析的原理：</h5><ol>
<li>标签定位</li>
<li>提取标签、标签属性中存储的数据值</li>
</ol>
<h5 id="3-4-2bs4数据解析的原理："><a href="#3-4-2bs4数据解析的原理：" class="headerlink" title="3.4.2bs4数据解析的原理："></a>3.4.2bs4数据解析的原理：</h5><ol>
<li>实例化一个BeautifulSoup对象，并且将页面源码数据及加载到该对象中</li>
<li>通过调用BeautifulSoup对象中相关的属性或者方法进行签定位和数据解析</li>
</ol>
<h5 id="3-4-3-如何实例化BeautifulSoup对象："><a href="#3-4-3-如何实例化BeautifulSoup对象：" class="headerlink" title="3.4.3 如何实例化BeautifulSoup对象："></a>3.4.3 如何实例化BeautifulSoup对象：</h5><p>from bs4 import BeautifulSoup<br>对象的实例化：</p>
<ol>
<li>将本地的html文档中的数据加载到该对象中<br>fp &#x3D; open(‘.&#x2F;test.html’, ‘r’, encoding&#x3D;’utf-8’)<br>soup &#x3D; BeautifulSoup(fp, ‘lxml’)</li>
<li>将互联网上获取的页面源码加载到对象中<br>page_text &#x3D; requests.text<br>soup &#x3D; BeautifulSoup(page_text, ‘lxml’)</li>
</ol>
<h5 id="3-4-4-提供的用于数据解析的方法和属性："><a href="#3-4-4-提供的用于数据解析的方法和属性：" class="headerlink" title="3.4.4 提供的用于数据解析的方法和属性："></a>3.4.4 提供的用于数据解析的方法和属性：</h5><p>soup.tagName : 返回的是文档中第一次出现的 tagName 对应的标签<br>soup.find():</p>
<ul>
<li>find(‘tagName’):  等同于soup.div</li>
<li>属性定位：<br>soup.find(‘div’, class_&#x2F;id&#x2F;attr&#x3D;’song’)<br>soup.find_all(‘tagName’): 返回符合要求的所有标签（列表）</li>
</ul>
<h5 id="3-4-5-select"><a href="#3-4-5-select" class="headerlink" title="3.4.5 select:"></a>3.4.5 select:</h5><p>select(‘某种选择器（id，class，标签。。。选择器）’): 返回的是一个列表</p>
<p>层级选择器：</p>
<ul>
<li>soup.select(‘.tang &gt; ul &gt; li &gt; a’)：&gt;表示的是一个层级</li>
<li>soup.select(‘.tang &gt; ul a’)：空格表示的是多个层级</li>
</ul>
<h5 id="3-4-6-获取标签之间的文本数据："><a href="#3-4-6-获取标签之间的文本数据：" class="headerlink" title="3.4.6 获取标签之间的文本数据："></a>3.4.6 获取标签之间的文本数据：</h5><p>soup.a.text&#x2F;string&#x2F;get_text()   : 两个属性一个方法</p>
<ul>
<li>text&#x2F;get_text(): 可以获取某一个标签中所有的文本内容（不属于直系的也可以）</li>
<li>string：只可以获取该标签下面直系的文本内容</li>
</ul>
<h5 id="3-4-7-获取标签中属性值："><a href="#3-4-7-获取标签中属性值：" class="headerlink" title="3.4.7 获取标签中属性值："></a>3.4.7 获取标签中属性值：</h5><p>soup.a[‘href’]</p>
<p>各种用法的例子代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="comment"># 将本地的html文档中的数据加载到该对象中</span></span><br><span class="line">fp = <span class="built_in">open</span>(<span class="string">&#x27;./test.html&#x27;</span>, <span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"><span class="comment"># 第一个参数为文件路径，第二个为固定的lxml解析器</span></span><br><span class="line">soup = BeautifulSoup(fp, <span class="string">&#x27;lxml&#x27;</span>)</span><br><span class="line"><span class="comment"># print(soup)</span></span><br><span class="line"><span class="built_in">print</span>(soup.a)  <span class="comment"># soup.tagName 返回的是html中第一次出现的tagName标签</span></span><br><span class="line"><span class="built_in">print</span>(soup.div)</span><br><span class="line"><span class="comment"># find(&#x27;tagName&#x27;):等同于soup.div</span></span><br><span class="line"><span class="built_in">print</span>(soup.find(<span class="string">&#x27;div&#x27;</span>))  <span class="comment"># print(soup.div)</span></span><br><span class="line"><span class="built_in">print</span>(soup.find(<span class="string">&#x27;div&#x27;</span>, class_=<span class="string">&#x27;song&#x27;</span>))  <span class="comment"># 注意&#x27;class&#x27;为关键字，加上下划线&#x27;class_&#x27;为参数名称</span></span><br><span class="line"><span class="built_in">print</span>(soup.find_all(<span class="string">&#x27;a&#x27;</span>))  <span class="comment"># 返回是一个列表类型</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(soup.select(<span class="string">&#x27;.tang&#x27;</span>))  <span class="comment"># &#x27;.&#x27;表示class  select返回列表</span></span><br><span class="line"> <span class="comment"># 1.层级选择器&#x27;&gt;&#x27;表示一个层级  2.bs4支持属性定位，不支持索引定位 3.返回的是列表</span></span><br><span class="line"><span class="built_in">print</span>(soup.select(<span class="string">&#x27;.tang &gt; ul &gt; li &gt; a&#x27;</span>)[<span class="number">0</span>]) </span><br><span class="line"><span class="built_in">print</span>(soup.select(<span class="string">&#x27;.tang &gt; ul &gt; li &gt; a&#x27;</span>)[<span class="number">0</span>].text)</span><br><span class="line"><span class="built_in">print</span>(soup.select(<span class="string">&#x27;.tang &gt; ul &gt; li &gt; a&#x27;</span>)[<span class="number">0</span>].string)</span><br><span class="line"><span class="built_in">print</span>(soup.select(<span class="string">&#x27;.tang &gt; ul &gt; li &gt; a&#x27;</span>)[<span class="number">0</span>].get_text())</span><br><span class="line"><span class="built_in">print</span>(soup.select(<span class="string">&#x27;.tang &gt; ul &gt; li &gt; a&#x27;</span>)[<span class="number">0</span>][<span class="string">&#x27;href&#x27;</span>])</span><br></pre></td></tr></table></figure>
<p>bs4解析案例实战 1</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line">f = <span class="built_in">open</span>(<span class="string">&quot;菜价.csv&quot;</span>,<span class="string">&quot;w&quot;</span>)</span><br><span class="line">csvwriter = csv.writer(f)</span><br><span class="line"></span><br><span class="line">url = <span class="string">&quot;https://www.xinfadi.com.cn/marketanalysis/0/list/1.shtml&quot;</span></span><br><span class="line">response = requests.get(url)</span><br><span class="line">page = BeautifulSoup(response.text,<span class="string">&quot;lxml&quot;</span>)</span><br><span class="line"></span><br><span class="line">table = page.find(<span class="string">&quot;table&quot;</span>,class_=<span class="string">&quot;hq_table&quot;</span>)</span><br><span class="line"></span><br><span class="line">trs = table.find_all(<span class="string">&quot;tr&quot;</span>)[<span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> tr <span class="keyword">in</span> trs:</span><br><span class="line">	tds = tr.find_all(<span class="string">&quot;td&quot;</span>)</span><br><span class="line">	name = tds[<span class="number">0</span>].text</span><br><span class="line">	low = tds[<span class="number">1</span>].text</span><br><span class="line">	avg = tds[<span class="number">2</span>].text</span><br><span class="line">	high = tds[<span class="number">3</span>].text</span><br><span class="line">	guige = tds[<span class="number">4</span>].text</span><br><span class="line">	dangwei = tds[<span class="number">5</span>].text</span><br><span class="line">	date = tds[<span class="number">6</span>].text</span><br><span class="line">	csvwriter.writerow([name,low,avg,high,guige,dangwei,date])</span><br><span class="line">f.close()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;over!&quot;</span>)</span><br></pre></td></tr></table></figure>

<h5 id="3-4-8-标签树的遍历"><a href="#3-4-8-标签树的遍历" class="headerlink" title="3.4.8 标签树的遍历"></a>3.4.8 标签树的遍历</h5><p><img src="https://img-blog.csdnimg.cn/e2b5b63c110c4559805cf42d50716f3e.png" alt="在这里插入图片描述"></p>
<h6 id="3-4-8-1-标签树的下行遍历"><a href="#3-4-8-1-标签树的下行遍历" class="headerlink" title="3.4.8.1 标签树的下行遍历"></a>3.4.8.1 标签树的下行遍历</h6><p><img src="https://img-blog.csdnimg.cn/c76d9bcea2944195a7e9d7f4c4b412c4.png" alt="在这里插入图片描述"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> lxml</span><br><span class="line"></span><br><span class="line">url = <span class="string">&quot;https://www.baidu.com&quot;</span></span><br><span class="line">response = requests.get(url)</span><br><span class="line">soup = BeautifuSoup(response.text,<span class="string">&quot;lxml&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> child <span class="keyword">in</span> soup.body.children: <span class="comment"># 遍历儿子节点</span></span><br><span class="line">	<span class="built_in">print</span>(child)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> child <span class="keyword">in</span> soup.body.descendants: <span class="comment">#遍历子孙节点</span></span><br><span class="line">	<span class="built_in">print</span>(child)</span><br><span class="line">	</span><br></pre></td></tr></table></figure>

<h6 id="3-4-8-2-标签树的上行遍历"><a href="#3-4-8-2-标签树的上行遍历" class="headerlink" title="3.4.8.2 标签树的上行遍历"></a>3.4.8.2 标签树的上行遍历</h6><p><img src="https://img-blog.csdnimg.cn/5c0a0e6f3154436c8ab892b8a1b7535e.png" alt="在这里插入图片描述"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> lxml</span><br><span class="line"></span><br><span class="line">url = <span class="string">&quot;https://www.baidu.com&quot;</span></span><br><span class="line">response = requests.get(url)</span><br><span class="line">soup = BeautifuSoup(response.text,<span class="string">&quot;lxml&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> parent <span class="keyword">in</span> soup.a.parents:</span><br><span class="line">	<span class="keyword">if</span> parent <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">		<span class="built_in">print</span>(parent)</span><br><span class="line">	<span class="keyword">else</span>:</span><br><span class="line">		<span class="built_in">print</span>(parent.name)</span><br></pre></td></tr></table></figure>
<p>3.4.8.2 标签树的平行遍历<br><img src="https://img-blog.csdnimg.cn/52ac91afdc5c4979872a064143f43a1c.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/b5277fe8adfc4106995c72f604d55d00.png" alt="在这里插入图片描述"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> lxml</span><br><span class="line"></span><br><span class="line">url = <span class="string">&quot;https://www.baidu.com&quot;</span></span><br><span class="line">response = requests.get(url)</span><br><span class="line">soup = BeautifuSoup(response.text,<span class="string">&quot;lxml&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> sibling <span class="keyword">in</span> soup.a.next_sibling: <span class="comment">#遍历后续节点</span></span><br><span class="line">	<span class="built_in">print</span>(sibling)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> sibling <span class="keyword">in</span> soup.a.previous_sibling: <span class="comment"># 遍历前续节点</span></span><br><span class="line">	<span class="built_in">print</span>(sibling)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>bs4解析案例实战 2</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 爬取优美图库</span></span><br><span class="line"><span class="comment"># 1.拿到主页面的源代码。然后提取到子页面的链接地址，href </span></span><br><span class="line"><span class="comment"># 2.通过href拿到子页面的内容。从子页面中找到图片的下载地址img -&gt; src</span></span><br><span class="line"><span class="comment"># 3.下载图片</span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> lxml</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line">url = <span class="string">&quot;https://umei.cc&quot;</span></span><br><span class="line">headers = &#123;</span><br><span class="line">	<span class="string">&quot;User-Agent&quot;</span>:<span class="string">&quot;Mozilla/5.0&quot;</span></span><br><span class="line">&#125;</span><br><span class="line">response = requests.get(url,headers=headers)</span><br><span class="line">response.encoding = response.apparent_encoding</span><br><span class="line"></span><br><span class="line">soup = BeautifulSoup(response.text,<span class="string">&quot;lxml&quot;</span>)</span><br><span class="line">alist = soup.find(<span class="string">&quot;div&quot;</span>,class_=<span class="string">&quot;TypeList&quot;</span>).find_all(<span class="string">&#x27;a&#x27;</span>)</span><br><span class="line"></span><br><span class="line">links = []</span><br><span class="line"><span class="keyword">for</span> a <span class="keyword">in</span> alist:</span><br><span class="line">	href = a.get(<span class="string">&#x27;href&#x27;</span>)</span><br><span class="line">	res2 = requests.get(href,headers=headers)</span><br><span class="line">	res2.encoding = res2.apparent_encoding</span><br><span class="line">	soup2 = BeautifulSoup(res2.text,<span class="string">&quot;lxml&quot;</span>)</span><br><span class="line">	img = soup2.find(<span class="string">&quot;p&quot;</span>,align=<span class="string">&quot;center&quot;</span>).find(<span class="string">&quot;img&quot;</span>)</span><br><span class="line">	src = img.get(<span class="string">&quot;src&quot;</span>)</span><br><span class="line">	<span class="comment"># 下载图片</span></span><br><span class="line">	img_resp = requests.get(src)</span><br><span class="line">	img_name = src.split(<span class="string">&quot;/&quot;</span>)[-<span class="number">1</span>]</span><br><span class="line">	<span class="keyword">with</span> <span class="built_in">open</span>(img_name,<span class="string">&quot;wb&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">		f.write(img_resp.content) <span class="comment"># 返回字节</span></span><br><span class="line">	<span class="built_in">print</span>(img_name,<span class="string">&quot;over&quot;</span>)</span><br><span class="line">	time.sleep(<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;all_over&quot;</span>)	</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="3-5-xpath解析基础"><a href="#3-5-xpath解析基础" class="headerlink" title="3.5 xpath解析基础"></a>3.5 xpath解析基础</h4><p><strong>xpath解析</strong>：最常用且最便捷高效的一种解析方式，同时也是通用性最强的一种方式。</p>
<p><strong>xpath解析原理</strong>：</p>
<ol>
<li>实例化一个etree的对象，且需要将被解析的页面源码数据加载到该对象中。</li>
<li>调用etree对象中的xpath方法结合着xpath表达式实现标签的定位和内容的捕获。</li>
</ol>
<p><strong>如何实例化一个etree对象</strong>：<br>from lxml import etree</p>
<ol>
<li><p>将本地的html文档中的源码数据加载到etree对象中：<br>etree.parse(filePath)</p>
</li>
<li><p>可以将从互联网上获取的源码数据加载到该对象中:<br>etree.HTML(‘page_text’)</p>
</li>
<li><p>xpath(‘xpath表达式’):<br> - &#x2F;：表示的是从根节点开始定位。表示的是一个层级<br> - &#x2F;&#x2F;：表示的是多个层级；可以表示从任意位置开始定位。<br> - 属性定位：&#x2F;&#x2F;div[@class&#x3D;”song”] 语法：tag[@attrName&#x3D;”attrValue”]<br> - 索引定位：&#x2F;&#x2F;div[@class&#x3D;”song”]&#x2F;p[3] 索引是从1开始的<br> - 取文本：<br>     &#x2F;text() 获取的是标签中的直系文本内容<br>     &#x2F;&#x2F;text() 获取标签中非直系文本的内容（所有的文本内容）<br> - 取属性：<br> &#x2F;@attrName &#x3D;&#x3D;&gt;img&#x2F;@src</p>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> lxm <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">xml = <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&lt;book&gt; </span></span><br><span class="line"><span class="string">	&lt;id&gt;1&lt;/id&gt;</span></span><br><span class="line"><span class="string">	&lt;name&gt;野花遍地香&lt;/ name&gt;</span></span><br><span class="line"><span class="string">	&lt;price&gt;1.23&lt;/price&gt; </span></span><br><span class="line"><span class="string">	&lt;nick&gt;臭豆腐&lt;/nick&gt;</span></span><br><span class="line"><span class="string">	&lt;author&gt; </span></span><br><span class="line"><span class="string">		&lt;nick id=&quot;10086&quot;&gt;周大强&lt;/nick&gt;</span></span><br><span class="line"><span class="string">		&lt;nick id=&quot;10010&quot;&gt;周芷若 &lt;/nick&gt;</span></span><br><span class="line"><span class="string">		&lt;nick class=&quot;jqy&quot;&gt;周杰伦&lt;/nick&gt;</span></span><br><span class="line"><span class="string">		&lt;nick class=&quot;jolin&quot;&gt;蔡 依林&lt;/nick&gt;</span></span><br><span class="line"><span class="string">		&lt;div&gt; </span></span><br><span class="line"><span class="string">			&lt;nick&gt;惹了&lt;/nick&gt; </span></span><br><span class="line"><span class="string">		&lt;/div&gt;</span></span><br><span class="line"><span class="string">	&lt;/ author&gt;</span></span><br><span class="line"><span class="string">	&lt;partner&gt;</span></span><br><span class="line"><span class="string">		&lt;nick id=&quot;ppc&quot;&gt;胖胖陈&lt;/nick&gt;</span></span><br><span class="line"><span class="string">		&lt;nick id=&quot;ppbc&quot;&gt;胖胖不陈&lt;/ nick&gt;</span></span><br><span class="line"><span class="string">	&lt;/ partner&gt;</span></span><br><span class="line"><span class="string">&lt;/book&gt;</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">tree = etree.XML(xml)</span><br><span class="line"><span class="comment"># result = tree. xpath(&quot;/book&quot;) # /表示层级关系。第一个/是根节点</span></span><br><span class="line"><span class="comment"># result = tree. xpath (&quot; /book/ name&#x27;&quot;)</span></span><br><span class="line"><span class="comment"># result = tree. xpath(&quot; /book/name/text()&quot;) # text()拿文本</span></span><br><span class="line"><span class="comment"># result = tree， xpath(&quot;/book/author//nick/text()&quot;) # //后代</span></span><br><span class="line"><span class="comment"># result = tree. xpath(&quot; /book/ author/*/nick/text()&quot;) # *任意的节点。通配符</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># from lxml import etree</span></span><br><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> html</span><br><span class="line">etree = html.etree</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># 实例化好了一个etree对象，且将被解析的源码加载到了该对象中</span></span><br><span class="line">    tree = etree.parse(<span class="string">&#x27;test.html&#x27;</span>)</span><br><span class="line">    <span class="comment"># r = tree.xpath(&#x27;/html/body/div&#x27;)</span></span><br><span class="line">    <span class="comment"># r = tree.xpath(&#x27;/html//div&#x27;)</span></span><br><span class="line">    <span class="comment"># r = tree.xpath(&#x27;//div&#x27;)</span></span><br><span class="line">    <span class="comment"># r = tree.xpath(&#x27;//div[@class=&quot;song&quot;]&#x27;)</span></span><br><span class="line">    <span class="comment"># r = tree.xpath(&#x27;//div[@class=&quot;song&quot;]/p[3]&#x27;)  # 索引从1开始，不是从0开始</span></span><br><span class="line">    <span class="comment"># r = tree.xpath(&#x27;//div[@class=&quot;tang&quot;]//li[5]/a/text()&#x27;)[0]</span></span><br><span class="line">    <span class="comment"># r = tree.xpath(&#x27;//li[7]//text()&#x27;)</span></span><br><span class="line">    <span class="comment"># r = tree.xpath(&#x27;//div[@class=&quot;tang&quot;]//text()&#x27;)</span></span><br><span class="line">    r = tree.xpath(<span class="string">&#x27;//div[@class=&quot;song&quot;]/img/@src&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(r)</span><br><span class="line">	li_ls = tree.xpath(<span class="string">&#x27;/html/body/div[@class=&quot;tang&quot;]/li&#x27;</span>)</span><br><span class="line">	<span class="keyword">for</span> li <span class="keyword">in</span> li_ls:</span><br><span class="line">		res = ls.xpath(<span class="string">&#x27;./a/text()&#x27;</span>) <span class="comment"># 相对查找 .表示当前节点</span></span><br></pre></td></tr></table></figure>

<h4 id="3-5-2-xpath实战"><a href="#3-5-2-xpath实战" class="headerlink" title="3.5.2 xpath实战"></a>3.5.2 xpath实战</h4><h4 id="①案例一：58二手房"><a href="#①案例一：58二手房" class="headerlink" title="①案例一：58二手房"></a>①案例一：58二手房</h4><p>需求：爬取58二手房中的房源信息 (2021.7.27版58同城。网站的html代码架构已经改变，和教程里的版本不一样）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> html</span><br><span class="line">etree = html.etree</span><br><span class="line"></span><br><span class="line"><span class="comment"># 需求：爬取58二手房中的房源信息  (2021.7.27版58同城。网站的代码已经改变，和教程里的版本不一样）</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># 爬取到页面源码数据</span></span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36&#x27;</span></span><br><span class="line">    &#125;</span><br><span class="line">    url = <span class="string">&#x27;https://bj.58.com/ershoufang/&#x27;</span></span><br><span class="line">    page_text = requests.get(url=url, headers=headers).text</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 数据解析</span></span><br><span class="line">    tree = etree.HTML(page_text)</span><br><span class="line">    div_list = tree.xpath(<span class="string">&#x27;//div[@class=&quot;property-content&quot;]&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(div_list)</span><br><span class="line">    fp = <span class="built_in">open</span>(<span class="string">&#x27;58.txt&#x27;</span>, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> div <span class="keyword">in</span> div_list:</span><br><span class="line">        <span class="comment"># 局部解析</span></span><br><span class="line">        title = div.xpath(<span class="string">&#x27;./div[@class=&quot;property-content-detail&quot;]/&#x27;</span></span><br><span class="line">                          <span class="string">&#x27;div[@class=&quot;property-content-title&quot;]/h3/text()&#x27;</span>)[<span class="number">0</span>]  <span class="comment"># 若开头用&#x27;/&#x27;则表示html根目录，&#x27;./&#x27;表示当前div指向的</span></span><br><span class="line">        <span class="built_in">print</span>(title)</span><br><span class="line">        price = div.xpath(<span class="string">&#x27;./div[@class=&quot;property-price&quot;]/p[@class=&quot;property-price-total&quot;]//text()&#x27;</span>)[<span class="number">0</span>]</span><br><span class="line">        <span class="built_in">print</span>(price)</span><br><span class="line">        <span class="comment"># print(div.xpath(&#x27;./h3/text()&#x27;))</span></span><br><span class="line">        fp.write(<span class="string">&#x27;标题:&#x27;</span> + title + <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">        fp.write(<span class="string">&#x27;价格:&#x27;</span> + price + <span class="string">&#x27;万\n&#x27;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="四、模拟登录"><a href="#四、模拟登录" class="headerlink" title="四、模拟登录"></a>四、模拟登录</h1><h4 id="4-1-模拟登录Cookie操作"><a href="#4-1-模拟登录Cookie操作" class="headerlink" title="4.1 模拟登录Cookie操作"></a>4.1 模拟登录Cookie操作</h4><ul>
<li>http&#x2F;https协议特性：无状态。</li>
<li>没有请求到对应数据的原因：<br>发起的第二次基于个人主页页面请求的时候，服务器端并不知道该请求是基于登录状态下的请求。</li>
<li>cookie：用来让服务器端记录客户端的相关状态。</li>
<li>处理cookie的形式：<ul>
<li>手动处理：通过抓包工具获得cookie值，将该值封装到headers中（不推荐）<br>（通用性不强，处理麻烦；有的页面cookie是动态变化的，有的具有有效时长）</li>
<li>自动处理：<ul>
<li>cookie值的来源是哪里？<br>  模拟登录post请求后，由服务器端创建。</li>
<li>session会话对象：<ul>
<li>作用：<ol>
<li>可以进行请求的发送</li>
<li>如果请求过程中产生了cookie，则该cookie会被自动存储&#x2F;携带在该session对象中</li>
</ol>
</li>
</ul>
</li>
<li>操作流程：<ol>
<li>创建一个session对象：session &#x3D; requests.Session()</li>
<li>使用session对象进行模拟登录post请求发送（cookie会被存储在session中）</li>
<li>session对象对个人主页对应的get请求进行发送（携带了cookie）<br>爬取用户信息修改版（使用session对象）：</li>
</ol>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> html</span><br><span class="line">etree = html.etree</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> CodeClass <span class="keyword">import</span> Chaojiying_Client</span><br><span class="line"></span><br><span class="line"><span class="comment"># ①创建一个session对象</span></span><br><span class="line">session = requests.Session()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.对验证码图片进行捕获和识别</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    url = <span class="string">&#x27;https://www.renren.com/login&#x27;</span></span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36&#x27;</span></span><br><span class="line">    &#125;</span><br><span class="line">    page_text = requests.get(url=url, headers=headers).text</span><br><span class="line">    tree = etree.HTML(page_text)</span><br><span class="line"></span><br><span class="line">    code_img_src = tree.xpath(<span class="string">&#x27;/html/body/div/div[3]/div/div[1]/div[2]/div[2]/div[3]/div/img/@src&#x27;</span>)[<span class="number">0</span>]</span><br><span class="line">    code_img_data = requests.get(url=code_img_src, headers=headers)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;./code.jpg&#x27;</span>, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">        fp.write(code_img_data)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用超级鹰提供的示例代码对验证码图片进行识别</span></span><br><span class="line">    chaojiying = Chaojiying_Client(<span class="string">&#x27;qwq&#x27;</span>, <span class="string">&#x27;qwq&#x27;</span>, <span class="string">&#x27;qwq&#x27;</span>)</span><br><span class="line">    im = <span class="built_in">open</span>(<span class="string">&#x27;code.jpg&#x27;</span>, <span class="string">&#x27;rb&#x27;</span>).read()</span><br><span class="line">    result = chaojiying.PostPic(im, <span class="number">1902</span>)</span><br><span class="line">    <span class="built_in">print</span>(result)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    login_url = <span class="string">&#x27;&#x27;</span></span><br><span class="line">    data = &#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment"># ②使用session进行请求的发送</span></span><br><span class="line">    response = session.post(url=login_url, headers=headers, data=data)</span><br><span class="line">    <span class="built_in">print</span>(response.status_code)     <span class="comment"># 如果是200，则请求成功</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 爬取当前用户的个人主页对应的页面数据</span></span><br><span class="line">    detail_url = <span class="string">&#x27;http://www.renren.com/*********/profile&#x27;</span></span><br><span class="line">    <span class="comment"># 手动cookie处理</span></span><br><span class="line">    <span class="comment"># 通用性不强，处理麻烦</span></span><br><span class="line">    <span class="comment"># 有的页面cookie是动态变化的，有的具有有效时长</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># headers = &#123;</span></span><br><span class="line">    <span class="comment">#     &#x27;Cookie&#x27;: &#x27;*****&#x27;</span></span><br><span class="line">    <span class="comment"># &#125;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># ③使用携带cookie的session进行get请求的发送</span></span><br><span class="line">    detail_page_text = session.get(url=detail_url, headers=headers).text</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;detailPage.html&#x27;</span>, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">        fp.write(detail_page_text)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="4-2-反盗链"><a href="#4-2-反盗链" class="headerlink" title="4.2 反盗链"></a>4.2 反盗链</h4><p>部分链接跳转时会溯源，检查是从那个url跳转过来<br>由请求头中的refers控制。</p>
<h4 id="4-3-代理"><a href="#4-3-代理" class="headerlink" title="4.3 代理"></a>4.3 代理</h4><ul>
<li>代理：破解封IP这种反爬机制。</li>
<li>什么是代理：<ul>
<li>代理服务器。</li>
</ul>
</li>
<li>代理的作用：<ul>
<li>突破自身IP访问的限制。</li>
<li>隐藏自身真实的IP。</li>
</ul>
</li>
<li>代理相关的网站：<ul>
<li>快代理</li>
<li>西祠代理已经挂了qwq（下面找了一个可以用的）</li>
<li>西拉代理（登陆之后每天可以免费获取500个网络代理网址IP。注册以后点击上方“免费API接口”，按照提示操作就能获取免费的代理IP了）</li>
<li><a target="_blank" rel="noopener" href="http://www.goubanjia.com(这个也挂了qwq)/">www.goubanjia.com（这个也挂了qwq）</a></li>
</ul>
</li>
<li>IP查询网站：<ul>
<li>ip.293.net</li>
</ul>
</li>
<li>代理IP的类型：<ul>
<li>http：应用到http协议对应的url中</li>
<li>https：应用到https协议对应的url中</li>
</ul>
</li>
<li>代理IP的匿名度：<ul>
<li><pre><code>透明：服务器它知道该次请求使用了代理，也知道请求对应的真实IP
</code></pre>
</li>
<li>匿名：服务器它知道该次请求使用了代理，不知道请求对应的真实IP</li>
<li>高匿：服务器不知道该次请求使用了代理，不知道请求对应的真实IP</li>
</ul>
</li>
<li>反爬机制：封IP</li>
<li>反反爬策略：使用代理进行请求发送</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    url = <span class="string">&#x27;https://www.baidu.com/s?wd=ip&#x27;</span></span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36&#x27;</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># page_text = requests.get(url=url, headers=headers, proxies=&#123;&quot;https&quot;: &#x27;免费的代理暂时没找到好用的，付费的还没试，使用的时候把：后面的去掉&#x27;&#125;).text</span></span><br><span class="line">    <span class="comment"># page_text = requests.get(url=url, headers=headers, proxies=&#123;&quot;https&quot;: &#x27;&#x27;&#125;).text</span></span><br><span class="line">    page_text = requests.get(url=url, headers=headers).text</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;ip.html&#x27;</span>, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">        fp.write(page_text)</span><br><span class="line"><span class="comment"># 反爬机制：封IP</span></span><br><span class="line"><span class="comment"># 反反爬策略：使用代理进行请求发送</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1 id="第六章-异步爬虫"><a href="#第六章-异步爬虫" class="headerlink" title="第六章 异步爬虫"></a>第六章 异步爬虫</h1><p>进程：资源单位，一个进程里至少有一个线程<br>线程：执行单位<br>协程：又称为微线程，在一个线程中执行，执行函数时可以随时中断，由程序（用户）自身控制，执行效率极高，与多线程比较，没有切换线程的开销和多线程锁机制</p>
<h3 id="6-1异步爬虫的方式："><a href="#6-1异步爬虫的方式：" class="headerlink" title="6.1异步爬虫的方式："></a>6.1异步爬虫的方式：</h3><h4 id="1-多线程、多进程（不建议）："><a href="#1-多线程、多进程（不建议）：" class="headerlink" title="1. 多线程、多进程（不建议）："></a>1. 多线程、多进程（不建议）：</h4><p>好处：可以为相关阻塞的操作单独开启线程或进程，阻塞操作就可以异步执行。<br>弊端：无法无限制的开启多线程或者多进程。</p>
<h3 id="6-2-代码实现"><a href="#6-2-代码实现" class="headerlink" title="6.2 代码实现"></a>6.2 代码实现</h3><h4 id="①单线程爬取数据"><a href="#①单线程爬取数据" class="headerlink" title="①单线程爬取数据"></a>①单线程爬取数据</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">            <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36&#x27;</span></span><br><span class="line">    &#125;</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_content</span>(<span class="params">url</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;正在爬取：&#x27;</span>, url)</span><br><span class="line">    <span class="comment"># get方法是一个阻塞的方法</span></span><br><span class="line">    response = requests.get(url=url, headers=headers)</span><br><span class="line">    <span class="built_in">print</span>(response.status_code)</span><br><span class="line">    <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">        <span class="keyword">return</span> response.content</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">parse_content</span>(<span class="params">content</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;相应数据的长度为：&#x27;</span>, <span class="built_in">len</span>(content))</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    urls = [</span><br><span class="line">            <span class="string">&#x27;https://downsc.chinaz.net/Files/DownLoad/jianli/202107/jianli15646.rar&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;https://downsc.chinaz.net/Files/DownLoad/jianli/202107/jianli15647.rar&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;https://downsc.chinaz.net/Files/DownLoad/jianli/202107/jianli15651.rar&#x27;</span>,</span><br><span class="line">        ]</span><br><span class="line">    <span class="keyword">for</span> url <span class="keyword">in</span> urls:</span><br><span class="line">        content = get_content(url)</span><br><span class="line">        parse_content(content)</span><br></pre></td></tr></table></figure>
<h4 id="2-线程池、进程池（适当的使用）："><a href="#2-线程池、进程池（适当的使用）：" class="headerlink" title="2. 线程池、进程池（适当的使用）："></a>2. 线程池、进程池（适当的使用）：</h4><p>好处：我们可以降低系统对进程或者线程创建和销毁的频率，从而很好地降低系统的开销。<br>弊端：池中线程或进程的数量是有上限的。<br>原则：线程池处理的是阻塞且较为耗时的操作</p>
<h4 id="②线程池爬取数据"><a href="#②线程池爬取数据" class="headerlink" title="②线程池爬取数据"></a>②线程池爬取数据</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># import time</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># # 使用单线程串行方式执行</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># def get_page(str):</span></span><br><span class="line"><span class="comment">#     print(&quot;正在下载:&quot;, str)</span></span><br><span class="line"><span class="comment">#     time.sleep(2)</span></span><br><span class="line"><span class="comment">#     print(&quot;下载成功&quot;, str)</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># if __name__ == &quot;__main__&quot;:</span></span><br><span class="line"><span class="comment">#     name_list = [&#x27;xiaozi&#x27;, &#x27;aa&#x27;, &#x27;bb&#x27;, &#x27;cc&#x27;]</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#     start_time = time.time()</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#     for i in range(len(name_list)):</span></span><br><span class="line"><span class="comment">#         get_page(name_list[i])</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#     end_time = time.time()</span></span><br><span class="line"><span class="comment">#     print(&#x27;%d second&#x27; % (end_time - start_time))</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="comment"># 导入线程池对应的类</span></span><br><span class="line"><span class="keyword">from</span> multiprocessing.dummy <span class="keyword">import</span> Pool</span><br><span class="line"><span class="comment"># 使用线程池方式执行</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_page</span>(<span class="params"><span class="built_in">str</span></span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;正在下载:&quot;</span>, <span class="built_in">str</span>)</span><br><span class="line">    time.sleep(<span class="number">2</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;下载成功&quot;</span>, <span class="built_in">str</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    start_time = time.time()</span><br><span class="line">    name_list = [<span class="string">&#x27;xiaozi&#x27;</span>, <span class="string">&#x27;aa&#x27;</span>, <span class="string">&#x27;bb&#x27;</span>, <span class="string">&#x27;cc&#x27;</span>]</span><br><span class="line">    <span class="comment"># 实例化一个线程池对象</span></span><br><span class="line">    <span class="comment"># (优化后耗时2秒，试了一下，把4改成3耗时4秒，推测是有一条为单线)</span></span><br><span class="line">    pool = Pool(<span class="number">4</span>)</span><br><span class="line">    <span class="comment"># 将列表中每一个列表元素传递到gert_page进行处理</span></span><br><span class="line">    <span class="comment"># map的返回值是get_page函数的返回值，为一个列表，此处不需要进行处理</span></span><br><span class="line">    pool.<span class="built_in">map</span>(get_page, name_list)</span><br><span class="line"></span><br><span class="line">    end_time = time.time()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;%d second&#x27;</span> % (end_time - start_time))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="③爬虫中应用线程池（动态加载的video标签待解决"><a href="#③爬虫中应用线程池（动态加载的video标签待解决" class="headerlink" title="③爬虫中应用线程池（动态加载的video标签待解决)"></a>③爬虫中应用线程池（动态加载的video标签待解决)</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> html</span><br><span class="line">etree = html.etree</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> multiprocessing.dummy <span class="keyword">import</span> Pool</span><br><span class="line"></span><br><span class="line"><span class="comment"># 需求：爬取视频的视频数据</span></span><br><span class="line">headers = &#123;</span><br><span class="line">            <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36&#x27;</span></span><br><span class="line">    &#125;</span><br><span class="line"><span class="comment"># 原则：线程池处理的是阻塞且较为耗时的操作</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_video_data</span>(<span class="params">dic</span>):</span><br><span class="line">    url = dic[<span class="string">&#x27;url&#x27;</span>]</span><br><span class="line">    <span class="built_in">print</span>(dic[<span class="string">&#x27;name&#x27;</span>], <span class="string">&#x27;downloading...&#x27;</span>)</span><br><span class="line">    data = requests.get(url=url, headers=headers).content</span><br><span class="line">    <span class="comment"># 持久化存储操作</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(dic[<span class="string">&#x27;name&#x27;</span>], <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">        fp.write(data)</span><br><span class="line">        <span class="built_in">print</span>(dic[<span class="string">&#x27;name&#x27;</span>], <span class="string">&#x27;下载成功&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># 对下述url发起请求解析出视频详情页的url和视频名称</span></span><br><span class="line">    url = <span class="string">&#x27;https://www.pearvideo.com/category_5&#x27;</span></span><br><span class="line">    page_text = requests.get(url=url, headers=headers).text</span><br><span class="line"></span><br><span class="line">    tree = etree.HTML(page_text)</span><br><span class="line">    li_list = tree.xpath(<span class="string">&#x27;//ul[@id=&quot;listvideoListUl&quot;]/li&#x27;</span>)</span><br><span class="line">    urls = []  <span class="comment"># 存储所有视频的名称和链接</span></span><br><span class="line">    <span class="keyword">for</span> li <span class="keyword">in</span> li_list:</span><br><span class="line">        detail_url = <span class="string">&#x27;https://www.pearvideo.com/&#x27;</span> + li.xpath(<span class="string">&#x27;./div/a/@href&#x27;</span>)[<span class="number">0</span>]</span><br><span class="line">        name = li.xpath(<span class="string">&#x27;./div/a/div[2]/text()&#x27;</span>)[<span class="number">0</span>] + <span class="string">&#x27;.mp4&#x27;</span></span><br><span class="line">        <span class="built_in">print</span>(detail_url, name)</span><br><span class="line">        <span class="comment"># 对详情页的url发起请求</span></span><br><span class="line">        detail_page_text = requests.get(url=detail_url, headers=headers).text</span><br><span class="line">        <span class="comment"># 从详情页中解析出视频的地址(url)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 用作案例的网站在教程中的那个时间段(一几年，大概是18年)</span></span><br><span class="line">        <span class="comment"># 没有video标签，视频的地址在js里面，所以不能用bs4和xpath</span></span><br><span class="line">        <span class="comment"># 但是网站已经更新(2021.08.01)，我现在学的时候src的Url已经在video标签中了</span></span><br><span class="line">        <span class="comment"># 因此此处正则不再适用,咱用xpath修改一下</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 教程代码：</span></span><br><span class="line">        <span class="comment"># ex = &#x27;srcUrl=&quot;(.*?)&quot;,vdoUrl&#x27;</span></span><br><span class="line">        <span class="comment"># video_url = re.findall(ex, detail_page_text)[0]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 咱的代码：</span></span><br><span class="line">        <span class="comment"># detail_tree = etree.HTML(detail_page_text)</span></span><br><span class="line">        <span class="comment"># print(detail_page_text)</span></span><br><span class="line">        <span class="comment"># video = detail_tree.xpath(&#x27;//*[@id=&quot;JprismPlayer&quot;]/video&#x27;)</span></span><br><span class="line">        <span class="comment"># print(video)</span></span><br><span class="line">        <span class="comment"># video_url = video[0]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 哈哈，放video标签之后不知道是人家反爬了还是咱的爬取有问题</span></span><br><span class="line">        <span class="comment"># 应该是弄得Ajax动态加载</span></span><br><span class="line">        <span class="comment"># 爬到的：</span></span><br><span class="line">        <span class="comment"># &lt;div class=&quot;main-video-box&quot; id=&quot;drag_target1&quot;&gt;</span></span><br><span class="line">        <span class="comment">#     &lt;div class=&quot;img prism-player&quot; style=&quot;height:100% !important;&quot; id=&quot;JprismPlayer&quot;&gt;</span></span><br><span class="line">        <span class="comment">#     &lt;/div&gt;</span></span><br><span class="line">        <span class="comment"># &lt;/div&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># F12看到的：</span></span><br><span class="line">        <span class="comment"># &lt;div class=&quot;main-video-box&quot; id=&quot;drag_target1&quot;&gt;</span></span><br><span class="line">        <span class="comment">#     &lt;div class=&quot;img prism-player play&quot; style=&quot;height: 100% !important; width: 100%;&quot; id=&quot;JprismPlayer&quot;</span></span><br><span class="line">        <span class="comment">#     x-webkit-airplay=&quot;&quot; playsinline=&quot;&quot; webkit-playsinline=&quot;&quot; &gt;</span></span><br><span class="line">        <span class="comment">#         &lt; video webkit-playsinline=&quot;&quot; playsinline=&quot;&quot; x-webkit-airplay=&quot;&quot; autoplay=&quot;autoplay&quot;</span></span><br><span class="line">        <span class="comment">#         src=&quot;https://video.pearvideo.com/mp4/third/20210801/cont-1737209-12785353-091735-hd.mp4&quot;</span></span><br><span class="line">        <span class="comment">#         style=&quot;width: 100%; height: 100%;&quot;&gt;</span></span><br><span class="line">        <span class="comment">#         &lt; / video &gt;</span></span><br><span class="line">        <span class="comment">#         ...</span></span><br><span class="line">        <span class="comment">#     &lt;/div&gt;</span></span><br><span class="line">        <span class="comment"># &lt;/div&gt;</span></span><br><span class="line">        <span class="comment"># 又挖了一个坑，以后再填qwq</span></span><br><span class="line">        video_url = <span class="string">&#x27;&#x27;</span></span><br><span class="line">        dic = &#123;</span><br><span class="line">            <span class="string">&#x27;name&#x27;</span>: name,</span><br><span class="line">            <span class="string">&#x27;url&#x27;</span>: video_url</span><br><span class="line">        &#125;</span><br><span class="line">        urls.append(video_url)</span><br><span class="line">        <span class="comment"># 使用线程池对视频数据进行请求(较为耗时的阻塞操作)</span></span><br><span class="line">        pool = Pool(<span class="number">4</span>)</span><br><span class="line">        pool.<span class="built_in">map</span>(get_video_data, urls)</span><br><span class="line"></span><br><span class="line">        pool.close()</span><br><span class="line">        pool.join()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="3-单线程-异步协程（推荐）："><a href="#3-单线程-异步协程（推荐）：" class="headerlink" title="3. 单线程 + 异步协程（推荐）："></a>3. 单线程 + 异步协程（推荐）：</h4><p>一些概念和两个关键字：<br>    - ①event_loop：事件循环，相当于一个无限循环，我们可以把一些函数注册到这个事件循环上，当满足某些条件时，函数就会被循环执行。<br>    - ②coroutline：协程对象，我们可以将协程对象注册到事件循环中，它会被事件循环调用。我们可以使用async关键字来定义一个方法，这个方法在调用时不会被立即被执行，而是返回一个协程对象。<br>    - ③task：任务，它是对协程对象的进一步封装，包含了任务的各个状态。<br>    - ④future：代表将来执行或还没有执行的任务，实际上和task没有本质区别。<br>    - ⑤async：定义一个协程。<br>    - ⑥await：用来挂起阻塞方法的执行。</p>
<h4 id="④协程"><a href="#④协程" class="headerlink" title="④协程"></a>④协程</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">request</span>(<span class="params">url</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;正在请求的url是&#x27;</span>, url)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;请求成功&#x27;</span>, url)</span><br><span class="line">    <span class="keyword">return</span> url</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># async修饰的函数，调用之后返回的一个协程对象</span></span><br><span class="line">c = request(<span class="string">&#x27;www.baidu.com&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># # 创建一个事件循环对象</span></span><br><span class="line"><span class="comment"># loop = asyncio.get_event_loop()</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># # 将协程对象注册到loop中，然后启动loop</span></span><br><span class="line"><span class="comment"># loop.run_until_complete(c)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># # task的使用</span></span><br><span class="line"><span class="comment"># loop = asyncio.get_event_loop()</span></span><br><span class="line"><span class="comment"># # 基于loop创建task任务对象</span></span><br><span class="line"><span class="comment"># task = loop.create_task(c)</span></span><br><span class="line"><span class="comment"># print(task)</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># loop.run_until_complete(task)</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># print(task)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># # future的使用</span></span><br><span class="line"><span class="comment"># loop = asyncio.get_event_loop()</span></span><br><span class="line"><span class="comment"># task = asyncio.ensure_future(c)</span></span><br><span class="line"><span class="comment"># print(task)</span></span><br><span class="line"><span class="comment"># loop.run_until_complete(task)</span></span><br><span class="line"><span class="comment"># print(task)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">callback_func</span>(<span class="params">task</span>):</span><br><span class="line">    <span class="comment"># result返回的是任务对象中封装的协程对象对应函数的返回值</span></span><br><span class="line">    <span class="built_in">print</span>(task.result())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 绑定回调</span></span><br><span class="line">loop = asyncio.get_event_loop()</span><br><span class="line">task = asyncio.ensure_future(c)</span><br><span class="line"><span class="comment"># 将回调函数绑定到任务对象中</span></span><br><span class="line">task.add_done_callback(callback_func)</span><br><span class="line">loop.run_until_complete(task)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="⑤多任务异步协程01"><a href="#⑤多任务异步协程01" class="headerlink" title="⑤多任务异步协程01"></a>⑤多任务异步协程01</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">request</span>(<span class="params">url</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;正在下载&#x27;</span>, url)</span><br><span class="line">    <span class="comment"># time.sleep(2) 在异步协程中如果出现了同步模块相关的代码，那么就无法实现异步</span></span><br><span class="line">    <span class="comment"># 当在asyncio中遇到阻塞操作必须进行手动挂起</span></span><br><span class="line">    <span class="keyword">await</span> asyncio.sleep(<span class="number">2</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;下载完毕&#x27;</span>, url)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">start = time.time()</span><br><span class="line">urls = [</span><br><span class="line">    <span class="string">&#x27;www.baidu.com&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;www.sogou.com&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;www.goubanjia.com&#x27;</span></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 任务列表：存放多个任务对象</span></span><br><span class="line">stasks = []</span><br><span class="line"><span class="keyword">for</span> url <span class="keyword">in</span> urls:</span><br><span class="line">    c = request(url)</span><br><span class="line">    task = asyncio.ensure_future(c)</span><br><span class="line">    stasks.append(task)</span><br><span class="line"></span><br><span class="line">loop = asyncio.get_event_loop()</span><br><span class="line"><span class="comment"># 需要将任务列表封装到wait中</span></span><br><span class="line">loop.run_until_complete(asyncio.wait(stasks))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(time.time()-start)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="⑥多任务异步协程02"><a href="#⑥多任务异步协程02" class="headerlink" title="⑥多任务异步协程02"></a>⑥多任务异步协程02</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> Flask</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">app = Flask(__name__)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&#x27;/john&#x27;</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">index_john</span>():</span><br><span class="line">    time.sleep(<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;Hello john&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&#x27;/smith&#x27;</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">index_smith</span>():</span><br><span class="line">    time.sleep(<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;Hello smith&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&#x27;/tom&#x27;</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">index_tom</span>():</span><br><span class="line">    time.sleep(<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;Hello tom&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    app.run(threaded=<span class="literal">True</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>异步协程实现代码（此处仍为单线）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">start = time.time()</span><br><span class="line">urls = [</span><br><span class="line">    <span class="string">&#x27;http://127.0.0.1:5000/john&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;http://127.0.0.1:5000/smith&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;http://127.0.0.1:5000/tom&#x27;</span>,</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">get_page</span>(<span class="params">url</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;正在下载&#x27;</span>, url)</span><br><span class="line">    <span class="comment"># requests.get是基于同步的，必须使用基于异步的网络请求模块进行指定url的请求发送</span></span><br><span class="line">    <span class="comment"># aiohttp:基于异步网络请求的模块</span></span><br><span class="line">    response = requests.get(url)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;下载完毕&#x27;</span>, response.text)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tasks = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> url <span class="keyword">in</span> urls:</span><br><span class="line">    c = get_page(url)</span><br><span class="line">    task = asyncio.ensure_future(c)</span><br><span class="line">    tasks.append(task)</span><br><span class="line"></span><br><span class="line">loop = asyncio.get_event_loop()</span><br><span class="line">loop.run_until_complete(asyncio.wait(tasks))</span><br><span class="line"></span><br><span class="line">end = time.time()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;总耗时:&#x27;</span>, end-start)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="⑦aiohttp实现多任务异步协程"><a href="#⑦aiohttp实现多任务异步协程" class="headerlink" title="⑦aiohttp实现多任务异步协程"></a>⑦aiohttp实现多任务异步协程</h4><ul>
<li>aiohttp:基于异步网络请求的模块</li>
<li>环境安装：pip install aiohttp</li>
<li>使用环境中的ClientSession</li>
<li>与requests模块的区别，调用的不是属性，而是方法：<ul>
<li>text()返回字符串形式的响应数据</li>
<li>read()返回二进制形式的响应数据</li>
<li>json()返回的是json对象</li>
</ul>
</li>
</ul>
<p>代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> aiohttp</span><br><span class="line"><span class="comment"># 环境安装：pip install aiohttp</span></span><br><span class="line"><span class="comment"># 使用环境中的ClientSession</span></span><br><span class="line">start = time.time()</span><br><span class="line">urls = [</span><br><span class="line">    <span class="string">&#x27;http://127.0.0.1:5000/john&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;http://127.0.0.1:5000/smith&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;http://127.0.0.1:5000/tom&#x27;</span>,</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">get_page</span>(<span class="params">url</span>):</span><br><span class="line">    <span class="keyword">async</span> <span class="keyword">with</span> aiohttp.ClientSession() <span class="keyword">as</span> session:</span><br><span class="line">        <span class="comment"># 使用不同的请求类型:</span></span><br><span class="line">        <span class="comment"># get()、post()</span></span><br><span class="line">        <span class="comment"># 添加相关的参数:</span></span><br><span class="line">        <span class="comment"># headers,params/data proxy=&#x27;http://ip:port&#x27;</span></span><br><span class="line">        <span class="comment"># 例如:</span></span><br><span class="line">        <span class="comment"># async with await session.get(url, params, proxy) as response:</span></span><br><span class="line">        <span class="comment"># async with await session.post(url, data, proxy) as response:</span></span><br><span class="line">        <span class="keyword">async</span> <span class="keyword">with</span> <span class="keyword">await</span> session.get(url) <span class="keyword">as</span> response:</span><br><span class="line">            <span class="comment"># text()返回字符串形式的响应数据</span></span><br><span class="line">            <span class="comment"># read()返回二进制形式的响应数据</span></span><br><span class="line">            <span class="comment"># json()返回的是json对象</span></span><br><span class="line">            <span class="comment"># 注意: 获取响应数据操作之前一定要使用await进行手动挂起</span></span><br><span class="line">            page_text = <span class="keyword">await</span> response.text()</span><br><span class="line">            <span class="built_in">print</span>(page_text)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tasks = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> url <span class="keyword">in</span> urls:</span><br><span class="line">    c = get_page(url)</span><br><span class="line">    task = asyncio.ensure_future(c)</span><br><span class="line">    tasks.append(task)</span><br><span class="line"></span><br><span class="line">loop = asyncio.get_event_loop()</span><br><span class="line">loop.run_until_complete(asyncio.wait(tasks))</span><br><span class="line"></span><br><span class="line">end = time.time()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;总耗时:&#x27;</span>, end-start)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="异步协程爬虫实战"><a href="#异步协程爬虫实战" class="headerlink" title="异步协程爬虫实战"></a>异步协程爬虫实战</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># http://dushu.baidu.com/api/pc/getCatalog?data=&#123;&quot;book_id&quot;:&quot;4306063500&quot;&#125; =&gt; 所有章节的内容(名称，cid)</span></span><br><span class="line"><span class="comment">#草节内部的内容</span></span><br><span class="line"><span class="comment"># http://dushu.baidu.com/api/pc/getChapterContent?data=&#123;&quot;book_id&quot;:&quot;4306063500,&quot;cid&quot;:&quot;4306063500|11348571，&quot;need_ bookinf&quot;:1&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">1.同步操作访问getCatalog方到所有章节的d和名称</span></span><br><span class="line"><span class="string">2.异步操作:访问getChapterContent 下载所有的文章内容</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">import</span> aiohttp</span><br><span class="line"><span class="keyword">import</span> aiofiles</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">aiodownload</span>(<span class="params">c_id,b_id,title</span>):</span><br><span class="line">	data = &#123;</span><br><span class="line">		<span class="string">&quot;book_id&quot;</span>:b_id,</span><br><span class="line">		<span class="string">&quot;cid&quot;</span>:<span class="string">f&quot;<span class="subst">&#123;b_id&#125;</span>|<span class="subst">&#123;c_id&#125;</span>&quot;</span>,</span><br><span class="line">		<span class="string">&quot;need_ bookinf&quot;</span>:<span class="number">1</span></span><br><span class="line">	&#125;</span><br><span class="line">	data = json.dumps(data) <span class="comment"># 变成json字符串</span></span><br><span class="line">	url = <span class="string">f&#x27;http://dushu.baidu.com/api/pc/getChapterContent?data=<span class="subst">&#123;data&#125;</span>&#x27;</span></span><br><span class="line">	<span class="keyword">async</span> <span class="keyword">with</span> aiohttp.ClientSession() <span class="keyword">as</span> session:</span><br><span class="line">		<span class="keyword">async</span> <span class="keyword">with</span> <span class="keyword">await</span> session.get(url) <span class="keyword">as</span> response:</span><br><span class="line">			dic = <span class="keyword">await</span> response.json()</span><br><span class="line">			</span><br><span class="line">			<span class="keyword">async</span> <span class="keyword">await</span> aiofiles.<span class="built_in">open</span>(title,mode=<span class="string">&quot;w&quot;</span>,encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">				<span class="keyword">await</span> f.write(dic[<span class="string">&#x27;data&#x27;</span>][<span class="string">&#x27;novel&#x27;</span>][<span class="string">&#x27;content&#x27;</span>]) <span class="comment"># 写入内容 </span></span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">getCatalog</span>(<span class="params">url</span>):</span><br><span class="line">	resp = requests.get(url)</span><br><span class="line">	dic = resp.json()</span><br><span class="line">	tasks = []</span><br><span class="line">	<span class="keyword">for</span> item <span class="keyword">in</span> dic[<span class="string">&#x27;data&#x27;</span>][<span class="string">&#x27;novel&#x27;</span>][<span class="string">&#x27;items&#x27;</span>]: <span class="comment"># 对应每一个章节的内容</span></span><br><span class="line">		title = item[<span class="string">&#x27;title&#x27;</span>]</span><br><span class="line">		c_id = item[<span class="string">&#x27;c_id&#x27;</span>]</span><br><span class="line">		<span class="comment"># 准备异步任务</span></span><br><span class="line">		tasks.append(asyncio.ensure_future(aiodownload(c_id,b_id,title)))</span><br><span class="line">	<span class="keyword">await</span> asyncio.wait(tasks)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">	b_id = <span class="string">&quot;4306063500&quot;</span></span><br><span class="line">	url = <span class="string">&#x27;http://dushu.baidu.com/api/pc/getCatalog?data=&#123;&quot;book_id&quot;:&quot;&#x27;</span>+ b_id +<span class="string">&#x27;&quot;&#125;&#x27;</span></span><br><span class="line">	asyncio.run(getGatalog(url))</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1 id="第七章-selenium"><a href="#第七章-selenium" class="headerlink" title="第七章 selenium"></a>第七章 selenium</h1><p>Selenium是一个自动化测试工具, 利用它可以驱动浏览器执行特定的动作, 如点击, 下拉等操作。<br>Selenium可以获取浏览器当前呈现的页面源代码，做到可见既可爬，对应JavaScript动态渲染的<br>信息爬取非常有效。但是缺点是速度较慢。</p>
<ul>
<li>环境的安装：pip install selenium</li>
<li>下载一个浏览器的驱动程序<ul>
<li>下载路径：<a target="_blank" rel="noopener" href="http://chromedriver.storage.googleapis.com/index.html">http://chromedriver.storage.googleapis.com/index.html</a></li>
<li>驱动程序和浏览器的映射关系：<a target="_blank" rel="noopener" href="https://blog.csdn.net/huilan_same/article/details/51896672">https://blog.csdn.net/huilan_same/article/details/51896672</a></li>
</ul>
</li>
<li>编写基于浏览器自动化的操作代码<ul>
<li>发起请求：get(url)</li>
<li>标签定位：find系列方法</li>
<li>标签交互：send_keys(‘***’)</li>
<li>执行js程序：excute_script(‘jsCode’)</li>
<li>前进：forward()</li>
<li>后退：back()</li>
<li>关闭浏览器：quit()</li>
</ul>
</li>
<li>selenium处理iframe<ul>
<li>如果定位的标签存在于iframe标签之中，则必须使用switch_to.frame(id)</li>
<li>动作链（拖动）：from selenium.webdriver import ActionChains # 破解滑动验证码的时候用的 可以拖动图片<ul>
<li>实例化一个动作链对象：action &#x3D; ActionChains(bro)</li>
<li>点击且长按操作：click_and_hold(div)</li>
<li>移动操作：move_by_offset(x, y) x-水平方向 y-竖直方向</li>
<li>让动作链立即执行：perform()</li>
<li>释放动作链对象：action.release()</li>
</ul>
</li>
<li>切换回原页面：switch_to.default_content()</li>
</ul>
</li>
</ul>
<h4 id="7-1-webdriver提供大量方法查询页面中的元素"><a href="#7-1-webdriver提供大量方法查询页面中的元素" class="headerlink" title="7.1 webdriver提供大量方法查询页面中的元素"></a>7.1 webdriver提供大量方法查询页面中的元素</h4><p>webdriver.find_element_by_*(查找第一个出现的)&#x2F;find_elements_by_*(查找全部):</p>
<pre><code>1. find_element_by_id
2. find_element_by_name
3. find_element_by_xpath
    使用XPath的主要原因:
        当你想获取唯一的但却没有id的元素时: 
            1.通过XPath使用元素的绝对位置来获取他(不推荐)
            2.相对于有一个id或name属性的元素(理论上的父元素)的来获取你想要的元素
            例如:
                &lt;html&gt;
                 &lt;body&gt;
                  &lt;form id=&quot;loginForm&quot;&gt;
                   &lt;input name=&quot;username&quot; type=&quot;text&quot; /&gt;
                  &lt;/form&gt;
                &lt;/body&gt;
                &lt;html&gt;
                username = driver.find_element_by_xpath(&quot;//form[input/@name=&#39;username&#39;]&quot;)
                username = driver.find_element_by_xpath(&quot;//form[@id=&#39;loginForm&#39;]/input[1]&quot;)
                username = driver.find_element_by_xpath(&quot;//input[@name=&#39;username&#39;]&quot;)
4. 通过链接里的文本获取超链接
    find_element_by_link_text  
    find_element_by_partial_link_text
    例如:
            &lt;a href=&quot;continue.html&quot;&gt;Continue&lt;/a&gt;
            continue_link = driver.find_element_by_link_text(&#39;Continue&#39;)
            #通过部分内容来查询
            continue_link = driver.find_element_by_partial_link_text(&#39;Conti&#39;)
5. find_element_by_tag_name
6. find_element_by_class_name
7. find_element_by_css_selector
    例如:
        &lt;p class=&quot;content&quot;&gt;Site content goes here.&lt;/p&gt;
        #获取p元素内的内容
        content = driver.find_element_by_css_selector(&#39;p.content&#39;)
</code></pre>
<h4 id="实战案例"><a href="#实战案例" class="headerlink" title="实战案例"></a>实战案例</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver <span class="keyword">import</span> Chrome</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.common.keys <span class="keyword">import</span> Keys </span><br><span class="line"></span><br><span class="line">web = Chrome( )</span><br><span class="line">web.get(<span class="string">&quot;http://Lagou.com&quot;</span> )</span><br><span class="line">web.find_element_by_xpath(<span class="string">&#x27;//*[@id=&quot;cboxCLose&quot;]&#x27;</span>).click()</span><br><span class="line">time.sleep(<span class="number">1</span>) <span class="comment"># 每次执行停1s防止浏览器动态加载速度慢于代码执行速度</span></span><br><span class="line"></span><br><span class="line">web.find_element_by_xpath(<span class="string">&#x27;//*[@id=&quot;search_input&quot;]&#x27;</span>).send_keys(<span class="string">&quot;python&quot;</span>,Keys.ENTER)</span><br><span class="line">time.sleep(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">web.find_element_by_xpath(<span class="string">&#x27;//*[@id=&quot;s_position_List&quot;]/ul/Li[1]/div[1]/div[1]/div[1]/a/h3&#x27;</span> ).click()</span><br><span class="line"></span><br><span class="line"><span class="comment">#如何进入到进窗口中进行提取</span></span><br><span class="line"><span class="comment">#注意,在selenium的眼中,新窗口默认是不切换过来的.</span></span><br><span class="line">web.switch_to.window(web.window_ handles[-<span class="number">1</span>])</span><br><span class="line"><span class="comment">#在新窗口中提取内容</span></span><br><span class="line">job_detail = web.find_element_by_xpath(<span class="string">&#x27;//*[@id=&quot;job_detail&quot;]/dd[2]/div&#x27;</span>).text</span><br><span class="line"></span><br><span class="line"><span class="comment"># 关闭子窗口</span></span><br><span class="line">web.close()</span><br><span class="line"><span class="comment"># 变更selenium窗口视角，回到原窗口</span></span><br><span class="line">web.switch_to.window(web.window_handles[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="7-2-动作链和-iframe-操作"><a href="#7-2-动作链和-iframe-操作" class="headerlink" title="7.2 动作链和 iframe 操作"></a>7.2 动作链和 iframe 操作</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> sleep</span><br><span class="line"><span class="comment"># 导入动作链对应的类</span></span><br><span class="line"><span class="keyword">from</span> selenium.webdriver <span class="keyword">import</span> ActionChains</span><br><span class="line"></span><br><span class="line">bro = webdriver.Chrome()</span><br><span class="line"></span><br><span class="line">bro.get(<span class="string">&#x27;https://www.runoob.com/try/try.php?filename=jqueryui-api-droppable&#x27;</span>)</span><br><span class="line"><span class="comment"># 如果定位的标签是存在于iframe标签之中的，必须进行如下操作再进行标签定位</span></span><br><span class="line">bro.switch_to.frame(<span class="string">&#x27;iframeResult&#x27;</span>)     <span class="comment"># 切换浏览器标签的作用域</span></span><br><span class="line">div = bro.find_element_by_id(<span class="string">&#x27;draggable&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 动作链</span></span><br><span class="line">action = ActionChains(bro)</span><br><span class="line"><span class="comment"># 点击长按指定的标签</span></span><br><span class="line">action.click_and_hold(div)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">    <span class="comment"># perform()立即执行动作链操作</span></span><br><span class="line">    <span class="comment"># move_by_offset(x, y): x-水平方向    y-竖直方向</span></span><br><span class="line">    action.move_by_offset(<span class="number">17</span>, <span class="number">0</span>).perform()</span><br><span class="line">    sleep(<span class="number">0.3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 释放动作链</span></span><br><span class="line">action.release()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(div)</span><br><span class="line"></span><br><span class="line">bro.quit()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="7-2-无头浏览器"><a href="#7-2-无头浏览器" class="headerlink" title="7.2 无头浏览器"></a>7.2 无头浏览器</h4><p>是一个基于Webkit的“无界面”(headless)浏览器,它会把网站加载到内存并执行页面上的JavaScript,<br>因为不会展示图形界面,所以运行起来比完整的浏览器要高效<br>对于其他浏览器可以通过</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> sleep</span><br><span class="line"><span class="comment"># 实现无可视化界面</span></span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.chrome.options <span class="keyword">import</span> Options</span><br><span class="line"><span class="comment"># 实现规避检测</span></span><br><span class="line"><span class="keyword">from</span> selenium.webdriver <span class="keyword">import</span> ChromeOptions</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实现无可视化界面的操作</span></span><br><span class="line">chrome_options = Options()</span><br><span class="line">chrome_options.add_argument(<span class="string">&#x27;--headless&#x27;</span>)</span><br><span class="line">chrome_options.add_argument(<span class="string">&#x27;--disable-gpu&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实现规避检测</span></span><br><span class="line">option = ChromeOptions()</span><br><span class="line">option.add_experimental_option(<span class="string">&#x27;excludeSwitches&#x27;</span>, [<span class="string">&#x27;enable-automation&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如何实现让selenium规避被检测的风险</span></span><br><span class="line">bro = webdriver.Chrome(chrome_options=chrome_options, options=option)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 无可视化界面(无头浏览器)     phantomJs</span></span><br><span class="line"><span class="comment"># webdriver.phantomjs</span></span><br><span class="line"><span class="comment"># 不建议用phantomJs，因为它已经停止更新与维护</span></span><br><span class="line">bro.get(<span class="string">&#x27;https://www.baidu.com/&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(bro.page_source) <span class="comment"># 如何拿到页面代码(经过数据加载以及js执行之后的结果的html内容)</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="12306模拟登录"><a href="#12306模拟登录" class="headerlink" title="12306模拟登录"></a>12306模拟登录</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver <span class="keyword">import</span> ActionChains</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Chaojiying_Client</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, username, password, soft_id</span>):</span><br><span class="line">        self.username = username</span><br><span class="line">        password = password.encode(<span class="string">&#x27;utf8&#x27;</span>)</span><br><span class="line">        self.password = md5(password).hexdigest()</span><br><span class="line">        self.soft_id = soft_id</span><br><span class="line">        self.base_params = &#123;</span><br><span class="line">            <span class="string">&#x27;user&#x27;</span>: self.username,</span><br><span class="line">            <span class="string">&#x27;pass2&#x27;</span>: self.password,</span><br><span class="line">            <span class="string">&#x27;softid&#x27;</span>: self.soft_id,</span><br><span class="line">        &#125;</span><br><span class="line">        self.headers = &#123;</span><br><span class="line">            <span class="string">&#x27;Connection&#x27;</span>: <span class="string">&#x27;Keep-Alive&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0)&#x27;</span>,</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">PostPic</span>(<span class="params">self, im, codetype</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        im: 图片字节</span></span><br><span class="line"><span class="string">        codetype: 题目类型 参考 http://www.chaojiying.com/price.html</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        params = &#123;</span><br><span class="line">            <span class="string">&#x27;codetype&#x27;</span>: codetype,</span><br><span class="line">        &#125;</span><br><span class="line">        params.update(self.base_params)</span><br><span class="line">        files = &#123;<span class="string">&#x27;userfile&#x27;</span>: (<span class="string">&#x27;ccc.jpg&#x27;</span>, im)&#125;</span><br><span class="line">        r = requests.post(<span class="string">&#x27;http://upload.chaojiying.net/Upload/Processing.php&#x27;</span>, data=params, files=files, headers=self.headers)</span><br><span class="line">        <span class="keyword">return</span> r.json()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">ReportError</span>(<span class="params">self, im_id</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        im_id:报错题目的图片ID</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        params = &#123;</span><br><span class="line">            <span class="string">&#x27;id&#x27;</span>: im_id,</span><br><span class="line">        &#125;</span><br><span class="line">        params.update(self.base_params)</span><br><span class="line">        r = requests.post(<span class="string">&#x27;http://upload.chaojiying.net/Upload/ReportError.php&#x27;</span>, data=params, headers=self.headers)</span><br><span class="line">        <span class="keyword">return</span> r.json()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用selenium打开登录页面</span></span><br><span class="line">    bro = webdriver.Chrome()</span><br><span class="line">    bro.get(<span class="string">&#x27;https://kyfw.12306.cn/otn/resources/login.html&#x27;</span>)</span><br><span class="line">    time.sleep(<span class="number">1</span>)</span><br><span class="line">	chaojiying = Chaojiying_Client(<span class="string">&#x27;zhangyi132&#x27;</span>, <span class="string">&#x27;zhangjinpeng101&#x27;</span>, <span class="string">&#x27;920271&#x27;</span>)</span><br><span class="line">    btn = bro.find_element_by_xpath(<span class="string">&#x27;/html/body/div[2]/div[2]/ul/li[2]/a&#x27;</span>)</span><br><span class="line">    btn.click()</span><br><span class="line">    time.sleep(<span class="number">3</span>)</span><br><span class="line">	im_element = bro.find_element_by_xpath(<span class="string">&#x27;//*[@id=&quot;J-loginImg&quot;]&#x27;</span>)</span><br><span class="line">    im = im_element.screenshot_as_png</span><br><span class="line">    dic = choajiying.PostPic(im,<span class="number">9004</span>)</span><br><span class="line">    result = dic[<span class="string">&#x27;pic_str&#x27;</span>]</span><br><span class="line">    res_ls = result.split(<span class="string">&#x27;|&#x27;</span>)</span><br><span class="line">   </span><br><span class="line">    <span class="comment"># 遍历列表，使用动作链对每一个列表元素对应的x,y指定的位置进行点击操作</span></span><br><span class="line">    <span class="keyword">for</span> res <span class="keyword">in</span> res_ls:</span><br><span class="line">    	k = res.split(<span class="string">&quot;,&quot;</span>)</span><br><span class="line">    	x,y = <span class="built_in">int</span>(k[<span class="number">0</span>]),<span class="built_in">int</span>(k[<span class="number">1</span>])</span><br><span class="line">        ActionChains(bro).move_to_element_with_offset(im_element, x, y).click().perform()</span><br><span class="line">        time.sleep(<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">    bro.find_element_by_id(<span class="string">&#x27;username&#x27;</span>).send_keys(<span class="string">&#x27;******&#x27;</span>)</span><br><span class="line">    time.sleep(<span class="number">2</span>)</span><br><span class="line">    bro.find_element_by_id(<span class="string">&#x27;password&#x27;</span>).send_keys(<span class="string">&#x27;******&#x27;</span>)</span><br><span class="line">    time.sleep(<span class="number">2</span>)</span><br><span class="line">    bro.find_element_by_id(<span class="string">&#x27;loginSub&#x27;</span>).click()</span><br><span class="line">    time.sleep(<span class="number">5</span>)</span><br><span class="line">    <span class="comment"># 拖拽</span></span><br><span class="line">    btn = bro.find_element_by_xpath(<span class="string">&#x27;//*[@id=&quot;nc_1n1z&quot;]&#x27;</span>)</span><br><span class="line">	ActionChains(bro).drag_and_drop_by_offset(btn, <span class="number">300</span>, <span class="number">0</span>).perform()</span><br><span class="line"></span><br><span class="line">    bro.quit()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="7-3-反反爬"><a href="#7-3-反反爬" class="headerlink" title="7.3 反反爬"></a>7.3 反反爬</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#如果你的程序被识别到了怎么办?</span></span><br><span class="line"><span class="comment"># 1. chrome的版本号如果小于88</span></span><br><span class="line"><span class="comment"># 在你启动浏览器的时候(此时没有加载任何网页内容)，向页面嵌入js代码。去掉webdriver</span></span><br><span class="line"><span class="comment"># web = Chrome( )</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># web. execute_ cdp_ cmd (&quot;Page . addScriptToEva LuateOnNewDocument&quot;, &#123;</span></span><br><span class="line"><span class="comment"># &quot;source&quot; :&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># navigator.webdriver = undefined</span></span><br><span class="line"><span class="comment"># object.defineProperty(navigator, &#x27;webdriver&#x27;, &#123;</span></span><br><span class="line"><span class="comment">#	get:() =&gt; undefined </span></span><br><span class="line"><span class="comment"># &#125;)</span></span><br><span class="line"><span class="comment"># &quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># &#125;)</span></span><br><span class="line"><span class="comment"># web.get( XXXXXXX )</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. chrome版本大于 88</span></span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.chrome.options <span class="keyword">import</span> Options</span><br><span class="line">option = Options()</span><br><span class="line"><span class="comment"># option.add_experimental_option( &#x27;excludeSwitches&#x27;, [&#x27;enable-automation&#x27;])</span></span><br><span class="line">option.add_argument( <span class="string">&#x27;--disable-blink-features=AutomationControlled&#x27;</span> )</span><br><span class="line">web = Chrome(options=option)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1 id="第八章-验证码识别"><a href="#第八章-验证码识别" class="headerlink" title="第八章 验证码识别"></a>第八章 验证码识别</h1><p>实战：识别古诗文网登录页面中的验证码。</p>
<ul>
<li>使用打码平台识别验证码的编码流程：<ul>
<li>将验证码图片进行本地下载</li>
<li>调用平台提供的示例代码进行图片数据识别<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> html</span><br><span class="line">etree = html.etree</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> hashlib <span class="keyword">import</span> md5</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Chaojiying_Client</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, username, password, soft_id</span>):</span><br><span class="line">        self.username = username</span><br><span class="line">        password = password.encode(<span class="string">&#x27;utf8&#x27;</span>)</span><br><span class="line">        self.password = md5(password).hexdigest()</span><br><span class="line">        self.soft_id = soft_id</span><br><span class="line">        self.base_params = &#123;</span><br><span class="line">            <span class="string">&#x27;user&#x27;</span>: self.username,</span><br><span class="line">            <span class="string">&#x27;pass2&#x27;</span>: self.password,</span><br><span class="line">            <span class="string">&#x27;softid&#x27;</span>: self.soft_id,</span><br><span class="line">        &#125;</span><br><span class="line">        self.headers = &#123;</span><br><span class="line">            <span class="string">&#x27;Connection&#x27;</span>: <span class="string">&#x27;Keep-Alive&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0)&#x27;</span>,</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">PostPic</span>(<span class="params">self, im, codetype</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        im: 图片字节</span></span><br><span class="line"><span class="string">        codetype: 题目类型 参考 http://www.chaojiying.com/price.html</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        params = &#123;</span><br><span class="line">            <span class="string">&#x27;codetype&#x27;</span>: codetype,</span><br><span class="line">        &#125;</span><br><span class="line">        params.update(self.base_params)</span><br><span class="line">        files = &#123;<span class="string">&#x27;userfile&#x27;</span>: (<span class="string">&#x27;ccc.jpg&#x27;</span>, im)&#125;</span><br><span class="line">        r = requests.post(<span class="string">&#x27;http://upload.chaojiying.net/Upload/Processing.php&#x27;</span>, data=params, files=files, headers=self.headers)</span><br><span class="line">        <span class="keyword">return</span> r.json()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">ReportError</span>(<span class="params">self, im_id</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        im_id:报错题目的图片ID</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        params = &#123;</span><br><span class="line">            <span class="string">&#x27;id&#x27;</span>: im_id,</span><br><span class="line">        &#125;</span><br><span class="line">        params.update(self.base_params)</span><br><span class="line">        r = requests.post(<span class="string">&#x27;http://upload.chaojiying.net/Upload/ReportError.php&#x27;</span>, data=params, headers=self.headers)</span><br><span class="line">        <span class="keyword">return</span> r.json()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># 将验证码图片下载到本地</span></span><br><span class="line"></span><br><span class="line">    url = <span class="string">&#x27;https://so.gushiwen.cn/user/login.aspx?from=http://so.gushiwen.cn/user/collect.aspx&#x27;</span></span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36&#x27;</span></span><br><span class="line">    &#125;</span><br><span class="line">    page_text = requests.get(url=url, headers=headers).text</span><br><span class="line">    <span class="comment"># 解析验证码图片img中src属性值</span></span><br><span class="line">    tree = etree.HTML(page_text)</span><br><span class="line">    code_img_src = <span class="string">&#x27;https://so.gushiwen.cn&#x27;</span> + tree.xpath(<span class="string">&#x27;//*[@id=&quot;imgCode&quot;]/@src&#x27;</span>)[<span class="number">0</span>]</span><br><span class="line">    img_data = requests.get(url=code_img_src, headers=headers).content</span><br><span class="line">    <span class="comment"># 将验证码图片保存到本地</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;./code.jpg&#x27;</span>, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">        fp.write(img_data)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 调用打码平台的示例程序进行验证码图片数据识别</span></span><br><span class="line">    chaojiying = Chaojiying_Client(<span class="string">&#x27;你的账号qwq&#x27;</span>, <span class="string">&#x27;你的密码（不给你咱的）&#x27;</span>, <span class="string">&#x27;96001&#x27;</span>)</span><br><span class="line">    <span class="comment"># 用户中心&gt;&gt;软件ID 生成一个替换 96001</span></span><br><span class="line">    im = <span class="built_in">open</span>(<span class="string">&#x27;code.jpg&#x27;</span>, <span class="string">&#x27;rb&#x27;</span>).read()</span><br><span class="line">    <span class="comment"># 本地图片文件路径 来替换 a.jpg 有时WIN系统须要//</span></span><br><span class="line">    <span class="built_in">print</span>(chaojiying.PostPic(im, <span class="number">1902</span>))</span><br><span class="line">    <span class="comment"># 1902 验证码类型  </span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
<h1 id="第九章-scrapy-框架"><a href="#第九章-scrapy-框架" class="headerlink" title="第九章 scrapy 框架"></a>第九章 scrapy 框架</h1><h4 id="9-1-scrapy框架的基本使用"><a href="#9-1-scrapy框架的基本使用" class="headerlink" title="9.1 scrapy框架的基本使用"></a>9.1 scrapy框架的基本使用</h4><ul>
<li>环境安装：pip intall scrapy</li>
<li>基本使用：<ol>
<li>创建一个工程： scrapy startproject xxx</li>
<li>cd xxx</li>
<li>在spiders子目录中创建一个爬虫文件<br> scrapy genspider spiderName <a target="_blank" rel="noopener" href="http://www.xxx.com(随便指定的url)/">www.xxx.com(随便指定的url)</a></li>
</ol>
</li>
<li>需要暂时修改的信息：<br>settings.py中修改并添加如下代码<ol>
<li>ROBOTSTXT_OBEY的值由True改为False（robots协议，详见第一章）</li>
<li>添加指定代码，使相应数据中不输出正常的日志刷屏 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Obey robots.txt rules</span></span><br><span class="line">ROBOTSTXT_OBEY = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示指定类型的日志信息</span></span><br><span class="line">LOG_LEVEL = <span class="string">&#x27;ERROR&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># UA 伪装</span></span><br><span class="line">USER_AGENT = <span class="string">&#x27;Mozilla/5.0&#x27;</span></span><br></pre></td></tr></table></figure></li>
<li>执行工程： scrapy crawl spiderName</li>
</ol>
</li>
</ul>
<p>spiders子目录中创建的爬虫文件分析：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">FirstSpider</span>(scrapy.Spider):</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 爬虫文件名称：爬虫源文件的唯一标识</span></span><br><span class="line">    name = <span class="string">&#x27;first&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 允许的域名：用来限定 start_urls 列表中哪些 url 可以进行请求发送</span></span><br><span class="line">    <span class="comment"># allowed_domains = [&#x27;www.xxx.com&#x27;]       # 通常不用(例如爬取图片时图片的url就不会被访问到，因为图片的域名可能和网站域名不一致)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 起始的url列表：该列表中存放的url会被scrapy自动进行的请求发送</span></span><br><span class="line">    start_urls = [<span class="string">&#x27;http://www.xxx.com/&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 用作于数据解析：response 参数表示的是请求成功后对应的响应对象</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>): </span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
<h4 id="9-2-scrapy-数据解析"><a href="#9-2-scrapy-数据解析" class="headerlink" title="9.2 scrapy 数据解析"></a>9.2 scrapy 数据解析</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">QiubaiSpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&#x27;qiubai&#x27;</span></span><br><span class="line">    <span class="comment"># allowed_domains = [&#x27;www.xxx.com&#x27;]</span></span><br><span class="line">    start_urls = [<span class="string">&#x27;http://www.qiushibaike.com/text/&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">        <span class="comment"># 解析：作者的名称&amp;段子的内容</span></span><br><span class="line">        div_list = response.xpath(<span class="string">&#x27;//div[@class=&quot;col1 old-style-col1&quot;]/div&#x27;</span>)</span><br><span class="line">        <span class="comment"># print(div_list)</span></span><br><span class="line">        <span class="keyword">for</span> div <span class="keyword">in</span> div_list:</span><br><span class="line">            <span class="comment"># xpath 返回的是列表，但列表元素一定是 selector 类型的对象</span></span><br><span class="line">            <span class="comment"># extract 可以将 selector 对象中 data 参数存储的字符串提取出来</span></span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">            	author = div.xpath(<span class="string">&#x27;./div[1]/a[2]/h2/text()&#x27;</span>)[<span class="number">0</span>].extract()</span><br><span class="line">            <span class="keyword">except</span>:</span><br><span class="line">            	author = <span class="string">&#x27;匿名用户&#x27;</span></span><br><span class="line">            <span class="comment"># 列表中只有一个元素时可以调用以下代码来替代上面的一行</span></span><br><span class="line">            <span class="comment"># author = div.xpath(&#x27;./div[1]/a[2]/h2/text()&#x27;).extract_first()</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 列表调用了 extract 之后，则表示将列表中每一个 selector 对象中 data 对应的字符串提取了出来</span></span><br><span class="line">            content = div.xpath(<span class="string">&#x27;./a[1]/div/span//text()&#x27;</span>).extract()</span><br><span class="line">            content = <span class="string">&#x27;&#x27;</span>.join(content)</span><br><span class="line">            <span class="built_in">print</span>(author, content)</span><br><span class="line">            <span class="keyword">break</span></span><br></pre></td></tr></table></figure>

<h4 id="9-3-scrapy-持久化存储"><a href="#9-3-scrapy-持久化存储" class="headerlink" title="9.3 scrapy 持久化存储"></a>9.3 scrapy 持久化存储</h4><h4 id="Ⅰ知识点"><a href="#Ⅰ知识点" class="headerlink" title="Ⅰ知识点"></a>Ⅰ知识点</h4><ul>
<li>基于终端指令：<ul>
<li>要求：只可以将parse方法的返回值存储到本地的文本文件中</li>
<li>注意：持久化存储对应的文本文件的类型只可以为：‘json’, ‘jsonlines’, ‘jl’, ‘csv’, ‘xml’, ‘marshal’, ‘pickle’   (没有’txt’)</li>
<li>指令：scrapy crawl xxx -o filePath</li>
<li>好处：简洁高效便捷</li>
<li>缺点：局限性比较强（数据只可以存储到指定后缀的文本文件中）</li>
</ul>
</li>
<li>基于管道：<ul>
<li>编码流程：<ol>
<li>数据解析</li>
<li>在 item 类中定义相关的属性</li>
<li>将解析的数据存储到 item 类型的对象</li>
<li>将 item 类型的对象提交给管道进行持久化存储的操作</li>
<li>在管道类(pipelines)的 process_item 中要将其接收到的 item 对象中存储的数据进行持久化存储操作</li>
<li>在配置文件中开启管道</li>
</ol>
</li>
<li>好处：<br>  通用性强</li>
</ul>
</li>
</ul>
<h4 id="Ⅱ代码部分"><a href="#Ⅱ代码部分" class="headerlink" title="Ⅱ代码部分"></a>Ⅱ代码部分</h4><p>①基于终端指令<br>代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">QiubaiSpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&#x27;qiubai&#x27;</span></span><br><span class="line">    <span class="comment"># allowed_domains = [&#x27;www.xxx.com&#x27;]</span></span><br><span class="line">    start_urls = [<span class="string">&#x27;http://www.qiushibaike.com/text/&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">        <span class="comment"># 解析：作者的名称&amp;段子的内容</span></span><br><span class="line">        div_list = response.xpath(<span class="string">&#x27;//div[@class=&quot;col1 old-style-col1&quot;]/div&#x27;</span>)</span><br><span class="line">        all_data = []       <span class="comment"># 存储所有解析到的数据</span></span><br><span class="line">        <span class="comment"># print(div_list)</span></span><br><span class="line">        <span class="keyword">for</span> div <span class="keyword">in</span> div_list:</span><br><span class="line">            <span class="comment"># xpath返回的是列表，但列表元素一定是selector类型的对象</span></span><br><span class="line">            <span class="comment"># extract可以将selector对象中data参数存储的字符串提取出来</span></span><br><span class="line">            author = div.xpath(<span class="string">&#x27;./div[1]/a[2]/h2/text()&#x27;</span>)[<span class="number">0</span>].extract()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 列表中只有一个元素时可以调用以下代码来替代上面的一行</span></span><br><span class="line">            <span class="comment"># author = div.xpath(&#x27;./div[1]/a[2]/h2/text()&#x27;).extract_first()</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 列表调用了extract之后，则表示将列表中每一个selector对象中data对应的字符串提取了出来</span></span><br><span class="line">            content = div.xpath(<span class="string">&#x27;./a[1]/div/span//text()&#x27;</span>).extract()</span><br><span class="line">            content = <span class="string">&#x27;&#x27;</span>.join(content)</span><br><span class="line"></span><br><span class="line">            dic = &#123;</span><br><span class="line">                <span class="string">&#x27;author&#x27;</span>: author,</span><br><span class="line">                <span class="string">&#x27;content&#x27;</span>: content</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            all_data.append(dic)</span><br><span class="line">            <span class="comment"># print(author, content)</span></span><br><span class="line">            <span class="comment"># break</span></span><br><span class="line">        <span class="keyword">return</span> all_data</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>②基于管道<br>代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="comment"># from qiubaiPro.items import QiubaiproItem</span></span><br><span class="line"><span class="keyword">from</span> ..items <span class="keyword">import</span> QiubaiproItem</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">QiubaiSpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&#x27;qiubai&#x27;</span></span><br><span class="line">    <span class="comment"># allowed_domains = [&#x27;www.xxx.com&#x27;]</span></span><br><span class="line">    start_urls = [<span class="string">&#x27;http://www.qiushibaike.com/text/&#x27;</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># -----基于管道-----</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">        <span class="comment"># 解析：作者的名称&amp;段子的内容</span></span><br><span class="line">        div_list = response.xpath(<span class="string">&#x27;//div[@class=&quot;col1 old-style-col1&quot;]/div&#x27;</span>)</span><br><span class="line">        all_data = []       <span class="comment"># 存储所有解析到的数据</span></span><br><span class="line">        <span class="comment"># print(div_list)</span></span><br><span class="line">        <span class="keyword">for</span> div <span class="keyword">in</span> div_list:</span><br><span class="line">            <span class="comment"># xpath返回的是列表，但列表元素一定是selector类型的对象</span></span><br><span class="line">            <span class="comment"># extract可以将selector对象中data参数存储的字符串提取出来</span></span><br><span class="line">            <span class="comment"># | : 正常用户的xpath 和匿名用户的 xpath</span></span><br><span class="line">            author = div.xpath(<span class="string">&#x27;./div[1]/a[2]/h2/text() | ./div[1]/span/h2/text()&#x27;</span>)[<span class="number">0</span>].extract()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 列表调用了extract之后，则表示将列表中每一个selector对象中data对应的字符串提取了出来</span></span><br><span class="line">            content = div.xpath(<span class="string">&#x27;./a[1]/div/span//text()&#x27;</span>).extract()</span><br><span class="line">            content = <span class="string">&#x27;&#x27;</span>.join(content)</span><br><span class="line"></span><br><span class="line">            item = QiubaiproItem() </span><br><span class="line">            item[<span class="string">&#x27;author&#x27;</span>] = author</span><br><span class="line">            item[<span class="string">&#x27;content&#x27;</span>] = content</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 将ite提交给了管道</span></span><br><span class="line">            <span class="keyword">yield</span> item</span><br></pre></td></tr></table></figure>
<h4 id="Ⅲ面试题"><a href="#Ⅲ面试题" class="headerlink" title="Ⅲ面试题"></a>Ⅲ面试题</h4><p>面试题：将爬取到的数据一份存储到本地，一份存储到数据库，如何实现？</p>
<ul>
<li>管道文件中的一个管道类对应的是将数据存储到一种平台</li>
<li>爬虫文件提交的item只会给管道文件中第一个被执行的管道类接受</li>
<li>process_item中的return item表示将item传递给下一个即将被执行的管道类</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Define your item pipelines here</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Don&#x27;t forget to add your pipeline to the ITEM_PIPELINES setting</span></span><br><span class="line"><span class="comment"># See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># useful for handling different item types with a single interface</span></span><br><span class="line"><span class="keyword">from</span> itemadapter <span class="keyword">import</span> ItemAdapter</span><br><span class="line"><span class="keyword">import</span> pymysql</span><br><span class="line"><span class="comment"># 存到本地</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">QiubaikePipeline</span>:</span><br><span class="line">    fp = <span class="literal">None</span></span><br><span class="line">    <span class="comment"># 重新父类方法，该方法只在开始爬虫时被调用一次</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">open_spider</span>(<span class="params">self,spider</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;开始爬虫&quot;</span>)</span><br><span class="line">        self.fp = <span class="built_in">open</span>(<span class="string">&#x27;qiubai.txt&#x27;</span>,<span class="string">&#x27;w&#x27;</span>,encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">close_spider</span>(<span class="params">self,spider</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;结束爬虫&quot;</span>)</span><br><span class="line">        self.fp.close()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 专门用来处理item类型对象</span></span><br><span class="line">    <span class="comment"># 该方法可以接收爬虫文件提交过来的item对象</span></span><br><span class="line">    <span class="comment"># 该方法每接收一个item就会被调用一次</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_item</span>(<span class="params">self, item, spider</span>):</span><br><span class="line">        author = item[<span class="string">&#x27;author&#x27;</span>]</span><br><span class="line">        content = item[<span class="string">&#x27;content&#x27;</span>]</span><br><span class="line">        self.fp.write(author+<span class="string">&#x27;:&#x27;</span>+content+<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> item <span class="comment">#就会传递给下一个即将被执行的管道类</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 管道文件中一个管道类对应将一组 数据存储到一个平台或者载体中</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MysqlPipeline</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    conn = <span class="literal">None</span></span><br><span class="line">    cursor = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">open_spider</span>(<span class="params">self,spider</span>):</span><br><span class="line">        self.conn = pymysql.connect(host=<span class="string">&quot;localhost&quot;</span>,port=<span class="number">3306</span>,user=<span class="string">&#x27;root&#x27;</span>,password=<span class="string">&#x27;123456&#x27;</span>,db=<span class="string">&#x27;test&#x27;</span>,charset=<span class="string">&quot;utf8&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;开始爬虫&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">close_spider</span>(<span class="params">self,spider</span>):</span><br><span class="line">        self.cursor.close()</span><br><span class="line">        self.conn.close()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;结束爬虫&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_item</span>(<span class="params">self, item, spider</span>):</span><br><span class="line">        self.cursor = self.conn.cursor()</span><br><span class="line">        <span class="keyword">try</span>: <span class="comment"># 无异常事务提交</span></span><br><span class="line">            self.cursor.execute(<span class="string">&#x27;insert into qiubaike values(&quot;%s&quot;,&quot;%s&quot;)&#x27;</span>%(item[<span class="string">&quot;author&quot;</span>],item[<span class="string">&quot;content&quot;</span>]))</span><br><span class="line">            self.conn.commit()</span><br><span class="line">        <span class="keyword">except</span> Exception <span class="keyword">as</span> e: <span class="comment"># 有异常回滚</span></span><br><span class="line">            <span class="built_in">print</span>(e)</span><br><span class="line">            self.conn.rollback()</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="9-4基于是spider的全站数据爬取"><a href="#9-4基于是spider的全站数据爬取" class="headerlink" title="9.4基于是spider的全站数据爬取"></a>9.4基于是spider的全站数据爬取</h4><ul>
<li>基于spider的全站数据爬取<ul>
<li>就是将网站中某板块下的全部页码对应的页面数据进行爬取和解析</li>
<li>需求：爬取校花网中的照片的名称</li>
<li>实现方式：<ol>
<li>将所有页面的url添加到start_urls列表（不推荐）</li>
<li>自行手动请求发送（推荐）<ul>
<li>手动请求发送：<ul>
<li>yield scrapy.Request(url, callback)：callback专门用于数据解析</li>
</ul>
</li>
</ul>
</li>
</ol>
</li>
</ul>
</li>
</ul>
<p>样例代码如下：</p>
<ul>
<li>创建工程：scrapy startproject xiaohuaPro</li>
<li>创建文件：scrapy genspider xiaohua <a target="_blank" rel="noopener" href="http://www.xxx.com/">www.xxx.com</a></li>
<li>修改设置：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">USER_AGENT = <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Obey robots.txt rules</span></span><br><span class="line">ROBOTSTXT_OBEY = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">LOG_LEVEL = <span class="string">&#x27;ERROR&#x27;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>编写代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">XiaohuaSpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&#x27;xiaohua&#x27;</span></span><br><span class="line">    <span class="comment"># allowed_domains = [&#x27;www.xxx.com&#x27;]</span></span><br><span class="line">    start_urls = [<span class="string">&#x27;http://www.521609.com/meinvxiaohua/&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 生成一个通用的url模板（不可变）</span></span><br><span class="line">    url = <span class="string">&#x27;http://www.521609.com/meinvxiaohua/list12%d.html&#x27;</span></span><br><span class="line">    page_num = <span class="number">2</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">        li_list = response.xpath(<span class="string">&#x27;/html/body/div[4]/div[2]/div[2]/ul/li&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> li <span class="keyword">in</span> li_list:</span><br><span class="line">            img_name = li.xpath(<span class="string">&#x27;./a[2]/b/text() | ./a[2]/text()&#x27;</span>).extract_first()</span><br><span class="line">            <span class="built_in">print</span>(img_name)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.page_num &lt;= <span class="number">11</span>:</span><br><span class="line">            new_url = <span class="built_in">format</span>(self.url % self.page_num)</span><br><span class="line">            self.page_num += <span class="number">1</span></span><br><span class="line">            <span class="comment"># 手动请求发送:callback回调函数是专门用于数据解析</span></span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(url=new_url, callback=self.parse)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="9-5-五大核心组件"><a href="#9-5-五大核心组件" class="headerlink" title="9.5 五大核心组件"></a>9.5 五大核心组件</h4><ul>
<li>爬虫(Spiders)<ul>
<li>爬虫是主要干活的,用于从特定的网页中提取自己需要的信息即所谓的实体(Item).用户也可以从中提取出链接让Scrapy继续抓取下一个页面</li>
</ul>
</li>
<li>引擎(Engine)<ul>
<li>用来处理整个系统的数据流处理触发事务(框架核心)</li>
</ul>
</li>
<li>调度器(Scheduler)<ul>
<li>用来接受引擎发过来的请求,压入队列中,并在引擎再次请求的时候返回.可以想像成一个URL (抓取网页的网址或者说是链接)的优先队列，由它来决定下一个要抓取的网址是什么，同时去除重复的网址</li>
</ul>
</li>
<li>下载器(Downloader)<ul>
<li>用于下载网页内容,并将网页内容返回给蜘蛛(Scrapy下载器是建立在twisted这个高效的异步模型上的)</li>
</ul>
</li>
<li>项目管道(Pipeline)<ul>
<li>负责处理爬虫从网页中抽取的实体, 主要的功能是持久化实体、验证实体的有效性、清除不需要的信息。当页面被爬虫解析后,将被发送到项目管道,并经过几个特定的次序处理数据。</li>
</ul>
</li>
</ul>
<p><img src="https://img-blog.csdnimg.cn/551c5d4f759343dc84d6fe77b52e114c.png" alt="在这里插入图片描述"></p>
<h4 id="9-6-请求传参"><a href="#9-6-请求传参" class="headerlink" title="9.6 请求传参"></a>9.6 请求传参</h4><p>使用场景：如果要爬取的数据不在同一张页面中。（深度爬取）<br>需求：爬取boss的岗位名称，岗位描述（原网站似乎改为动态加载数据，代码不可用，仅供参考）<br>代码：</p>
<ul>
<li>settings.py<br>常规操作：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Crawl responsibly by identifying yourself (and your website) on the user-agent</span></span><br><span class="line">USER_AGENT = <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Obey robots.txt rules</span></span><br><span class="line">ROBOTSTXT_OBEY = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">LOG_LEVEL = <span class="string">&#x27;ERROR&#x27;</span></span><br><span class="line"></span><br><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">   <span class="string">&#x27;bossPro.pipelines.BossproPipeline&#x27;</span>: <span class="number">300</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ul>
<li>pipelines.py</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BossproPipeline</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_item</span>(<span class="params">self, item, spider</span>):</span><br><span class="line">        <span class="built_in">print</span>(item)</span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure>

<ul>
<li>item.py</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BossproItem</span>(scrapy.Item):</span><br><span class="line">    <span class="comment"># define the fields for your item here like:</span></span><br><span class="line">    job_name = scrapy.Field()</span><br><span class="line">    job_desc = scrapy.Field()</span><br><span class="line">    <span class="comment"># pass</span></span><br></pre></td></tr></table></figure>

<ul>
<li>boss.py</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> bossPro.items <span class="keyword">import</span> BossproItem</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BossSpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&#x27;boss&#x27;</span></span><br><span class="line">    <span class="comment"># allowed_domains = [&#x27;www.xxx.com&#x27;]</span></span><br><span class="line">    start_urls = [<span class="string">&#x27;https://www.zhipin.com/c101010100/?query=python&amp;ka=sel-city-101010100&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    url = <span class="string">&#x27;https://www.zhipin.com/c101010100/?query=python&amp;page=%d&#x27;</span></span><br><span class="line">    page_num = <span class="number">2</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 回调函数接受item</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse_detail</span>(<span class="params">self, response</span>):</span><br><span class="line">        item = response.meta[<span class="string">&#x27;item&#x27;</span>]</span><br><span class="line">        </span><br><span class="line">        job_desc = response.xpath(<span class="string">&#x27;/html/body/div[1]/div[2]/div[3]/div/div[2]/div[2]/div[1]/div//text()&#x27;</span>).extract()</span><br><span class="line">        job_desc = <span class="string">&#x27;&#x27;</span>.join(job_desc)</span><br><span class="line">        <span class="comment"># print(job_desc)</span></span><br><span class="line">        item[<span class="string">&#x27;job_desc&#x27;</span>] = job_desc</span><br><span class="line"></span><br><span class="line">        <span class="keyword">yield</span> item</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 解析首页的岗位名称</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">        li_list = response.xpath(<span class="string">&#x27;//*[@id=&quot;main&quot;]/div/div[3]/ul/li&#x27;</span>)</span><br><span class="line">        <span class="comment"># print(li_list)</span></span><br><span class="line">        <span class="keyword">for</span> li <span class="keyword">in</span> li_list:</span><br><span class="line">            item = BossproItem()</span><br><span class="line"></span><br><span class="line">            job_name = <span class="string">&#x27;https://www.zhipin.com&#x27;</span> + li.xpath(<span class="string">&#x27;.//div[@class=&quot;info-primary&quot;]/div[1]/div/div[1]/a/text()&#x27;</span>).extract_first()</span><br><span class="line">            item[<span class="string">&#x27;job_name&#x27;</span>] = job_name</span><br><span class="line">            <span class="comment"># print(job_name)</span></span><br><span class="line">            <span class="comment"># print(&#x27;12&#x27;)</span></span><br><span class="line">            detail_url = li.xpath(<span class="string">&#x27;.//div[@class=&quot;info-primary&quot;]/div[1]/div/div[1]/a/@href&#x27;</span>).extract_first()</span><br><span class="line">            <span class="comment"># 对详情页发情求获取详情页的页面源码数据</span></span><br><span class="line">            <span class="comment"># 手动请求的发送</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 请求传参：meta=&#123;&#125;，可以将meta字典传递给请求对应的回调函数</span></span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(detail_url, callback=self.parse_detail, meta=&#123;<span class="string">&#x27;item&#x27;</span>: item&#125;)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 分页操作</span></span><br><span class="line">        <span class="keyword">if</span> self.page_num &lt;= <span class="number">3</span>:</span><br><span class="line">            new_url = <span class="built_in">format</span>(self.url % self.page_num)</span><br><span class="line">            self.page_num += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(new_url, callback=self.parse)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="9-7-7-图片数据爬取之ImagesPipeline"><a href="#9-7-7-图片数据爬取之ImagesPipeline" class="headerlink" title="9.7 7.图片数据爬取之ImagesPipeline"></a>9.7 7.图片数据爬取之ImagesPipeline</h4><ul>
<li>图片数据爬取之ImagesPipeline<ul>
<li>基于scrapy爬取字符串类型的数据和爬取图片类型数据的区别？<ul>
<li>字符串：只需要基于xpath进行解析且提交管道进行持久化存储</li>
<li>图片：xpath及洗出图片src的属性值。单独的对图片地址发起请求获取图片二进制类型的数据。</li>
</ul>
</li>
<li>ImagesPipeline：<ul>
<li>只需要将img的src的属性值进行解析，提交到管道，管道就会对图片的src进行请求发送获取图片的二进制类型的数据，且还会帮我们进行持久化存储</li>
</ul>
</li>
<li>需求：爬取站长素材中的高清图片</li>
<li>使用流程：<ol>
<li>数据解析（图片的地址）</li>
<li>将存储图片地址的item提交到指定的管道类</li>
<li>在管道文件中自定制一个基于imagesPipeLine的一个管道类<ul>
<li>get_media_requests</li>
<li>file_path</li>
<li>item_completed</li>
</ul>
</li>
<li>在配置文件中：<br>  指定图片的存储目录：IMAGES_STORE &#x3D; ‘.&#x2F;imgs’<br>  指定开启的管道：自定制的管道类</li>
</ol>
</li>
</ul>
</li>
</ul>
<p>代码如下：</p>
<ul>
<li>img.py</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. 数据解析（图片的地址）</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> ..items <span class="keyword">import</span> ImgproItem</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ImgSpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&#x27;img&#x27;</span></span><br><span class="line">    <span class="comment"># allowed_domains = [&#x27;www.xxx.com&#x27;]</span></span><br><span class="line">    start_urls = [<span class="string">&#x27;https://sc.chinaz.com/tupian/&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">        div_list = response.xpath(<span class="string">&#x27;//div[@id=&quot;container&quot;]/div&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> div <span class="keyword">in</span> div_list:</span><br><span class="line">            <span class="comment"># 注意：使用伪属性 src2</span></span><br><span class="line">            src = <span class="string">&#x27;https:&#x27;</span> + div.xpath(<span class="string">&#x27;./div/a/img/@src2&#x27;</span>).extract_first()</span><br><span class="line">            <span class="built_in">print</span>(src)</span><br><span class="line"></span><br><span class="line">            item = ImgproItem()</span><br><span class="line">            item[<span class="string">&#x27;src&#x27;</span>] = src</span><br><span class="line"></span><br><span class="line">            <span class="keyword">yield</span> item</span><br></pre></td></tr></table></figure>
<ul>
<li>items.py</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 2.将存储图片地址的item提交到指定的管道类</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ImgproItem</span>(scrapy.Item):</span><br><span class="line">    <span class="comment"># define the fields for your item here like:</span></span><br><span class="line">    src = scrapy.Field()</span><br><span class="line">    <span class="comment"># pass</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<ul>
<li>pipelines.py<br>重写类（类的方法）</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 3.在管道文件中自定制一个基于imagesPipeLine的一个管道类</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># class ImgproPipeline:</span></span><br><span class="line"><span class="comment">#     def process_item(self, item, spider):</span></span><br><span class="line"><span class="comment">#         return item</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> scrapy.pipelines.images <span class="keyword">import</span> ImagesPipeline</span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ImgsPipeline</span>(<span class="title class_ inherited__">ImagesPipeline</span>):</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 可以根据图片地址进行图片数据的请求</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_media_requests</span>(<span class="params">self, item, info</span>):</span><br><span class="line"></span><br><span class="line">        <span class="keyword">yield</span> scrapy.Request(item[<span class="string">&#x27;src&#x27;</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 指定图片存储的路径</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">file_path</span>(<span class="params">self, request, response=<span class="literal">None</span>, info=<span class="literal">None</span>, *, item=<span class="literal">None</span></span>):</span><br><span class="line">        imgName = request.url.split(<span class="string">&#x27;/&#x27;</span>)[-<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">return</span> imgName</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">item_completed</span>(<span class="params">self, results, item, info</span>):</span><br><span class="line">        <span class="keyword">return</span> item     <span class="comment"># 返回给下一个即将被执行的管道类</span></span><br></pre></td></tr></table></figure>

<ul>
<li>settings.py</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">4.在配置文件中：</span></span><br><span class="line"><span class="string">	指定图片的存储目录：IMAGES_STORE = &#x27;./imgs&#x27;</span></span><br><span class="line"><span class="string">	指定开启的管道：自定制的管道类</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># Crawl responsibly by identifying yourself (and your website) on the user-agent</span></span><br><span class="line">USER_AGENT = <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Obey robots.txt rules</span></span><br><span class="line">ROBOTSTXT_OBEY = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">LOG_LEVEL = <span class="string">&#x27;ERROR&#x27;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>开启管道、更改类名</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">   <span class="comment">#  开启管道、更改类名</span></span><br><span class="line">   <span class="string">&#x27;imgPro.pipelines.ImgsPipeline&#x27;</span>: <span class="number">300</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>指定图片存储的目录</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">IMAGES_STORE = <span class="string">&#x27;./imgs&#x27;</span></span><br></pre></td></tr></table></figure>
<h4 id="9-8-中间件-MiddleWare"><a href="#9-8-中间件-MiddleWare" class="headerlink" title="9.8 中间件 MiddleWare"></a>9.8 中间件 MiddleWare</h4><h4 id="①知识点"><a href="#①知识点" class="headerlink" title="①知识点"></a>①知识点</h4><ul>
<li>中间件<ul>
<li>下载中间件</li>
<li>位置：引擎和下载器之间</li>
<li>作用：批量拦截到整个工程的所有请求和响应</li>
<li>拦截请求：<ul>
<li>UA伪装：process_request</li>
<li>代理IP：process_exception: return request</li>
</ul>
</li>
<li>拦截响应：<ul>
<li>篡改响应数据、响应对象</li>
<li>需求：爬取网易新闻中的新闻数据（标题和内容）<ol>
<li>通过网易新闻的首页解析出五大板块对应的详情页url（没有动态加载）</li>
<li>每一个板块对应的新闻标题都是动态加载出来的（动态加载）</li>
<li>通过解析出每一条新闻详情页的url获取详情页的页面源码，解析出新闻内容</li>
</ol>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="②代码"><a href="#②代码" class="headerlink" title="②代码"></a>②代码</h4><ul>
<li>Ⅰ拦截请求<br>拦截请求代码部分：</li>
<li>middle.py</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MiddleSpider</span>(scrapy.Spider):</span><br><span class="line">    <span class="comment"># 爬取百度</span></span><br><span class="line">    name = <span class="string">&#x27;middle&#x27;</span></span><br><span class="line">    <span class="comment"># allowed_domains = [&#x27;www.xxx.com&#x27;]</span></span><br><span class="line">    start_urls = [<span class="string">&#x27;http://www.baidu.com/s?wd=ip&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">        page_text = response.text</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;./ip.html&#x27;</span>, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">            fp.write(page_text)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ul>
<li>settings.py</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 协议改为FALSE</span></span><br><span class="line">ROBOTSTXT_OBEY = <span class="literal">False</span></span><br><span class="line"><span class="comment"># 开启DOWNLOADER_MIDDLEWARES</span></span><br><span class="line">DOWNLOADER_MIDDLEWARES = &#123;</span><br><span class="line">   <span class="string">&#x27;middlePro.middlewares.MiddleproDownloaderMiddleware&#x27;</span>: <span class="number">543</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ul>
<li>middlewares.py</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MiddleproDownloaderMiddleware</span>:</span><br><span class="line">    <span class="comment"># Not all methods need to be defined. If a method is not defined,</span></span><br><span class="line">    <span class="comment"># scrapy acts as if the downloader middleware does not modify the</span></span><br><span class="line">    <span class="comment"># passed objects.</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># @classmethod</span></span><br><span class="line">    <span class="comment"># def from_crawler(cls, crawler):</span></span><br><span class="line">    <span class="comment">#     # This method is used by Scrapy to create your spiders.</span></span><br><span class="line">    <span class="comment">#     s = cls()</span></span><br><span class="line">    <span class="comment">#     crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)</span></span><br><span class="line">    <span class="comment">#     return s</span></span><br><span class="line"></span><br><span class="line">    user_agent_list = [</span><br><span class="line">        <span class="string">&quot;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 &quot;</span></span><br><span class="line">        <span class="string">&quot;(KHTML, like Gecko) Chrome/22.0.1207.1 Safari/537.1&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Mozilla/5.0 (X11; CrOS i686 2268.111.0) AppleWebKit/536.11 &quot;</span></span><br><span class="line">        <span class="string">&quot;(KHTML, like Gecko) Chrome/20.0.1132.57 Safari/536.11&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.6 &quot;</span></span><br><span class="line">        <span class="string">&quot;(KHTML, like Gecko) Chrome/20.0.1092.0 Safari/536.6&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.6 &quot;</span></span><br><span class="line">        <span class="string">&quot;(KHTML, like Gecko) Chrome/20.0.1090.0 Safari/536.6&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.1 &quot;</span></span><br><span class="line">        <span class="string">&quot;(KHTML, like Gecko) Chrome/19.77.34.5 Safari/537.1&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/536.5 &quot;</span></span><br><span class="line">        <span class="string">&quot;(KHTML, like Gecko) Chrome/19.0.1084.9 Safari/536.5&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Mozilla/5.0 (Windows NT 6.0) AppleWebKit/536.5 &quot;</span></span><br><span class="line">        <span class="string">&quot;(KHTML, like Gecko) Chrome/19.0.1084.36 Safari/536.5&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 &quot;</span></span><br><span class="line">        <span class="string">&quot;(KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Mozilla/5.0 (Windows NT 5.1) AppleWebKit/536.3 &quot;</span></span><br><span class="line">        <span class="string">&quot;(KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_8_0) AppleWebKit/536.3 &quot;</span></span><br><span class="line">        <span class="string">&quot;(KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 &quot;</span></span><br><span class="line">        <span class="string">&quot;(KHTML, like Gecko) Chrome/19.0.1062.0 Safari/536.3&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 &quot;</span></span><br><span class="line">        <span class="string">&quot;(KHTML, like Gecko) Chrome/19.0.1062.0 Safari/536.3&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 &quot;</span></span><br><span class="line">        <span class="string">&quot;(KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 &quot;</span></span><br><span class="line">        <span class="string">&quot;(KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Mozilla/5.0 (Windows NT 6.1) AppleWebKit/536.3 &quot;</span></span><br><span class="line">        <span class="string">&quot;(KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 &quot;</span></span><br><span class="line">        <span class="string">&quot;(KHTML, like Gecko) Chrome/19.0.1061.0 Safari/536.3&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/535.24 &quot;</span></span><br><span class="line">        <span class="string">&quot;(KHTML, like Gecko) Chrome/19.0.1055.1 Safari/535.24&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/535.24 &quot;</span></span><br><span class="line">        <span class="string">&quot;(KHTML, like Gecko) Chrome/19.0.1055.1 Safari/535.24&quot;</span></span><br><span class="line">    ]</span><br><span class="line">    PROXY_http = [</span><br><span class="line">        <span class="string">&#x27;153.180.102.104:80&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;195.208.131.189:56055&#x27;</span>,</span><br><span class="line">    ]</span><br><span class="line">    PROXY_https = [</span><br><span class="line">        <span class="string">&#x27;120.83.49.90:9000&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;95.189.112.214:35508&#x27;</span>,</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 拦截请求</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_request</span>(<span class="params">self, request, spider</span>):</span><br><span class="line">        <span class="comment"># UA伪装</span></span><br><span class="line">        request.headers[<span class="string">&#x27;User-Agent&#x27;</span>] = random.choice(self.user_agent_list)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 为了验证代理操作是否生效</span></span><br><span class="line">        request.meta[<span class="string">&#x27;proxy&#x27;</span>] = <span class="string">&#x27;http://116.62.198.43&#x27;</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 拦截所有的异常</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_response</span>(<span class="params">self, request, response, spider</span>):</span><br><span class="line">        <span class="comment"># Called with the response returned from the downloader.</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Must either;</span></span><br><span class="line">        <span class="comment"># - return a Response object</span></span><br><span class="line">        <span class="comment"># - return a Request object</span></span><br><span class="line">        <span class="comment"># - or raise IgnoreRequest</span></span><br><span class="line">        <span class="keyword">return</span> response</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 拦截发生异常的请求</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_exception</span>(<span class="params">self, request, exception, spider</span>):</span><br><span class="line">        <span class="keyword">if</span> request.url.split(<span class="string">&#x27;:&#x27;</span>)[<span class="number">0</span>] == <span class="string">&#x27;http&#x27;</span>:</span><br><span class="line">            <span class="comment"># 代理</span></span><br><span class="line">            request.meta[<span class="string">&#x27;proxy&#x27;</span>] = <span class="string">&#x27;http://&#x27;</span> + random.choice(self.PROXY_http)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            request.meta[<span class="string">&#x27;proxy&#x27;</span>] = <span class="string">&#x27;https://&#x27;</span> + random.choice(self.PROXY_https)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> request     <span class="comment"># 将修正之后的请求对象进行重新的请求发送</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># def spider_opened(self, spider):</span></span><br><span class="line">    <span class="comment">#     spider.logger.info(&#x27;Spider opened: %s&#x27; % spider.name)</span></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="Ⅱ拦截响应"><a href="#Ⅱ拦截响应" class="headerlink" title="Ⅱ拦截响应"></a>Ⅱ拦截响应</h4><p>爬取网易新闻数据代码如下：</p>
<ul>
<li>wangyi.py</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> ..items <span class="keyword">import</span> WangyiproItem</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">WangyiSpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&#x27;wangyi&#x27;</span></span><br><span class="line">    <span class="comment"># allowed_domains = [&#x27;www.xxx.com&#x27;]</span></span><br><span class="line">    start_urls = [<span class="string">&#x27;https://news.163.com/&#x27;</span>]</span><br><span class="line">    modules_urls = []       <span class="comment"># 存储五个板块对应详情页url</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 解析五大板块对应详情页url</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 实例化一个浏览器对象</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.bro = webdriver.Chrome(executable_path=<span class="string">r&#x27;D:\pycharm\PyCharm Community Edition 2020.1.2\CrawlerCourse\07DynamicLoad_dataProcessing\chromedriver.exe&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">        li_list = response.xpath(<span class="string">&#x27;//*[@id=&quot;index2016_wrap&quot;]/div[2]/div[2]/div[2]/div[2]/div/ul/li&#x27;</span>)</span><br><span class="line">        alist = [<span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>]</span><br><span class="line">        <span class="keyword">for</span> index <span class="keyword">in</span> alist:</span><br><span class="line">            module_url = li_list[index].xpath(<span class="string">&#x27;./a/@href&#x27;</span>).extract_first()</span><br><span class="line">            self.modules_urls.append(module_url)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 依次对每一个板块对应的页面进行请求</span></span><br><span class="line">        <span class="keyword">for</span> url <span class="keyword">in</span> self.modules_urls:       <span class="comment"># 对每一个板块的url进行请求发送</span></span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(url, callback=self.parse_module)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 每一个板块对应的新闻标题相关的内容都是动态加载</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse_module</span>(<span class="params">self, response</span>):       <span class="comment"># 解析每一个板块页面中对应新闻的标题和新闻详情页的url</span></span><br><span class="line">        <span class="comment"># response.xpath()</span></span><br><span class="line">        div_list = response.xpath(<span class="string">&#x27;/html/body/div/div[3]/div[4]/div[1]/div[1]/div/ul/li/div/div&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> div <span class="keyword">in</span> div_list:</span><br><span class="line">            title = div.xpath(<span class="string">&#x27;./div/div[1]/h3/a/text()&#x27;</span>).extract_first()</span><br><span class="line">            news_detail_url = div.xpath(<span class="string">&#x27;./div/div[1]/h3/a/@href&#x27;</span>).extract_first()</span><br><span class="line"></span><br><span class="line">            item = WangyiproItem()</span><br><span class="line">            item[<span class="string">&#x27;title&#x27;</span>] = title</span><br><span class="line">            <span class="comment"># 对新闻详情页的url发起请求</span></span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(url=news_detail_url, callback=self.parse_detail, meta=&#123;<span class="string">&#x27;item&#x27;</span>: item&#125;)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse_detail</span>(<span class="params">self, response</span>):       <span class="comment"># 解析新闻内容</span></span><br><span class="line">        content = response.xpath(<span class="string">&#x27;/html/body/div[3]/div[1]/div[3]/div[2]//text()&#x27;</span>).extract()</span><br><span class="line">        content = <span class="string">&#x27;&#x27;</span>.join(content)</span><br><span class="line">        item = response.meta[<span class="string">&#x27;item&#x27;</span>]</span><br><span class="line">        item[<span class="string">&#x27;content&#x27;</span>] = content</span><br><span class="line"></span><br><span class="line">        <span class="keyword">yield</span> item</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">close</span>(<span class="params">self, spider</span>):</span><br><span class="line">        self.bro.quit()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ul>
<li>middlewares.py</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Define here the models for your spider middleware</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># See documentation in:</span></span><br><span class="line"><span class="comment"># https://docs.scrapy.org/en/latest/topics/spider-middleware.html</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> signals</span><br><span class="line"></span><br><span class="line"><span class="comment"># useful for handling different item types with a single interface</span></span><br><span class="line"><span class="keyword">from</span> itemadapter <span class="keyword">import</span> is_item, ItemAdapter</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> scrapy.http <span class="keyword">import</span> HtmlResponse</span><br><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> sleep</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">WangyiproDownloaderMiddleware</span>:</span><br><span class="line">    <span class="comment"># Not all methods need to be defined. If a method is not defined,</span></span><br><span class="line">    <span class="comment"># scrapy acts as if the downloader middleware does not modify the</span></span><br><span class="line">    <span class="comment"># passed objects.</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_request</span>(<span class="params">self, request, spider</span>):</span><br><span class="line">        <span class="comment"># Called for each request that goes through the downloader</span></span><br><span class="line">        <span class="comment"># middleware.</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Must either:</span></span><br><span class="line">        <span class="comment"># - return None: continue processing this request</span></span><br><span class="line">        <span class="comment"># - or return a Response object</span></span><br><span class="line">        <span class="comment"># - or return a Request object</span></span><br><span class="line">        <span class="comment"># - or raise IgnoreRequest: process_exception() methods of</span></span><br><span class="line">        <span class="comment">#   installed downloader middleware will be called</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 该方法拦截五大板块对应的响应对象，进行篡改</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_response</span>(<span class="params">self, request, response, spider</span>):      <span class="comment"># spider表示爬虫对象</span></span><br><span class="line">        bro = spider.bro        <span class="comment"># 获取了在爬虫类中定义的浏览器对象</span></span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">        <span class="comment"># 挑选出指定的响应对象进行篡改</span></span><br><span class="line">        <span class="comment"># 通过url指定request</span></span><br><span class="line">        <span class="comment"># 通过request指定response</span></span><br><span class="line">        <span class="keyword">if</span> request.url <span class="keyword">in</span> spider.modules_urls:</span><br><span class="line">            bro.get(request.url)        <span class="comment"># 五个板块对应的url进行请求</span></span><br><span class="line">            sleep(<span class="number">3</span>)</span><br><span class="line">            page_text = bro.page_source     <span class="comment"># 包含了动态加载的新闻数据</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># response    # 五大板块对应的响应对象</span></span><br><span class="line">            <span class="comment"># 针对定位到的这些response进行篡改</span></span><br><span class="line">            <span class="comment"># 实例化一个新的响应对象（符合需求的：包含动态加载出的新闻数据），替代旧的响应对象</span></span><br><span class="line">            <span class="comment"># 如何获取动态加载出的新闻数据？</span></span><br><span class="line">            <span class="comment">#    基于selenium便捷的获取动态加载数据</span></span><br><span class="line">            new_response = HtmlResponse(url=request.url, body=page_text, encoding=<span class="string">&#x27;utf-8&#x27;</span>, request=request)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">return</span> new_response</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># response    # 其他请求对应的响应对象</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">return</span> response</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_exception</span>(<span class="params">self, request, exception, spider</span>):</span><br><span class="line">        <span class="comment"># Called when a download handler or a process_request()</span></span><br><span class="line">        <span class="comment"># (from other downloader middleware) raises an exception.</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Must either:</span></span><br><span class="line">        <span class="comment"># - return None: continue processing this exception</span></span><br><span class="line">        <span class="comment"># - return a Response object: stops process_exception() chain</span></span><br><span class="line">        <span class="comment"># - return a Request object: stops process_exception() chain</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ul>
<li>pipelines.py</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">WangyiproPipeline</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_item</span>(<span class="params">self, item, spider</span>):</span><br><span class="line">        <span class="built_in">print</span>(item)</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ul>
<li>settings.py</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1.常规操作</span></span><br><span class="line">USER_AGENT = <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36&#x27;</span></span><br><span class="line"></span><br><span class="line">ROBOTSTXT_OBEY = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">LOG_LEVEL = <span class="string">&#x27;ERROR&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.开启中间件</span></span><br><span class="line">DOWNLOADER_MIDDLEWARES = &#123;</span><br><span class="line">   <span class="string">&#x27;wangyiPro.middlewares.WangyiproDownloaderMiddleware&#x27;</span>: <span class="number">543</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.开启管道</span></span><br><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">   <span class="string">&#x27;wangyiPro.pipelines.WangyiproPipeline&#x27;</span>: <span class="number">300</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="9-9-CrawlSpider"><a href="#9-9-CrawlSpider" class="headerlink" title="9.9 CrawlSpider"></a>9.9 CrawlSpider</h4><ul>
<li>CrawlSpider：类，Spider的一个子类<ul>
<li>全站数据爬取的方式：<ul>
<li>基于Spider：手动请求发送</li>
<li>基于CrawlSpider：</li>
</ul>
</li>
<li>CrawlSpider的使用：<ul>
<li>创建一个工程</li>
<li>cd XXX</li>
<li>创建爬虫文件（CrawlSpider）：<ul>
<li>scrapy genspider -t crawl xxx <a target="_blank" rel="noopener" href="http://www.xxx.com/">www.xxx.com</a></li>
<li>链接提取器：<ul>
<li>作用：根据指定的规则（allow）进行指定链接的提取</li>
</ul>
</li>
<li>规则解析器：<ul>
<li>作用：将链接提取器提取到的链接进行指定规则（callback）的解析</li>
</ul>
</li>
</ul>
</li>
<li>需求：爬取sun网站中的编号、新闻标题、新闻内容、标号<ul>
<li>分析：爬取的数据不在同一张页面中。</li>
</ul>
<ol>
<li>可以使用链接提取器提取所有的页码链接</li>
<li>让链接提取器提取所有的新闻详情页的链接</li>
</ol>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>案例代码（目标网站已改版，代码暂未调整）：</p>
<ul>
<li>sun.py</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> scrapy.linkextractors <span class="keyword">import</span> LinkExtractor</span><br><span class="line"><span class="keyword">from</span> scrapy.spiders <span class="keyword">import</span> CrawlSpider, Rule</span><br><span class="line"><span class="keyword">from</span> ..items <span class="keyword">import</span> SunproItem, DetailItem</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 需求：爬取sun网站中的编号、新闻标题、新闻内容、标号</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SunSpider</span>(<span class="title class_ inherited__">CrawlSpider</span>):</span><br><span class="line">    name = <span class="string">&#x27;sun&#x27;</span></span><br><span class="line">    <span class="comment"># allowed_domains = [&#x27;www.xxx.com&#x27;]</span></span><br><span class="line">    start_urls = [<span class="string">&#x27;http://wz.sun0769.com/index.php/question/questionType?type=4&amp;page=&#x27;</span>]</span><br><span class="line">    <span class="comment"># 这个网址进不去了，改版之后是：https://wz.sun0769.com/political/index，代码暂时不做调整</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 链接提取器：根据指定规则（allow=&quot;正则&quot;）进行指定链接的提取</span></span><br><span class="line">    link = LinkExtractor(allow=<span class="string">r&#x27;type=4&amp;page=\d+&#x27;</span>)</span><br><span class="line">    link_detail = LinkExtractor(allow=<span class="string">r&#x27;question/\d+/\d+\.shtml&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    rules = (</span><br><span class="line">        <span class="comment"># 规则解析器：将链接提取器提取到的链接进行指定规则（callback）的解析操作</span></span><br><span class="line">        Rule(link, callback=<span class="string">&#x27;parse_item&#x27;</span>, follow=<span class="literal">True</span>),</span><br><span class="line">        <span class="comment"># follow = True : 可以将链接提取器 继续作用到 链接提取器提取到的链接 所对应的页面中</span></span><br><span class="line">        Rule(link_detail, callback=<span class="string">&#x27;parse_detail&#x27;</span>, follow=<span class="literal">True</span>)     <span class="comment"># 此处 follow=True 可以不写</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 解析新闻编号和新闻标题</span></span><br><span class="line">    <span class="comment"># 如下两个方法不可以实现请求传参！</span></span><br><span class="line">    <span class="comment"># 无法将两个解析方法解析的数据存储到同一个item中，可以存储到两个item中</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse_item</span>(<span class="params">self, response</span>):</span><br><span class="line">        <span class="comment"># 注意：xpath表达式中不可以出现tbody标签</span></span><br><span class="line">        tr_list = response.xpath(<span class="string">&#x27;//*[@id=&quot;morelist&quot;]/div/table[2]//tr/td/table//tr&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> tr <span class="keyword">in</span> tr_list:</span><br><span class="line">            new_num = tr.xpath(<span class="string">&#x27;./td[1]/text()&#x27;</span>).extract_first()</span><br><span class="line">            new_title = tr.xpath(<span class="string">&#x27;./td[2]/a[2]/@title&#x27;</span>).extract_first()</span><br><span class="line">            item = SunproItem()</span><br><span class="line">            item[<span class="string">&#x27;title&#x27;</span>] = new_title</span><br><span class="line">            item[<span class="string">&#x27;new_num&#x27;</span>] = new_num</span><br><span class="line"></span><br><span class="line">            <span class="keyword">yield</span> item</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 解析新闻内容和新闻</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse_detail</span>(<span class="params">self, response</span>):</span><br><span class="line">        new_id = response.xpath(<span class="string">&#x27;/html/body/div[9]/table[1]//tr/td[2]/span[2]/text()&#x27;</span>).extract_first()</span><br><span class="line">        new_content = response.xpath(<span class="string">&#x27;/html/body/div[9]/table[2]//tr[1]//text()&#x27;</span>).extract()</span><br><span class="line">        new_content = <span class="string">&#x27;&#x27;</span>.join(new_content)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># print(new_id, new_content)</span></span><br><span class="line">        item = DetailItem()</span><br><span class="line">        item[<span class="string">&#x27;content&#x27;</span>] = new_content</span><br><span class="line">        item[<span class="string">&#x27;new_id&#x27;</span>] = new_id</span><br><span class="line"></span><br><span class="line">        <span class="keyword">yield</span> item</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ul>
<li>items.py</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SunproItem</span>(scrapy.Item):</span><br><span class="line">    <span class="comment"># define the fields for your item here like:</span></span><br><span class="line">    title = scrapy.Field()</span><br><span class="line">    new_num = scrapy.Field()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DetailItem</span>(scrapy.Item):</span><br><span class="line">    new_id = scrapy.Field()</span><br><span class="line">    content = scrapy.Field()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ul>
<li>pipelines.py</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SunproPipeline</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_item</span>(<span class="params">self, item, spider</span>):</span><br><span class="line">        <span class="comment"># 如何判断item的类型</span></span><br><span class="line">        <span class="comment"># 将数据写入数据库时，如何保证数据的一致性</span></span><br><span class="line">        <span class="keyword">if</span> item.__class__.__name__ == <span class="string">&#x27;DetailItem&#x27;</span>:</span><br><span class="line">            <span class="built_in">print</span>(item[<span class="string">&#x27;new_id&#x27;</span>], item[<span class="string">&#x27;content&#x27;</span>])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="built_in">print</span>(item[<span class="string">&#x27;new_num&#x27;</span>], item[<span class="string">&#x27;title&#x27;</span>])</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ul>
<li>settings.py<br>常规操作 + 开启管道</li>
</ul>
<h4 id="9-10-分布式爬虫"><a href="#9-10-分布式爬虫" class="headerlink" title="9.10 分布式爬虫"></a>9.10 分布式爬虫</h4><ul>
<li>分布式爬虫<ul>
<li><p>概念：我们需要搭建一个分布式的机群，让其对一组资源进行分布联合爬取</p>
</li>
<li><p>作用：提升爬取数据的效率</p>
</li>
<li><p>如何实现分布式？</p>
<ul>
<li>安装一个scrapy-redis的组件</li>
<li>原生的scrapy是不可以实现分布式爬虫，必须要让scrapy-redis组件一起实现分布式爬虫</li>
<li>为什么原生的scrapy不可以实现分布式？<ul>
<li>调度器不可以被分布式机群共享</li>
<li>管道也不可以被分布式机群共享</li>
</ul>
</li>
<li>scrapy-redis组件作用：<ul>
<li>可以给原生的scrapy框架提供可以被共享的管道和调度器</li>
</ul>
</li>
<li>实现流程：<ul>
<li>创建一个工程</li>
<li>创建一个基于CrawlSpider的爬虫文件</li>
<li>修改当前的爬虫文件：<ul>
<li>导包：from scrapy-redis.spiders import RedisCrawlSpider</li>
<li>将start_urls和allowed_domains进行注释</li>
<li>添加一个新属性：redis_key &#x3D; ‘sun’ 可以被共享的调度器队列名称</li>
<li>编写数据解析相关的操作</li>
<li>将当前爬虫类的父类修改成RedisCrawlSpider</li>
</ul>
</li>
<li>修改配置文件settings<ul>
<li>指定使用可以被共享的管道：<br>  ITEM_PIPELINES &#x3D; { ‘scrapy_redis.pipelines.RedisPipeline’: 400, }</li>
<li>指定调度器：<br>  DUPEFILTER_CLASS &#x3D; “scrapy_redis.dupefilter.RFPDupeFilter”<br>  SCHEDULER &#x3D; “scrapy_redis.scheduler.Scheduler”<br>  SCHEDULER_PERSIST &#x3D; True</li>
<li>指定redis服务器：<br>  REDIS_HOST &#x3D; ‘redis服务器的ip地址’<br>  REDIS_PORT &#x3D; 6379</li>
</ul>
</li>
<li>redis相关配置操作：<ul>
<li>配置redis的配置文件：<ul>
<li>Linux或者mac：redis.conf</li>
<li>windows:redis.windows.conf</li>
<li>打开配置文件修改：<ul>
<li>将bind 127.0.0.1删除或注释</li>
<li>关闭保护模式：protected-mode yes改为no</li>
</ul>
</li>
</ul>
</li>
<li>结合着配置文件开启redis服务<ul>
<li>redis-sever 配置文件</li>
</ul>
</li>
<li>启动客户端：<ul>
<li>redis-cli</li>
</ul>
</li>
</ul>
</li>
<li>执行工程：<ul>
<li>scrapy runspider xxx.py</li>
</ul>
</li>
<li>向调度器的队列中放入一个起始的url：<ul>
<li>调度器的队列在redis客户端中<ul>
<li>lpush xxx <a target="_blank" rel="noopener" href="http://www.xxx.com/">www.xxx.com</a></li>
</ul>
</li>
</ul>
</li>
<li>爬取到的数据存储在了redis的proName：items这个数据结构中</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>案例代码（目标网站已改版，部分代码暂未调整）</p>
<ul>
<li>fbs.py</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> scrapy.linkextractors <span class="keyword">import</span> LinkExtractor</span><br><span class="line"><span class="keyword">from</span> scrapy.spiders <span class="keyword">import</span> CrawlSpider, Rule</span><br><span class="line"><span class="keyword">from</span> scrapy-redis.spiders <span class="keyword">import</span> RedisCrawlSpider</span><br><span class="line"><span class="keyword">from</span> ..items <span class="keyword">import</span> FbsproItem</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">FbsSpider</span>(<span class="title class_ inherited__">RedisCrawlSpider</span>):</span><br><span class="line">    name = <span class="string">&#x27;fbs&#x27;</span></span><br><span class="line">    <span class="comment"># allowed_domains = [&#x27;www.xxx.com&#x27;]</span></span><br><span class="line">    <span class="comment"># start_urls = [&#x27;http://www.xxx.com/&#x27;]</span></span><br><span class="line">    redis_key = <span class="string">&#x27;sun&#x27;</span></span><br><span class="line">    rules = (</span><br><span class="line">        Rule(LinkExtractor(allow=<span class="string">r&#x27;type=4&amp;page=\d+&#x27;</span>), callback=<span class="string">&#x27;parse_item&#x27;</span>, follow=<span class="literal">True</span>),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse_item</span>(<span class="params">self, response</span>):</span><br><span class="line">        tr_list = response.xpath(<span class="string">&#x27;//*[@id=&quot;morelist&quot;]/div/table[2]//tr/td/table//tr&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> tr <span class="keyword">in</span> tr_list:</span><br><span class="line">            new_num = tr.xpath(<span class="string">&#x27;./td[1]/text()&#x27;</span>).extract_first()</span><br><span class="line">            new_title = tr.xpath(<span class="string">&#x27;./td[2]/a[2]/@title&#x27;</span>).extract_first()</span><br><span class="line"></span><br><span class="line">            item = FbsproItem()</span><br><span class="line">            item[<span class="string">&#x27;title&#x27;</span>] = new_title</span><br><span class="line">            item[<span class="string">&#x27;new_num&#x27;</span>] = new_num</span><br><span class="line"></span><br><span class="line">            <span class="keyword">yield</span> item</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ul>
<li>items.py</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">FbsproItem</span>(scrapy.Item):</span><br><span class="line">    <span class="comment"># define the fields for your item here like:</span></span><br><span class="line">    title = scrapy.Field()</span><br><span class="line">    new_num = scrapy.Field()</span><br><span class="line">    <span class="comment"># pass</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ul>
<li>settings.py<br>常规操作（3步） + 指定管道 + 指定调度器 + 指定服务器</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 指定管道</span></span><br><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">   <span class="string">&#x27;scrapy_redis.pipelines.RedisPipeline&#x27;</span>: <span class="number">400</span>,</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># 指定调度器</span></span><br><span class="line"><span class="comment"># 使用scrapy-redis组件的去重队列（增加了一个去重容器类的配置，作用是用redis的set集合来存储请求的指纹数据，从而实现请求去重的持久化存储）</span></span><br><span class="line">DUPEFILTER_CLASS = <span class="string">&quot;scrapy_redis.dupefilter.RFPDupeFilter&quot;</span></span><br><span class="line"><span class="comment"># 使用scrapy-redis组件自己的调度器</span></span><br><span class="line">SCHEDULER = <span class="string">&quot;scrapy_redis.scheduler.Scheduler&quot;</span></span><br><span class="line"><span class="comment"># 是否允许暂停（配置调度器是否要持久化，也就是当爬虫结束了，要不要清空redis中请求队列和去重指纹的set）</span></span><br><span class="line">SCHEDULER_PERSIST = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 指定redis服务</span></span><br><span class="line">REDIS_HOST = <span class="string">&#x27;127.0.0.1&#x27;</span>    <span class="comment"># redis远程服务器的ip（修改）</span></span><br><span class="line">REDIS_PORT = <span class="number">6379</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="http://FraNny77.github.io">是甜豆腐脑</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://franny77.github.io/2022/09/09/spiderNotes/">http://franny77.github.io/2022/09/09/spiderNotes/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://FraNny77.github.io" target="_blank">是甜豆腐脑</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E7%88%AC%E8%99%AB/">爬虫</a></div><div class="post_share"><div class="social-share" data-image="http://cache.yisu.com/upload/admin/customer_case_img/2019-08-08/1565261709.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2022/09/14/MybatisNotes/"><img class="prev-cover" src="http://imgoss.cnu.cc/2204/27/a4383d0f926b346eb1fe8ff128e5b703.jpg?x-oss-process=style/content" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">MybatisNotes</div></div></a></div><div class="next-post pull-right"><a href="/2022/07/14/3%20.%20Gradient%20descent/"><img class="next-cover" src="http://imgoss.cnu.cc/2204/27/a4383d0f926b346eb1fe8ff128e5b703.jpg?x-oss-process=style/content" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">3. Gradient descent</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/./img/tx.JPG" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">是甜豆腐脑</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">19</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">18</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/FraNny77.io"><i class="fab fa-github"></i><span>Gituhub</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/FraNny77" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:2556725169@qq.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="https://blog.csdn.net/FraNny13" target="_blank" title="Blog"><i class="fab fa-algolia"></i></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E4%B8%80%E7%AB%A0"><span class="toc-text">第一章</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%88%AC%E8%99%AB%E7%9A%84%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86"><span class="toc-text">爬虫的基本原理</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-1-http%E5%8D%8F%E8%AE%AE-amp-https%E5%8D%8F%E8%AE%AE"><span class="toc-text">1.1 http协议&amp;https协议</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-2-URL%E5%92%8CURI"><span class="toc-text">1.2 URL和URI</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-3-%E8%AF%B7%E6%B1%82"><span class="toc-text">1.3 请求</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1-3-1%E8%AF%B7%E6%B1%82%E7%9A%84%E6%96%B9%E6%B3%95"><span class="toc-text">1.3.1请求的方法</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#%E8%AF%B7%E6%B1%82%E6%96%B9%E6%B3%95"><span class="toc-text">请求方法</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#1-3-2-%E8%AF%B7%E6%B1%82%E5%A4%B4"><span class="toc-text">1.3.2 请求头</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#1-3-3-%E5%93%8D%E5%BA%94"><span class="toc-text">1.3.3 响应</span></a></li></ol></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E4%BA%8C%E7%AB%A0"><span class="toc-text">第二章</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Requests-%E6%A8%A1%E5%9D%97"><span class="toc-text">Requests 模块</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-requests%E7%9A%84%E4%B8%BB%E8%A6%81%E6%96%B9%E6%B3%95"><span class="toc-text">2.1 requests的主要方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-get-%E6%96%B9%E6%B3%95"><span class="toc-text">2.2 get()方法</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#2-2-2-Response%E5%AF%B9%E8%B1%A1%E5%B1%9E%E6%80%A7"><span class="toc-text">2.2.2 Response对象属性</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9E%E6%88%98%E7%BC%96%E7%A0%81"><span class="toc-text">实战编码</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E2%91%A0%E7%88%AC%E5%8F%96%E6%90%9C%E7%8B%97%E9%A6%96%E9%A1%B5%E7%9A%84%E9%A1%B5%E9%9D%A2%E6%95%B0%E6%8D%AE"><span class="toc-text">①爬取搜狗首页的页面数据</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E2%91%A1%E7%BD%91%E9%A1%B5%E9%87%87%E9%9B%86%E5%99%A8"><span class="toc-text">②网页采集器</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E2%91%A2%E7%A0%B4%E8%A7%A3%E7%99%BE%E5%BA%A6%E7%BF%BB%E8%AF%91"><span class="toc-text">③破解百度翻译</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E2%91%A3%E8%B1%86%E7%93%A3%E7%94%B5%E5%BD%B1%E7%88%AC%E5%8F%96"><span class="toc-text">④豆瓣电影爬取</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E4%B8%89%E7%AB%A0"><span class="toc-text">第三章</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E8%A7%A3%E6%9E%90"><span class="toc-text">数据解析</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%BC%96%E7%A0%81%E6%B5%81%E7%A8%8B%EF%BC%9A"><span class="toc-text">编码流程：</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E8%A7%A3%E6%9E%90%E5%88%86%E7%B1%BB%EF%BC%9A"><span class="toc-text">数据解析分类：</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E8%A7%A3%E6%9E%90%E5%8E%9F%E7%90%86%E6%A6%82%E8%BF%B0%EF%BC%9A"><span class="toc-text">数据解析原理概述：</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F"><span class="toc-text">3.1 正则表达式</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%B8%B8%E7%94%A8%E5%85%83%E5%AD%97%E7%AC%A6"><span class="toc-text">常用元字符</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%BB%8F%E5%85%B8%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F"><span class="toc-text">经典正则表达式</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%B4%AA%E5%A9%AA%E5%8C%B9%E9%85%8D%E5%92%8C%E6%83%B0%E6%80%A7%E5%8C%B9%E9%85%8D"><span class="toc-text">贪婪匹配和惰性匹配</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%A1%88%E4%BE%8B"><span class="toc-text">案例</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-Re%E5%BA%93"><span class="toc-text">3.2 Re库</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#3-2-1-Match-%E5%AF%B9%E8%B1%A1"><span class="toc-text">3.2.1 Match 对象</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-2-2-Re%E5%BA%93%E4%B8%BB%E8%A6%81%E5%8A%9F%E8%83%BD%E5%87%BD%E6%95%B0"><span class="toc-text">3.2.2 Re库主要功能函数</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-2-3-%E9%A2%84%E5%8A%A0%E8%BD%BD%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F"><span class="toc-text">3.2.3 预加载正则表达式</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-2-4-%E5%86%85%E5%AE%B9%E8%8E%B7%E5%8F%96"><span class="toc-text">3.2.4 内容获取</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-3-%E6%A1%88%E4%BE%8B"><span class="toc-text">3.3 案例</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E2%91%A0%E7%88%AC%E5%8F%96%E8%B1%86%E7%93%A3top250%E7%94%B5%E5%BD%B1%E6%8E%92%E8%A1%8C"><span class="toc-text">①爬取豆瓣top250电影排行</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E2%91%A1%E7%88%AC%E5%8F%96%E7%94%B5%E5%BD%B1%E5%A4%A9%E5%A0%82"><span class="toc-text">②爬取电影天堂</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-4-Bs4%E8%A7%A3%E6%9E%90%E5%9F%BA%E7%A1%80"><span class="toc-text">3.4 Bs4解析基础</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#3-4-1%E6%95%B0%E6%8D%AE%E8%A7%A3%E6%9E%90%E7%9A%84%E5%8E%9F%E7%90%86%EF%BC%9A"><span class="toc-text">3.4.1数据解析的原理：</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-4-2bs4%E6%95%B0%E6%8D%AE%E8%A7%A3%E6%9E%90%E7%9A%84%E5%8E%9F%E7%90%86%EF%BC%9A"><span class="toc-text">3.4.2bs4数据解析的原理：</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-4-3-%E5%A6%82%E4%BD%95%E5%AE%9E%E4%BE%8B%E5%8C%96BeautifulSoup%E5%AF%B9%E8%B1%A1%EF%BC%9A"><span class="toc-text">3.4.3 如何实例化BeautifulSoup对象：</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-4-4-%E6%8F%90%E4%BE%9B%E7%9A%84%E7%94%A8%E4%BA%8E%E6%95%B0%E6%8D%AE%E8%A7%A3%E6%9E%90%E7%9A%84%E6%96%B9%E6%B3%95%E5%92%8C%E5%B1%9E%E6%80%A7%EF%BC%9A"><span class="toc-text">3.4.4 提供的用于数据解析的方法和属性：</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-4-5-select"><span class="toc-text">3.4.5 select:</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-4-6-%E8%8E%B7%E5%8F%96%E6%A0%87%E7%AD%BE%E4%B9%8B%E9%97%B4%E7%9A%84%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%EF%BC%9A"><span class="toc-text">3.4.6 获取标签之间的文本数据：</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-4-7-%E8%8E%B7%E5%8F%96%E6%A0%87%E7%AD%BE%E4%B8%AD%E5%B1%9E%E6%80%A7%E5%80%BC%EF%BC%9A"><span class="toc-text">3.4.7 获取标签中属性值：</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-4-8-%E6%A0%87%E7%AD%BE%E6%A0%91%E7%9A%84%E9%81%8D%E5%8E%86"><span class="toc-text">3.4.8 标签树的遍历</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#3-4-8-1-%E6%A0%87%E7%AD%BE%E6%A0%91%E7%9A%84%E4%B8%8B%E8%A1%8C%E9%81%8D%E5%8E%86"><span class="toc-text">3.4.8.1 标签树的下行遍历</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#3-4-8-2-%E6%A0%87%E7%AD%BE%E6%A0%91%E7%9A%84%E4%B8%8A%E8%A1%8C%E9%81%8D%E5%8E%86"><span class="toc-text">3.4.8.2 标签树的上行遍历</span></a></li></ol></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-5-xpath%E8%A7%A3%E6%9E%90%E5%9F%BA%E7%A1%80"><span class="toc-text">3.5 xpath解析基础</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-5-2-xpath%E5%AE%9E%E6%88%98"><span class="toc-text">3.5.2 xpath实战</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E2%91%A0%E6%A1%88%E4%BE%8B%E4%B8%80%EF%BC%9A58%E4%BA%8C%E6%89%8B%E6%88%BF"><span class="toc-text">①案例一：58二手房</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%9B%9B%E3%80%81%E6%A8%A1%E6%8B%9F%E7%99%BB%E5%BD%95"><span class="toc-text">四、模拟登录</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-1-%E6%A8%A1%E6%8B%9F%E7%99%BB%E5%BD%95Cookie%E6%93%8D%E4%BD%9C"><span class="toc-text">4.1 模拟登录Cookie操作</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-2-%E5%8F%8D%E7%9B%97%E9%93%BE"><span class="toc-text">4.2 反盗链</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-3-%E4%BB%A3%E7%90%86"><span class="toc-text">4.3 代理</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E5%85%AD%E7%AB%A0-%E5%BC%82%E6%AD%A5%E7%88%AC%E8%99%AB"><span class="toc-text">第六章 异步爬虫</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-1%E5%BC%82%E6%AD%A5%E7%88%AC%E8%99%AB%E7%9A%84%E6%96%B9%E5%BC%8F%EF%BC%9A"><span class="toc-text">6.1异步爬虫的方式：</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E5%A4%9A%E7%BA%BF%E7%A8%8B%E3%80%81%E5%A4%9A%E8%BF%9B%E7%A8%8B%EF%BC%88%E4%B8%8D%E5%BB%BA%E8%AE%AE%EF%BC%89%EF%BC%9A"><span class="toc-text">1. 多线程、多进程（不建议）：</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-2-%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"><span class="toc-text">6.2 代码实现</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E2%91%A0%E5%8D%95%E7%BA%BF%E7%A8%8B%E7%88%AC%E5%8F%96%E6%95%B0%E6%8D%AE"><span class="toc-text">①单线程爬取数据</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E7%BA%BF%E7%A8%8B%E6%B1%A0%E3%80%81%E8%BF%9B%E7%A8%8B%E6%B1%A0%EF%BC%88%E9%80%82%E5%BD%93%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%89%EF%BC%9A"><span class="toc-text">2. 线程池、进程池（适当的使用）：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E2%91%A1%E7%BA%BF%E7%A8%8B%E6%B1%A0%E7%88%AC%E5%8F%96%E6%95%B0%E6%8D%AE"><span class="toc-text">②线程池爬取数据</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E2%91%A2%E7%88%AC%E8%99%AB%E4%B8%AD%E5%BA%94%E7%94%A8%E7%BA%BF%E7%A8%8B%E6%B1%A0%EF%BC%88%E5%8A%A8%E6%80%81%E5%8A%A0%E8%BD%BD%E7%9A%84video%E6%A0%87%E7%AD%BE%E5%BE%85%E8%A7%A3%E5%86%B3"><span class="toc-text">③爬虫中应用线程池（动态加载的video标签待解决)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E5%8D%95%E7%BA%BF%E7%A8%8B-%E5%BC%82%E6%AD%A5%E5%8D%8F%E7%A8%8B%EF%BC%88%E6%8E%A8%E8%8D%90%EF%BC%89%EF%BC%9A"><span class="toc-text">3. 单线程 + 异步协程（推荐）：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E2%91%A3%E5%8D%8F%E7%A8%8B"><span class="toc-text">④协程</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E2%91%A4%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%BC%82%E6%AD%A5%E5%8D%8F%E7%A8%8B01"><span class="toc-text">⑤多任务异步协程01</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E2%91%A5%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%BC%82%E6%AD%A5%E5%8D%8F%E7%A8%8B02"><span class="toc-text">⑥多任务异步协程02</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E2%91%A6aiohttp%E5%AE%9E%E7%8E%B0%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%BC%82%E6%AD%A5%E5%8D%8F%E7%A8%8B"><span class="toc-text">⑦aiohttp实现多任务异步协程</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BC%82%E6%AD%A5%E5%8D%8F%E7%A8%8B%E7%88%AC%E8%99%AB%E5%AE%9E%E6%88%98"><span class="toc-text">异步协程爬虫实战</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E4%B8%83%E7%AB%A0-selenium"><span class="toc-text">第七章 selenium</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#7-1-webdriver%E6%8F%90%E4%BE%9B%E5%A4%A7%E9%87%8F%E6%96%B9%E6%B3%95%E6%9F%A5%E8%AF%A2%E9%A1%B5%E9%9D%A2%E4%B8%AD%E7%9A%84%E5%85%83%E7%B4%A0"><span class="toc-text">7.1 webdriver提供大量方法查询页面中的元素</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9E%E6%88%98%E6%A1%88%E4%BE%8B"><span class="toc-text">实战案例</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#7-2-%E5%8A%A8%E4%BD%9C%E9%93%BE%E5%92%8C-iframe-%E6%93%8D%E4%BD%9C"><span class="toc-text">7.2 动作链和 iframe 操作</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#7-2-%E6%97%A0%E5%A4%B4%E6%B5%8F%E8%A7%88%E5%99%A8"><span class="toc-text">7.2 无头浏览器</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#12306%E6%A8%A1%E6%8B%9F%E7%99%BB%E5%BD%95"><span class="toc-text">12306模拟登录</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#7-3-%E5%8F%8D%E5%8F%8D%E7%88%AC"><span class="toc-text">7.3 反反爬</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E5%85%AB%E7%AB%A0-%E9%AA%8C%E8%AF%81%E7%A0%81%E8%AF%86%E5%88%AB"><span class="toc-text">第八章 验证码识别</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E4%B9%9D%E7%AB%A0-scrapy-%E6%A1%86%E6%9E%B6"><span class="toc-text">第九章 scrapy 框架</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#9-1-scrapy%E6%A1%86%E6%9E%B6%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8"><span class="toc-text">9.1 scrapy框架的基本使用</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#9-2-scrapy-%E6%95%B0%E6%8D%AE%E8%A7%A3%E6%9E%90"><span class="toc-text">9.2 scrapy 数据解析</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#9-3-scrapy-%E6%8C%81%E4%B9%85%E5%8C%96%E5%AD%98%E5%82%A8"><span class="toc-text">9.3 scrapy 持久化存储</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E2%85%A0%E7%9F%A5%E8%AF%86%E7%82%B9"><span class="toc-text">Ⅰ知识点</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E2%85%A1%E4%BB%A3%E7%A0%81%E9%83%A8%E5%88%86"><span class="toc-text">Ⅱ代码部分</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E2%85%A2%E9%9D%A2%E8%AF%95%E9%A2%98"><span class="toc-text">Ⅲ面试题</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#9-4%E5%9F%BA%E4%BA%8E%E6%98%AFspider%E7%9A%84%E5%85%A8%E7%AB%99%E6%95%B0%E6%8D%AE%E7%88%AC%E5%8F%96"><span class="toc-text">9.4基于是spider的全站数据爬取</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#9-5-%E4%BA%94%E5%A4%A7%E6%A0%B8%E5%BF%83%E7%BB%84%E4%BB%B6"><span class="toc-text">9.5 五大核心组件</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#9-6-%E8%AF%B7%E6%B1%82%E4%BC%A0%E5%8F%82"><span class="toc-text">9.6 请求传参</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#9-7-7-%E5%9B%BE%E7%89%87%E6%95%B0%E6%8D%AE%E7%88%AC%E5%8F%96%E4%B9%8BImagesPipeline"><span class="toc-text">9.7 7.图片数据爬取之ImagesPipeline</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#9-8-%E4%B8%AD%E9%97%B4%E4%BB%B6-MiddleWare"><span class="toc-text">9.8 中间件 MiddleWare</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E2%91%A0%E7%9F%A5%E8%AF%86%E7%82%B9"><span class="toc-text">①知识点</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E2%91%A1%E4%BB%A3%E7%A0%81"><span class="toc-text">②代码</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E2%85%A1%E6%8B%A6%E6%88%AA%E5%93%8D%E5%BA%94"><span class="toc-text">Ⅱ拦截响应</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#9-9-CrawlSpider"><span class="toc-text">9.9 CrawlSpider</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#9-10-%E5%88%86%E5%B8%83%E5%BC%8F%E7%88%AC%E8%99%AB"><span class="toc-text">9.10 分布式爬虫</span></a></li></ol></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2022/09/14/MybatisNotes/" title="MybatisNotes"><img src="http://imgoss.cnu.cc/2204/27/a4383d0f926b346eb1fe8ff128e5b703.jpg?x-oss-process=style/content" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="MybatisNotes"/></a><div class="content"><a class="title" href="/2022/09/14/MybatisNotes/" title="MybatisNotes">MybatisNotes</a><time datetime="2022-09-14T01:42:07.000Z" title="发表于 2022-09-14 09:42:07">2022-09-14</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/09/09/spiderNotes/" title="spyderNotes"><img src="http://cache.yisu.com/upload/admin/customer_case_img/2019-08-08/1565261709.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="spyderNotes"/></a><div class="content"><a class="title" href="/2022/09/09/spiderNotes/" title="spyderNotes">spyderNotes</a><time datetime="2022-09-09T12:34:08.000Z" title="发表于 2022-09-09 20:34:08">2022-09-09</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/07/14/3%20.%20Gradient%20descent/" title="3. Gradient descent"><img src="http://imgoss.cnu.cc/2204/27/a4383d0f926b346eb1fe8ff128e5b703.jpg?x-oss-process=style/content" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="3. Gradient descent"/></a><div class="content"><a class="title" href="/2022/07/14/3%20.%20Gradient%20descent/" title="3. Gradient descent">3. Gradient descent</a><time datetime="2022-07-14T06:15:56.000Z" title="发表于 2022-07-14 14:15:56">2022-07-14</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/07/13/1.Regression/" title="1. Regression"><img src="http://imgoss.cnu.cc/2204/27/a4383d0f926b346eb1fe8ff128e5b703.jpg?x-oss-process=style/content" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="1. Regression"/></a><div class="content"><a class="title" href="/2022/07/13/1.Regression/" title="1. Regression">1. Regression</a><time datetime="2022-07-13T07:21:44.000Z" title="发表于 2022-07-13 15:21:44">2022-07-13</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/07/13/2.%20Error%20origin/" title="2. Error origin"><img src="http://imgoss.cnu.cc/2204/27/a4383d0f926b346eb1fe8ff128e5b703.jpg?x-oss-process=style/content" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="2. Error origin"/></a><div class="content"><a class="title" href="/2022/07/13/2.%20Error%20origin/" title="2. Error origin">2. Error origin</a><time datetime="2022-07-13T06:45:40.000Z" title="发表于 2022-07-13 14:45:40">2022-07-13</time></div></div></div></div></div></div></main><footer id="footer" style="background: #000000"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2022 By 是甜豆腐脑</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">试着和曾经仰望的人并肩</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>