<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>1. Regression | 是甜豆腐脑</title><meta name="keywords" content="Regression,机器学习"><meta name="author" content="是甜豆腐脑"><meta name="copyright" content="是甜豆腐脑"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="Regression是机器学习的第一课">
<meta property="og:type" content="article">
<meta property="og:title" content="1. Regression">
<meta property="og:url" content="http://franny77.github.io/2022/07/13/1.Regression/index.html">
<meta property="og:site_name" content="是甜豆腐脑">
<meta property="og:description" content="Regression是机器学习的第一课">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://imgoss.cnu.cc/2204/27/a4383d0f926b346eb1fe8ff128e5b703.jpg?x-oss-process=style/content">
<meta property="article:published_time" content="2022-07-13T07:21:44.000Z">
<meta property="article:modified_time" content="2022-09-22T08:17:09.557Z">
<meta property="article:author" content="是甜豆腐脑">
<meta property="article:tag" content="Regression">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://imgoss.cnu.cc/2204/27/a4383d0f926b346eb1fe8ff128e5b703.jpg?x-oss-process=style/content"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://franny77.github.io/2022/07/13/1.Regression/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: ture
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '1. Regression',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-09-22 16:17:09'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 5.4.2"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/./img/tx.JPG" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">19</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">18</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 链接</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></li><li><a class="site-page child" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('http://imgoss.cnu.cc/2204/27/a4383d0f926b346eb1fe8ff128e5b703.jpg?x-oss-process=style/content')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">是甜豆腐脑</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 链接</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></li><li><a class="site-page child" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">1. Regression</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-07-13T07:21:44.000Z" title="发表于 2022-07-13 15:21:44">2022-07-13</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-09-22T08:17:09.557Z" title="更新于 2022-09-22 16:17:09">2022-09-22</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">7.3k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>27分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="1. Regression"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1QX4y1F79d?spm_id_from=333.1007.top_right_bar_window_custom_collection.content.click">参考课程</a></p>
<h1 id="Regression：Case-Study"><a href="#Regression：Case-Study" class="headerlink" title="Regression：Case Study"></a>Regression：Case Study</h1><blockquote>
<p><strong>回归</strong>-案例研究</p>
</blockquote>
<h4 id="问题的导入：预测宝可梦的CP值"><a href="#问题的导入：预测宝可梦的CP值" class="headerlink" title="问题的导入：预测宝可梦的CP值"></a>问题的导入：预测宝可梦的CP值</h4><p>Estimating the Combat Power(CP) of a pokemon after evolution </p>
<p>我们期望根据已有的宝可梦进化前后的信息，来预测某只宝可梦进化后的cp值的大小</p>
<h4 id="确定Senario、Task和Model"><a href="#确定Senario、Task和Model" class="headerlink" title="确定Senario、Task和Model"></a>确定Senario、Task和Model</h4><h5 id="Senario"><a href="#Senario" class="headerlink" title="Senario"></a>Senario</h5><p>首先根据已有的data来确定Senario，我们拥有宝可梦进化前后cp值的这样一笔数据，input是进化前的宝可梦(包括它的各种属性)，output是进化后的宝可梦的cp值；因此我们的data是labeled，使用的Senario是<strong>Supervised Learning</strong></p>
<h5 id="Task"><a href="#Task" class="headerlink" title="Task"></a>Task</h5><p>然后根据我们想要function的输出类型来确定Task，我们预期得到的是宝可梦进化后的cp值，是一个scalar，因此使用的Task是<strong>Regression</strong></p>
<h5 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h5><p>关于Model，选择很多，这里采用的是<strong>Non-linear Model</strong></p>
<h4 id="设定具体参数"><a href="#设定具体参数" class="headerlink" title="设定具体参数"></a>设定具体参数</h4><p>$X$：   表示一只宝可梦，用下标表示该宝可梦的某种属性</p>
<p>$X_{cp}$：表示该宝可梦进化前的cp值</p>
<p>$X_s$：  表示该宝可梦是属于哪一种物种，比如妙瓜种子、皮卡丘…</p>
<p>$X_{hp}$：表示该宝可梦的hp值即生命值是多少</p>
<p>$X_w$： 代表该宝可梦的重重量</p>
<p>$X_h$： 代表该宝可梦的高度</p>
<p>$f()$： 表示我们要找的function</p>
<p>$y$：    表示function的output，即宝可梦进化后的cp值，是一个scalar</p>
<h4 id="Regression的具体过程"><a href="#Regression的具体过程" class="headerlink" title="Regression的具体过程"></a>Regression的具体过程</h4><h5 id="回顾一下machine-Learning的三个步骤："><a href="#回顾一下machine-Learning的三个步骤：" class="headerlink" title="回顾一下machine Learning的三个步骤："></a>回顾一下machine Learning的三个步骤：</h5><ul>
<li>定义一个model即function set</li>
<li>定义一个goodness of function损失函数去评估该function的好坏</li>
<li>找一个最好的function</li>
</ul>
<h5 id="Step1：Model-function-set"><a href="#Step1：Model-function-set" class="headerlink" title="Step1：Model (function set)"></a>Step1：Model (function set)</h5><p>如何选择一个function的模型呢？毕竟只有确定了模型才能调参。这里没有明确的思路，只能凭经验去一种种地试</p>
<h6 id="Linear-Model-线性模型"><a href="#Linear-Model-线性模型" class="headerlink" title="Linear Model 线性模型"></a>Linear Model 线性模型</h6><p>$y&#x3D;b+w \cdot X_{cp}$ </p>
<p>y代表进化后的cp值，$X_{cp}$代表进化前的cp值，w和b代表未知参数，可以是任何数值</p>
<p>根据不同的w和b，可以确定不同的无穷无尽的function，而$y&#x3D;b+w \cdot X_{cp}$这个抽象出来的式子就叫做model，是以上这些具体化的function的集合，即function set</p>
<p>实际上这是一种<strong>Linear Model</strong>，但只考虑了宝可梦进化前的cp值，因而我们可以将其扩展为：</p>
<p>&#x3D;&#x3D;$y&#x3D;b+ \sum w_ix_i$&#x3D;&#x3D;</p>
<p>**x<del>i</del>**： an attribute of input X  ( x<del>i</del> is also called <strong>feature</strong>，即特征值)</p>
<p>**w<del>i</del>**：weight of x<del>i</del></p>
<p><strong>b</strong>：  bias</p>
<p><img src="https://img-blog.csdnimg.cn/191b86684ef148f3a883e81f594c24bb.png#pic_center" alt="在这里插入图片描述"></p>
<h5 id="Step2：Goodness-of-Function-函数优度"><a href="#Step2：Goodness-of-Function-函数优度" class="headerlink" title="Step2：Goodness of Function 函数优度"></a>Step2：Goodness of Function 函数优度</h5><h6 id="参数说明"><a href="#参数说明" class="headerlink" title="参数说明"></a>参数说明</h6><p>$x^i$：用上标来表示一个完整的object的编号，$x^{i}$表示第i只宝可梦(下标表示该object中的component)</p>
<p>$\widehat{y}^i$：用$\widehat{y}$表示一个实际观察到的object输出，上标为i表示是第i个object</p>
<p>注：由于regression的输出值是scalar，因此$\widehat{y}$里面并没有component，只是一个简单的数值；但是未来如果考虑structured Learning的时候，我们output的object可能是有structured的，所以我们还是会需要用上标下标来表示一个完整的output的object和它包含的component</p>
<h6 id="Loss-function-损失函数"><a href="#Loss-function-损失函数" class="headerlink" title="Loss function 损失函数"></a>Loss function 损失函数</h6><p>为了衡量 function set 中的某个 function 的好坏，我们需要一个评估函数，即 &#x3D;&#x3D;Loss function&#x3D;&#x3D; ，损失函数，简称 <code>L</code>；<code>loss function</code> 是一个function的function</p>
<p>$$L(f)&#x3D;L(w,b)$$</p>
<p>input：a function;</p>
<p>output：how bad&#x2F;good it is;</p>
<p>由于$f:y&#x3D;b+w \cdot x_{cp}$，即 <code>f</code> 是由 <code>b</code> 和 <code>w</code> 决定的，因此 <code>input f</code> 就等价于 <code>input</code> 这个 <code>f</code> 里的 <code>b</code> 和 <code>w</code>，因此 &#x3D;&#x3D;Loss function 实际上是在衡量一组参数的好坏&#x3D;&#x3D;</p>
<p>之前提到的 model 是由我们自主选择的，这里的 loss function 也是，最常用的方法就是采用类似于方差和的形式来衡量参数的好坏，即预测值与真值差的平方和；这里真正的数值减估测数值的平方，叫做估测误差，Estimation error，将10个估测误差合起来就是 loss function</p>
<p>$$ L(f)&#x3D;L(w,b)&#x3D;\sum_{n&#x3D;1}^{10}(\widehat{y}^n-(b+w \cdot {x}^n_{cp}))^2$$</p>
<p>如果$L(f)$越大，说明该function表现得越不好；$L(f)$越小，说明该function表现得越好</p>
<h6 id="Loss-function可视化"><a href="#Loss-function可视化" class="headerlink" title="Loss function可视化"></a>Loss function可视化</h6><p>下图中是 loss function 的可视化，该图中的每一个点都代表一组<code>(w,b)</code>，也就是对应着一个<code>function</code>；而该点的颜色对应着的 loss function 的结果<code>L(w,b)</code>，它表示该点对应 function 的表现有多糟糕，颜色越偏红色代表 Loss 的数值越大，这个 function 的表现越不好，越偏蓝色代表 Loss 的数值越小，这个 function 的表现越好</p>
<p>比如图中用红色箭头标注的点就代表了b&#x3D;-180 , w&#x3D;-2对应的function，即$y&#x3D;-180-2 \cdot x_{cp}$，该点所在的颜色偏向于红色区域，因此这个 function 的 loss 比较大，表现并不好</p>
<p><img src="https://img-blog.csdnimg.cn/e460cfc3d6164e1b8b28826528da06cb.png#pic_center" alt="在这里插入图片描述"></p>
<h5 id="Step3：Pick-the-Best-Function"><a href="#Step3：Pick-the-Best-Function" class="headerlink" title="Step3：Pick the Best Function"></a>Step3：Pick the Best Function</h5><p>我们已经确定了loss function，他可以衡量我们的model里面每一个 function 的好坏，接下来我们要做的事情就是，从这个 function set 里面，挑选一个最好的 function</p>
<p>挑选最好的 function 这一件事情，写成 formulation&#x2F;equation 的样子如下：</p>
<p>$$f^*&#x3D;{arg} \underset{f}{min} L(f)$$，或者是</p>
<p>$$w^*,b^*&#x3D;{arg}\ \underset{w,b}{min} L(w,b)&#x3D;{arg}\  \underset{w,b}{min} \sum\limits^{10}<em>{n&#x3D;1}(\widehat{y}^n-(b+w \cdot x^n</em>{cp}))^2$$</p>
<p>也就是那个使$L(f)&#x3D;L(w,b)&#x3D;Loss$最小的$f$或$(w,b)$，就是我们要找的$f^*$或$(w^*,b^*)$(有点像极大似然估计的思想)</p>
<p>利用线性代数的知识，可以解得这个 closed-form solution，但这里采用的是一种更为普遍的方法——&#x3D;&#x3D;gradient descent(梯度下降法)&#x3D;&#x3D;</p>
<h4 id="Gradient-Descent-梯度下降"><a href="#Gradient-Descent-梯度下降" class="headerlink" title="Gradient Descent 梯度下降"></a>Gradient Descent 梯度下降</h4><p>上面的例子比较简单，用线性代数的知识就可以解；但是对于更普遍的问题来说，&#x3D;&#x3D;gradient descent的厉害之处在于，只要$L(f)$是可微分的，gradient descent都可以拿来处理这个$f$，找到表现比较好的parameters&#x3D;&#x3D;</p>
<h5 id="单个参数的问题"><a href="#单个参数的问题" class="headerlink" title="单个参数的问题"></a>单个参数的问题</h5><p>以只带单个参数 w 的 Loss Function <code>L(w)</code> 为例，首先保证$L(w)$是<strong>可微</strong>的<br>$$w^*&#x3D;{arg}\ \underset{w}{min} L(w) $$ 我们的目标就是找到这个使 Loss 最小的$w^*$，实际上就是寻找切线 L 斜率为 0 的 global minima 全局最小值点(注意，存在一些 local minima 局部极小值点，其斜率也是 0 )</p>
<p>有一个暴力的方法是，穷举所有的 w 值，去找到使 loss 最小的$w^*$，但是这样做是没有效率的；<br>而 gradient descent 就是用来解决这个效率问题的</p>
<ul>
<li><p>首先随机选取一个初始的点 $w^0$ (当然也不一定要随机选取，如果有办法可以得到比较接近$w^*$的表现得比较好的 $w^0$当 初始点，可以有效地提高查找 $w^*$ 的效率)</p>
</li>
<li><p>计算 $L$ 在 $w&#x3D;w^0$ 的位置的微分，即 $\frac{dL}{dw}|_{w&#x3D;w^0}$ ，几何意义即切线的斜率</p>
</li>
<li><p>如果切线斜率是 negative 负的，那么就应该使w变大，即往右踏一步；如果切线斜率是 positive 正的，那么就应该使w变小，即往左踏一步，每一步的步长 step size 就是 w 的改变量<br>  w的改变量 step size 的大小取决于两件事</p>
<ul>
<li><p>一是现在的微分值 $\frac{dL}{dw}$ 有多大，微分值越大代表现在在一个越陡峭的地方，那它要移动的距离就越大，反之就越小；</p>
</li>
<li><p>二是一个常数项 $η$ ，被称为 &#x3D;&#x3D;learning rate&#x3D;&#x3D;，即学习率，它决定了每次踏出的 step size 不只取决于现在的斜率，还取决于一个事先就定好的数值，如果   learning rate 比较大，那每踏出一步的时候，参数 w 更新的幅度就比较大，反之参数更新的幅度就比较小</p>
<p>  如果 learning rate 设置的大一些，那机器学习的速度就会比较快；但是 learning rate 如果太大，可能就会跳过最合适的 global minima 的点</p>
</li>
</ul>
</li>
<li><p>因此每次参数更新的大小是 $η \frac{dL}{dw}$，为了满足斜率为负时w变大，斜率为正时w变小，应当使原来的w减去更新的数值，即<br>  $$<br>  w^1&#x3D;w^0-η \frac{dL}{dw}|<em>{w&#x3D;w^0} \<br>  w^2&#x3D;w^1-η \frac{dL}{dw}|</em>{w&#x3D;w^1} \<br>  w^3&#x3D;w^2-η \frac{dL}{dw}|<em>{w&#x3D;w^2} \<br>  … \<br>  w^{i+1}&#x3D;w^i-η \frac{dL}{dw}|</em>{w&#x3D;w^i} \<br>  if\ \ (\frac{dL}{dw}|_{w&#x3D;w^i}&#x3D;&#x3D;0) \ \ then \ \ stop;<br>  $$<br>  此时 $w^i$ 对应的斜率为0，我们找到了一个极小值 local minima，这就出现了一个问题，当微分为0的时候，参数就会一直卡在这个点上没有办法再更新了，因此通过 gradient descent 找出来的 solution 其实并不是最佳解 global minima</p>
<p>  但幸运的是，在 linear regression上，是没有 local minima 的，因此可以使用这个方法</p>
</li>
</ul>
<h5 id="两个参数的问题"><a href="#两个参数的问题" class="headerlink" title="两个参数的问题"></a>两个参数的问题</h5><p>今天要解决的关于宝可梦的问题，是含有two parameters的问题，即$(w^*,b^*)&#x3D;arg\ \underset{w,b} {min} L(w,b)$</p>
<p>当然，它本质上处理单个参数的问题是一样的</p>
<ul>
<li><p>首先，也是随机选取两个初始值，$w^0$和$b^0$</p>
</li>
<li><p>然后分别计算$(w^0,b^0)$这个点上，L对w和b的偏微分，即$\frac{\partial L}{\partial w}|<em>{w&#x3D;w^0,b&#x3D;b^0}$ 和 $\frac{\partial L}{\partial b}|</em>{w&#x3D;w^0,b&#x3D;b^0}$</p>
</li>
<li><p>更新参数，当迭代跳出时，$(w^i,b^i)$对应着极小值点<br>  $$<br>  w^1&#x3D;w^0-η\frac{\partial L}{\partial w}|<em>{w&#x3D;w^0,b&#x3D;b^0} \ \ \ \ \ \ \ \  \ b^1&#x3D;b^0-η\frac{\partial L}{\partial b}|</em>{w&#x3D;w^0,b&#x3D;b^0} \<br>  w^2&#x3D;w^1-η\frac{\partial L}{\partial w}|<em>{w&#x3D;w^1,b&#x3D;b^1} \ \ \ \ \ \ \ \  \ b^2&#x3D;b^1-η\frac{\partial L}{\partial b}|</em>{w&#x3D;w^1,b&#x3D;b^1} \<br>  … \<br>  w^{i+1}&#x3D;w^{i}-η\frac{\partial L}{\partial w}|<em>{w&#x3D;w^{i},b&#x3D;b^{i}} \ \ \ \ \ \ \ \  \ b^{i+1}&#x3D;b^{i}-η\frac{\partial L}{\partial b}|</em>{w&#x3D;w^{i},b&#x3D;b^{i}} \<br>  if(\frac{\partial L}{\partial w}&#x3D;&#x3D;0 &amp;&amp; \frac{\partial L}{\partial b}&#x3D;&#x3D;0) \ \ \ then \ \ stop<br>  $$</p>
</li>
</ul>
<p>实际上，L 的gradient就是微积分中的那个梯度的概念，即<br>$$<br>\nabla L&#x3D;<br>\begin{bmatrix}<br>\frac{\partial L}{\partial w} \<br>\frac{\partial L}{\partial b}<br>\end{bmatrix}_{gradient}<br>$$<br>可视化效果如下：(三维坐标显示在二维图像中，loss的值用颜色来表示)<br><img src="https://img-blog.csdnimg.cn/516c94e431be40a2ad5f2e6a003cd533.png#pic_center" alt="在这里插入图片描述"></p>
<p>横坐标是b，纵坐标是w，颜色代表loss的值，越偏蓝色表示loss越小，越偏红色表示loss越大</p>
<p><strong>每次计算得到的梯度gradient，即由$\frac{\partial L}{\partial b}和\frac{\partial L}{\partial w}$组成的vector向量，就是该等高线的法线方向(对应图中红色箭头的反方向)；而$(-η\frac{\partial L}{\partial b},-η\frac{\partial L}{\partial w})$的作用就是让原先的$(w^i,b^i)$朝着gradient的反方向即等高线法线方向前进，其中η(learning rate)的作用是每次更新的跨度(对应图中红色箭头的长度)；经过多次迭代，最终gradient达到极小值点</strong></p>
<p>注：这里两个方向的η(learning rate)必须保持一致，这样每次更新坐标的step size是等比例缩放的，保证坐标前进的方向始终和梯度下降的方向一致；否则坐标前进的方向将会发生偏移</p>
<p><img src="https://img-blog.csdnimg.cn/267a0c060fa640a4a73b1addfd70dfc0.png#pic_center" alt="在这里插入图片描述"></p>
<h5 id="Gradient-Descent的缺点"><a href="#Gradient-Descent的缺点" class="headerlink" title="Gradient Descent的缺点"></a>Gradient Descent的缺点</h5><p>gradient descent有一个令人担心的地方，也就是我之前一直提到的，它每次迭代完毕，寻找到的梯度为0的点必然是极小值点，local minima；却不一定是最小值点，global minima</p>
<p>这会造成一个问题是说，如果loss function长得比较坑坑洼洼(极小值点比较多)，而每次初始化$w^0$的取值又是随机的，这会造成每次gradient descent停下来的位置都可能是不同的极小值点；而且当遇到梯度比较平缓(gradient≈0)的时候，gradient descent也可能会效率低下甚至可能会stuck卡住；也就是说通过这个方法得到的结果，是看人品的(滑稽</p>
<p><img src="https://img-blog.csdnimg.cn/7798437eb0314a8597b5e835183be990.png#pic_center" alt="在这里插入图片描述"></p>
<p>但是！在&#x3D;&#x3D;linear regression&#x3D;&#x3D;里，loss function实际上是<strong>convex</strong>的，是一个 <strong>凸函数</strong> ，是没有local optimal局部最优解的，他只有一个global minima，visualize出来的图像就是从里到外一圈一圈包围起来的椭圆形的等高线(就像前面的等高线图)，因此随便选一个起始点，根据gradient descent最终找出来的，都会是同一组参数</p>
<h4 id="回到pokemon的问题上来"><a href="#回到pokemon的问题上来" class="headerlink" title="回到pokemon的问题上来"></a>回到pokemon的问题上来</h4><h5 id="偏微分的计算"><a href="#偏微分的计算" class="headerlink" title="偏微分的计算"></a>偏微分的计算</h5><p>现在我们来求具体的L对w和b的偏微分<br>$$<br>L(w,b)&#x3D;\sum\limits_{n&#x3D;1}^{10}(\widehat{y}^n-(b+w\cdot x_{cp}^n))^2 \<br>\frac{\partial L}{\partial w}&#x3D;\sum\limits_{n&#x3D;1}^{10}2(\widehat{y}^n-(b+w\cdot x_{cp}^n))(-x_{cp}^n) \<br>\frac{\partial L}{\partial b}&#x3D;\sum\limits_{n&#x3D;1}^{10}2(\widehat{y}^n-(b+w\cdot x_{cp}^n))(-1)<br>$$</p>
<h5 id="How’s-the-results"><a href="#How’s-the-results" class="headerlink" title="How’s the results?"></a>How’s the results?</h5><p>根据gradient descent，我们得到的$y&#x3D;b+w\cdot x_{cp}$中最好的参数是b&#x3D;-188.4, w&#x3D;2.7</p>
<p>我们需要有一套评估系统来评价我们得到的最后这个function和实际值的误差error的大小；这里我们将training data里每一只宝可梦 $i$ 进化后的实际cp值与预测值之差的绝对值叫做$e^i$，而这些误差之和Average Error on Training Data为$\sum\limits_{i&#x3D;1}^{10}e^i&#x3D;31.9$</p>
<blockquote>
<p>What we really care about is the error on new data (testing data)</p>
</blockquote>
<p>当然我们真正关心的是generalization的case，也就是用这个model去估测新抓到的pokemon，误差会有多少，这也就是所谓的testing data的误差；于是又抓了10只新的pokemon，算出来的Average Error on Testing Data为$\sum\limits_{i&#x3D;1}^{10}e^i&#x3D;35.0$；可见training data里得到的误差一般是要比testing data要小，这也符合常识</p>
<p><img src="https://img-blog.csdnimg.cn/ececfd18b5d44ed7973e8e9e9bd39b0b.png" alt="result"></p>
<h5 id="How-can-we-do-better"><a href="#How-can-we-do-better" class="headerlink" title="How can we do better?"></a>How can we do better?</h5><p>我们有没有办法做得更好呢？这时就需要我们重新去设计model；如果仔细观察一下上图的data，就会发现在原先的cp值比较大和比较小的地方，预测值是相当不准的</p>
<p>实际上，从结果来看，最终的function可能不是一条直线，可能是稍微更复杂一点的曲线</p>
<h6 id="考虑-x-cp-2-的model"><a href="#考虑-x-cp-2-的model" class="headerlink" title="考虑$(x_{cp})^2$的model"></a>考虑$(x_{cp})^2$的model</h6><p><img src="https://img-blog.csdnimg.cn/7b3ea5d1698f49d582fab3ac93779b30.png" alt="在这里插入图片描述"></p>
<h6 id="考虑-x-cp-3-的model"><a href="#考虑-x-cp-3-的model" class="headerlink" title="考虑$(x_{cp})^3$的model"></a>考虑$(x_{cp})^3$的model</h6><p><img src="https://img-blog.csdnimg.cn/319b40fa3bd2452c9a3dd17ff4a9f50c.png" alt="在这里插入图片描述"></p>
<h6 id="考虑-x-cp-4-的model"><a href="#考虑-x-cp-4-的model" class="headerlink" title="考虑$(x_{cp})^4$的model"></a>考虑$(x_{cp})^4$的model</h6><p><img src="https://img-blog.csdnimg.cn/47807ba8025f44c0a3e729dbdf101cf9.png" alt="在这里插入图片描述"></p>
<h6 id="考虑-x-cp-5-的model"><a href="#考虑-x-cp-5-的model" class="headerlink" title="考虑$(x_{cp})^5$的model"></a>考虑$(x_{cp})^5$的model</h6><p><img src="https://img-blog.csdnimg.cn/f01a00d8ce65424fa9470b665505919a.png" alt="在这里插入图片描述"></p>
<h6 id="5个model的对比"><a href="#5个model的对比" class="headerlink" title="5个model的对比"></a>5个model的对比</h6><p>这5个model的training data的表现：随着$(x_{cp})^i$的高次项的增加，对应的average error会不断地减小；实际上这件事情非常容易解释，实际上低次的式子是高次的式子的特殊情况(令高次项$(X_{cp})^i$对应的$w_i$为0，高次式就转化成低次式)</p>
<p>也就是说，在gradient descent可以找到best function的前提下(多次式为Non-linear model，存在local optimal局部最优解，gradient descent不一定能找到global minima)，function所包含的项的次数越高，越复杂，error在training data上的表现就会越来越小；但是，我们关心的不是model在training data上的error表现，而是model在testing data上的error表现</p>
<p><img src="https://img-blog.csdnimg.cn/83e6a832f0c64a2bb05b3cd033df8364.png" alt="&lt;center&gt;&lt;img src=&quot;https://gitee.com/Sakura-gh/ML-notes/raw/master/img/Xcp-compare.png&quot; alt=&quot;compare&quot; style=&quot;width:60%;&quot; /&gt;&lt;/center&gt;"></p>
<p>在training data上，model越复杂，error就会越低；但是在testing data上，model复杂到一定程度之后，error非但不会减小，反而会暴增，在该例中，从含有$(X_{cp})^4$项的model开始往后的model，testing data上的error出现了大幅增长的现象，通常被称为<strong>overfitting过拟合</strong></p>
<p><img src="https://img-blog.csdnimg.cn/29708f57c7b8411bb39bb070bdc6503e.png" alt="&lt;center&gt;&lt;img src=&quot;https://gitee.com/Sakura-gh/ML-notes/raw/master/img/Xcp-overfitting.png&quot; alt=&quot;overfitting&quot; style=&quot;width:60%;&quot; /&gt;&lt;/center&gt;"></p>
<p>因此model不是越复杂越好，而是选择一个最适合的model，在本例中，包含$(X_{cp})^3$的式子是最适合的model</p>
<h5 id="进一步讨论其他参数"><a href="#进一步讨论其他参数" class="headerlink" title="进一步讨论其他参数"></a>进一步讨论其他参数</h5><h6 id="物种-x-s-的影响"><a href="#物种-x-s-的影响" class="headerlink" title="物种$x_s$的影响"></a>物种$x_s$的影响</h6><p>之前我们的model只考虑了宝可梦进化前的cp值，这显然是不对的，除了cp值外，还受到物种$x_s$的影响</p>
<p><img src="https://img-blog.csdnimg.cn/588b549112874523bebbbed38cda65d9.png" alt="hiden factor"></p>
<p>因此我们重新设计model：<br>$$<br>if \ \ x_s&#x3D;Pidgey: \ \ \ \ \ \ \ y&#x3D;b_1+w_1\cdot x_{cp} \<br>if \ \ x_s&#x3D;Weedle: \ \ \ \ \ \ y&#x3D;b_2+w_2\cdot x_{cp} \<br>if \ \ x_s&#x3D;Caterpie: \ \ \ \ y&#x3D;b_3+w_3\cdot x_{cp} \<br>if \ \ x_s&#x3D;Eevee: \ \ \ \ \ \ \ \ \ y&#x3D;b_4+w_4\cdot x_{cp}<br>$$<br>也就是根据不同的物种，设计不同的linear model(这里$x_s&#x3D;species \ of \ x$)，那如何将上面的四个if语句合并成一个linear model呢？</p>
<p>这里引入$δ(条件表达式)$的概念，当条件表达式为true，则δ为1；当条件表达式为false，则δ为0，因此可以通过下图的方式，将4个if语句转化成同一个linear model</p>
<p><img src="https://img-blog.csdnimg.cn/c722fedbfbad4e5f982a1c5847465404.png" alt="new-model"></p>
<p>有了上面这个model以后，我们分别得到了在training data和testing data上测试的结果：<br><img src="https://img-blog.csdnimg.cn/4bd2cb7a64f14b97a60a2b4f4d41fa35.png" alt="new-result"></p>
<h6 id="Hp值-x-hp-、height值-x-h-、weight值-x-w-的影响"><a href="#Hp值-x-hp-、height值-x-h-、weight值-x-w-的影响" class="headerlink" title="Hp值$x_{hp}$、height值$x_h$、weight值$x_w$的影响"></a>Hp值$x_{hp}$、height值$x_h$、weight值$x_w$的影响</h6><p>考虑所有可能有影响的参数，设计出这个最复杂的model：<br>$$<br>if \ \ x_s&#x3D;Pidgey: \ \  \ \ y’&#x3D;b_1+w_1\cdot x_{cp}+w_5\cdot(x_{cp})^2 \<br>if \ \ x_s&#x3D;Weedle: \ \ \ y’&#x3D;b_2+w_2\cdot x_{cp}+w_6\cdot(x_{cp})^2 \<br>if \ \ x_s&#x3D;Pidgey: \ \ \ y’&#x3D;b_3+w_3\cdot x_{cp}+w_7\cdot(x_{cp})^2 \<br>if \ \ x_s&#x3D;Eevee: \ \ \ \ y’&#x3D;b_4+w_4\cdot x_{cp}+w_8\cdot(x_{cp})^2 \<br>y&#x3D;y’+w_9\cdot x_{hp}+w_{10}\cdot(x_{hp})^2+w_{11}\cdot x_h+w_{12}\cdot (x_h)^2+w_{13}\cdot x_w+w_{14}\cdot (x_w)^2<br>$$<br>算出的training error&#x3D;1.9，但是，testing error&#x3D;102.3！<strong>这么复杂的model很大概率会发生overfitting</strong>(按照我的理解，overfitting实际上是我们多使用了一些input的变量或是变量的高次项使曲线跟training data拟合的更好，但不幸的是这些项并不是实际情况下被使用的，于是这个model在testing data上会表现得很糟糕)，overfitting就相当于是那个范围更大的韦恩图，它包含了更多的函数更大的范围，代价就是在准确度上表现得更糟糕</p>
<h6 id="regularization解决-overfitting-L2正则化解决过拟合问题"><a href="#regularization解决-overfitting-L2正则化解决过拟合问题" class="headerlink" title="regularization解决 overfitting (L2正则化解决过拟合问题)"></a>regularization解决 overfitting (L2正则化解决过拟合问题)</h6><blockquote>
<p>regularization 可以使曲线变得更加 smooth，training data 上的 error 变大，但是 testing data上的error变小。有关 regularization 的具体原理说明详见下一部分</p>
</blockquote>
<p>原来的 loss function 只考虑了 prediction 的 error，即 $\sum\limits_i^n(\widehat{y}^i-(b+\sum\limits_{j}w_jx_j))^2$ ；而 regularization 则是在原来的 loss function 的基础上加上了一项 $\lambda\sum(w_i)^2$，就是把这个 model 里面所有的 $w_i$ 的平方和用 λ 加权(其中 i 代表遍历 n 个training data，j 代表遍历 model 的每一项)</p>
<p>也就是说，<strong>我们期待参数 $w_i$ 越小甚至接近于 0 的 function，为什么呢？</strong></p>
<p>因为参数值接近0的function，是比较平滑的；&#x3D;&#x3D;所谓的平滑的意思是，当今天的输入有变化的时候，output对输入的变化是比较不敏感的&#x3D;&#x3D;</p>
<p>举例来说，对 $y&#x3D;b+\sum w_ix_i$ 这个 model，当 input 变化 $\Delta x_i$，output 的变化就是 $w_i\Delta x_i$，也就是说，如果 $w_i$ 越小越接近 0 的话，输出对输入就越不 sensitive 敏感，我们的 function 就是一个越平滑的 function；说到这里你会发现，我们之前没有把 bias——b 这个参数考虑进去的原因是 <strong>bias 的大小跟 function 的平滑程度是没有关系的</strong>，bias 值的大小只是把 function 上下移动而已</p>
<p><strong>那为什么我们喜欢比较平滑的 function 呢？</strong></p>
<p>如果我们有一个比较平滑的 function，由于输出对输入是不敏感的，测试的时候，一些 noises 噪声对这个平滑的 function 的影响就会比较小，而给我们一个比较好的结果<br><img src="https://img-blog.csdnimg.cn/aa94a6c9079c4313a217197b8c45be51.png" alt="在这里插入图片描述"></p>
<p><strong>注：这里的 λ 需要我们手动去调整以取得最好的值</strong></p>
<p>λ 值越大代表考虑 smooth 的那个 regularization 那一项的影响力越大，我们找到的 function 就越平滑</p>
<p>观察下图可知，当我们的λ越大的时候，在 training data 上得到的 error 其实是越大的，但是这件事情是非常合理的，因为当 λ 越大的时候，我们就越倾向于考虑 w 的值而越少考虑 error 的大小；但是有趣的是，虽然在training data上得到的error越大，但是在testing data上得到的error可能会是比较小的</p>
<p>下图中，当λ从0到100变大的时候，training error不断变大，testing error反而不断变小；但是当λ太大的时候(&gt;100)，在testing data上的error就会越来越大</p>
<p>&#x3D;&#x3D;我们喜欢比较平滑的function，因为它对noise不那么sensitive；但是我们又不喜欢太平滑的function，因为它就失去了对data拟合的能力；而function的平滑程度，就需要通过调整λ来决定&#x3D;&#x3D;，就像下图中，当λ&#x3D;100时，在testing data上的error最小，因此我们选择λ&#x3D;100</p>
<p>注：这里的error指的是$\frac{1}{n}\sum\limits_{i&#x3D;1}^n|\widehat{y}^i-y^i|$</p>
<p><img src="https://img-blog.csdnimg.cn/dee2bcf59624472c89246778dd9d102f.png" alt="在这里插入图片描述"></p>
<h4 id="conclusion总结"><a href="#conclusion总结" class="headerlink" title="conclusion总结"></a>conclusion总结</h4><h5 id="关于pokemon的cp值预测的流程总结："><a href="#关于pokemon的cp值预测的流程总结：" class="headerlink" title="关于pokemon的cp值预测的流程总结："></a>关于pokemon的cp值预测的流程总结：</h5><ul>
<li><p>根据已有的data特点(labeled data，包含宝可梦及进化后的 cp 值)，确定使用 supervised learning 监督学习</p>
</li>
<li><p>根据output的特点(输出的是 scalar 数值)，确定使用 regression 回归(linear or non-linear)</p>
</li>
<li><p>考虑包括进化前cp值、species、hp等各方面变量属性以及高次项的影响，我们的model可以采用这些input的一次项和二次型之和的形式，如：<br>  $$<br>  if \ \ x_s&#x3D;Pidgey: \ \  \ \ y’&#x3D;b_1+w_1\cdot x_{cp}+w_5\cdot(x_{cp})^2 \<br>  if \ \ x_s&#x3D;Weedle: \ \ \ y’&#x3D;b_2+w_2\cdot x_{cp}+w_6\cdot(x_{cp})^2 \<br>  if \ \ x_s&#x3D;Pidgey: \ \ \ y’&#x3D;b_3+w_3\cdot x_{cp}+w_7\cdot(x_{cp})^2 \<br>  if \ \ x_s&#x3D;Eevee: \ \ \ \ y’&#x3D;b_4+w_4\cdot x_{cp}+w_8\cdot(x_{cp})^2 \<br>  y&#x3D;y’+w_9\cdot x_{hp}+w_{10}\cdot(x_{hp})^2+w_{11}\cdot x_h+w_{12}\cdot (x_h)^2+w_{13}\cdot x_w+w_{14}\cdot (x_w)^2<br>  $$<br>  而为了保证function的平滑性，loss function应使用regularization，即$L&#x3D;\sum\limits_{i&#x3D;1}^n(\widehat{y}^i-y^i)^2+\lambda\sum\limits_{j}(w_j)^2$，注意bias——参数b对function平滑性无影响，因此不额外再次计入loss function(y的表达式里已包含w、b)</p>
</li>
<li><p>利用gradient descent对regularization版本的loss function进行梯度下降迭代处理，每次迭代都减去L对该参数的微分与learning rate之积，假设所有参数合成一个vector：$[w_0,w_1,w_2,…,w_j,…,b]^T$，那么每次梯度下降的表达式如下：<br>  $$<br>  梯度:<br>  \nabla L&#x3D;<br>  \begin{bmatrix}<br>  \frac{\partial L}{\partial w_0} \<br>  \frac{\partial L}{\partial w_1} \<br>  \frac{\partial L}{\partial w_2} \<br>  … \<br>  \frac{\partial L}{\partial w_j} \<br>  … \<br>  \frac{\partial L}{\partial b}<br>  \end{bmatrix}_{gradient}<br>  \ \ \<br>  gradient \ descent:<br>  \begin{bmatrix}<br>  w’_0\<br>  w’<em>1\<br>  w’<em>2\<br>  …\<br>  w’<em>j\<br>  …\<br>  b’<br>  \end{bmatrix}</em>{L&#x3D;L’}<br>  &#x3D; \ \ \ \ \ \<br>  \begin{bmatrix}<br>  w_0\<br>  w_1\<br>  w_2\<br>  …\<br>  w_j\<br>  …\<br>  b<br>  \end{bmatrix}</em>{L&#x3D;L_0}<br>  -\ \ \ \ \eta<br>  \begin{bmatrix}<br>  \frac{\partial L}{\partial w_0} \<br>  \frac{\partial L}{\partial w_1} \<br>  \frac{\partial L}{\partial w_2} \<br>  … \<br>  \frac{\partial L}{\partial w_j} \<br>  … \<br>  \frac{\partial L}{\partial b}<br>  \end{bmatrix}</em>{L&#x3D;L_0}<br>  $$<br>  当梯度稳定不变时，即$\nabla L$为0时，gradient descent便停止，此时如果采用的model是linear的，那么vector必然落于global minima处(凸函数)；如果采用的model是Non-linear的，vector可能会落于local minima处(此时需要采取其他办法获取最佳的function)</p>
<p>  假定我们已经通过各种方法到达了global minima的地方，此时的vector：$[w_0,w_1,w_2,…,w_j,…,b]^T$所确定的那个唯一的function就是在该λ下的最佳$f^*$，即loss最小</p>
</li>
<li><p>这里λ的最佳数值是需要通过我们不断调整来获取的，因此令λ等于0，10，100，1000，…不断使用gradient descent或其他算法得到最佳的parameters：$[w_0,w_1,w_2,…,w_j,…,b]^T$，并计算出这组参数确定的function——$f^*$对training data和testing data上的error值，直到找到那个使testing data的error最小的λ，(这里一开始λ&#x3D;0，就是没有使用regularization时的loss function)</p>
<p>  注：引入评价$f^*$的error机制，令error&#x3D;$\frac{1}{n}\sum\limits_{i&#x3D;1}^n|\widehat{y}^i-y^i|$，分别计算该$f^*$对training data和testing data(more important)的$error(f^*)$大小</p>
<blockquote>
<p>先设定λ-&gt;确定loss function-&gt;找到使loss最小的$[w_0,w_1,w_2,…,w_j,…,b]^T$-&gt;确定function-&gt;计算error-&gt;重新设定新的λ重复上述步骤-&gt;使testing data上的error最小的λ所对应的$[w_0,w_1,w_2,…,w_j,…,b]^T$所对应的function就是我们能够找到的最佳的function</p>
</blockquote>
</li>
</ul>
<h5 id="本章节总结："><a href="#本章节总结：" class="headerlink" title="本章节总结："></a>本章节总结：</h5><ul>
<li><p>Pokémon: Original CP and species almost decide the CP after evolution </p>
</li>
<li><p>There are probably other hidden factors</p>
</li>
<li><p>Gradient descent</p>
<ul>
<li>More theory and tips in the following lectures</li>
</ul>
</li>
<li><p>Overfitting and Regularization</p>
</li>
<li><p>We finally get average error &#x3D; 11.1 on the testing data</p>
</li>
<li><p>How about new data? Larger error? Lower error?(larger-&gt;need validation)</p>
</li>
<li><p>Next lecture: Where does the error come from?</p>
<ul>
<li>More theory about overfitting and regularization</li>
<li>The concept of validation(用来解决new data的error高于11.1的问题)</li>
</ul>
</li>
</ul>
<h4 id="附：Regularization-L1-L2-正则化解决-overfitting"><a href="#附：Regularization-L1-L2-正则化解决-overfitting" class="headerlink" title="附：Regularization( L1 L2 正则化解决 overfitting )"></a>附：Regularization( L1 L2 正则化解决 overfitting )</h4><blockquote>
<p>Regularization -&gt; redefine the loss function</p>
</blockquote>
<p>关于 overfitting 的问题，很大程度上是由于曲线为了更好地拟合 training data 的数据，而引入了更多的高次项，使得曲线更加“蜿蜒曲折”，反而导致了对 testing data 的误差更大</p>
<p>回过头来思考，我们之前衡量 model 中某个 function 的好坏所使用的 loss function，仅引入了真实值和预测值差值的平方和这一个衡量标准；我们想要避免 overfitting 过拟合的问题，就要使得高次项对曲线形状的影响尽可能小，因此我们要在 loss function 里引入高次项(非线性部分)的衡量标准，也就是将高次项的系数也加权放进 loss function 中，这样可以使得训练出来的 model 既满足预测值和真实值的误差小，又满足高次项的系数尽可能小而使曲线的形状比较稳定集中</p>
<p>以下图为例，如果 loss function 仅考虑了 $(\widehat{y}-y)^2$ 这一误差衡量标准，那么拟合出来的曲线就是红色虚线部分(过拟合)，而过拟合就是所谓的model对training data过度自信, 非常完美的拟合上了这些数据, 如果具备过拟合的能力, 那么这个方程就可能是一个比较复杂的非线性方程 , 正是因为这里的 $x^3$ 和 $x^2$ 使得这条虚线能够被弯来弯去, 所以整个模型就会特别努力地去学习作用在 $x^3$ 和 $x^2$ 上的c、d参数. <strong>但是在这个例子里，我们期望模型要学到的却是这条蓝色的曲线. 因为它能更有效地概括数据</strong>.而且只需要一个 $y&#x3D;a+bx$ 就能表达出数据的规律. </p>
<p>或者是说, 蓝色的线最开始时, 和红色线同样也有 c、d 两个参数, 可是最终学出来时, c 和 d 都学成了0, 虽然蓝色方程的误差要比红色大, 但是概括起数据来还是蓝色好</p>
<p><img src="https://img-blog.csdnimg.cn/1a6b73e72eb94ba49919f8bee1a54e0b.png#pic_center" alt="regularization"></p>
<p>这也是我们通常采用的方法，我们不可能一开始就否定高次项而直接只采用低次线性表达式的 model，因为有时候真实数据的确是符合高次项非线性曲线的分布的；而如果一开始直接采用高次非线性表达式的 model，就很有可能造成 overfitting，在曲线偏折的地方与真实数据的误差非常大。我们的目标应该是这样的：</p>
<p><strong>&#x3D;&#x3D;在无法确定真实数据分布的情况下，我们尽可能去改变 loss function 的评价标准&#x3D;&#x3D;</strong></p>
<ul>
<li><strong>我们的 model 的表达式要尽可能的复杂，包含尽可能多的参数和尽可能多的高次非线性项；</strong></li>
<li><strong>但是我们的 loss function 又有能力去控制这条曲线的参数和形状，使之不会出现 overfitting 过拟合的现象；</strong></li>
<li><strong>在真实数据满足高次非线性曲线分布的时候，loss function 控制训练出来的高次项的系数比较大，使得到的曲线比较弯折起伏；</strong></li>
<li><strong>在真实数据满足低次线性分布的时候，loss function 控制训练出来的高次项的系数比较小甚至等于 0，使得到的曲线接近 linear 分布</strong></li>
</ul>
<p>那我们如何保证能学出来这样的参数呢? 这就是 L1 L2 正规化出现的原因.</p>
<p>之前的loss function仅考虑了 $(\widehat{y}-y)^2$ 这一误差衡量标准，而<strong>L1 L2 正规化</strong>就是在这个loss function的后面多加了一个东西，即model中跟高次项系数有关的表达式；</p>
<ul>
<li><p>L1 正规化即加上 $λ\sum |w_j|$ 这一项，loss function 变成 $L&#x3D;\sum\limits_{i&#x3D;1}^n(\widehat{y}^i-y^i)^2+\lambda\sum\limits_{j}|w_j|$，即n个training data里的数据的真实值与预测值差值的平方和加上λ权重下的model表达式中所有项系数的绝对值之和</p>
</li>
<li><p>L2 正规化即加上 $\lambda\sum(w_j)^2$ 这一项，loss function变成$L&#x3D;\sum\limits_{i&#x3D;1}^n(\widehat{y}^i-y^i)^2+\lambda\sum\limits_{j}(w_j)^2$，即n个training data里的数据的真实值与预测值差值的平方和加上λ权重下的model表达式中所有项系数的平方和</p>
</li>
</ul>
<p>相对来说，L2 要更稳定一些，L1 的结果则不那么稳定，如果用 p 表示正规化程度，上面两式可总结如下：$L&#x3D;\sum\limits_{i&#x3D;1}^n(\widehat{y}^i-y^i)^2+\lambda\sum\limits_{j}(w_j)^p$<br><img src="https://img-blog.csdnimg.cn/fb53f6a767e74962b60f3bf8aed75203.png#pic_center" alt="L1-L2"></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="http://FraNny77.github.io">是甜豆腐脑</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://franny77.github.io/2022/07/13/1.Regression/">http://franny77.github.io/2022/07/13/1.Regression/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://FraNny77.github.io" target="_blank">是甜豆腐脑</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Regression/">Regression</a><a class="post-meta__tags" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></div><div class="post_share"><div class="social-share" data-image="http://imgoss.cnu.cc/2204/27/a4383d0f926b346eb1fe8ff128e5b703.jpg?x-oss-process=style/content" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2022/07/14/3%20.%20Gradient%20descent/"><img class="prev-cover" src="http://imgoss.cnu.cc/2204/27/a4383d0f926b346eb1fe8ff128e5b703.jpg?x-oss-process=style/content" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">3. Gradient descent</div></div></a></div><div class="next-post pull-right"><a href="/2022/07/13/2.%20Error%20origin/"><img class="next-cover" src="http://imgoss.cnu.cc/2204/27/a4383d0f926b346eb1fe8ff128e5b703.jpg?x-oss-process=style/content" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">2. Error origin</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/./img/tx.JPG" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">是甜豆腐脑</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">19</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">18</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/FraNny77.io"><i class="fab fa-github"></i><span>Gituhub</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/FraNny77" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:2556725169@qq.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="https://blog.csdn.net/FraNny13" target="_blank" title="Blog"><i class="fab fa-algolia"></i></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Regression%EF%BC%9ACase-Study"><span class="toc-text">Regression：Case Study</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%97%AE%E9%A2%98%E7%9A%84%E5%AF%BC%E5%85%A5%EF%BC%9A%E9%A2%84%E6%B5%8B%E5%AE%9D%E5%8F%AF%E6%A2%A6%E7%9A%84CP%E5%80%BC"><span class="toc-text">问题的导入：预测宝可梦的CP值</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%A1%AE%E5%AE%9ASenario%E3%80%81Task%E5%92%8CModel"><span class="toc-text">确定Senario、Task和Model</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Senario"><span class="toc-text">Senario</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Task"><span class="toc-text">Task</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Model"><span class="toc-text">Model</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%BE%E5%AE%9A%E5%85%B7%E4%BD%93%E5%8F%82%E6%95%B0"><span class="toc-text">设定具体参数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Regression%E7%9A%84%E5%85%B7%E4%BD%93%E8%BF%87%E7%A8%8B"><span class="toc-text">Regression的具体过程</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%9B%9E%E9%A1%BE%E4%B8%80%E4%B8%8Bmachine-Learning%E7%9A%84%E4%B8%89%E4%B8%AA%E6%AD%A5%E9%AA%A4%EF%BC%9A"><span class="toc-text">回顾一下machine Learning的三个步骤：</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Step1%EF%BC%9AModel-function-set"><span class="toc-text">Step1：Model (function set)</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#Linear-Model-%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B"><span class="toc-text">Linear Model 线性模型</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Step2%EF%BC%9AGoodness-of-Function-%E5%87%BD%E6%95%B0%E4%BC%98%E5%BA%A6"><span class="toc-text">Step2：Goodness of Function 函数优度</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#%E5%8F%82%E6%95%B0%E8%AF%B4%E6%98%8E"><span class="toc-text">参数说明</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#Loss-function-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-text">Loss function 损失函数</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#Loss-function%E5%8F%AF%E8%A7%86%E5%8C%96"><span class="toc-text">Loss function可视化</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Step3%EF%BC%9APick-the-Best-Function"><span class="toc-text">Step3：Pick the Best Function</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Gradient-Descent-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-text">Gradient Descent 梯度下降</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%8D%95%E4%B8%AA%E5%8F%82%E6%95%B0%E7%9A%84%E9%97%AE%E9%A2%98"><span class="toc-text">单个参数的问题</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%B8%A4%E4%B8%AA%E5%8F%82%E6%95%B0%E7%9A%84%E9%97%AE%E9%A2%98"><span class="toc-text">两个参数的问题</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Gradient-Descent%E7%9A%84%E7%BC%BA%E7%82%B9"><span class="toc-text">Gradient Descent的缺点</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9B%9E%E5%88%B0pokemon%E7%9A%84%E9%97%AE%E9%A2%98%E4%B8%8A%E6%9D%A5"><span class="toc-text">回到pokemon的问题上来</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%81%8F%E5%BE%AE%E5%88%86%E7%9A%84%E8%AE%A1%E7%AE%97"><span class="toc-text">偏微分的计算</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#How%E2%80%99s-the-results"><span class="toc-text">How’s the results?</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#How-can-we-do-better"><span class="toc-text">How can we do better?</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#%E8%80%83%E8%99%91-x-cp-2-%E7%9A%84model"><span class="toc-text">考虑$(x_{cp})^2$的model</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#%E8%80%83%E8%99%91-x-cp-3-%E7%9A%84model"><span class="toc-text">考虑$(x_{cp})^3$的model</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#%E8%80%83%E8%99%91-x-cp-4-%E7%9A%84model"><span class="toc-text">考虑$(x_{cp})^4$的model</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#%E8%80%83%E8%99%91-x-cp-5-%E7%9A%84model"><span class="toc-text">考虑$(x_{cp})^5$的model</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#5%E4%B8%AAmodel%E7%9A%84%E5%AF%B9%E6%AF%94"><span class="toc-text">5个model的对比</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%BF%9B%E4%B8%80%E6%AD%A5%E8%AE%A8%E8%AE%BA%E5%85%B6%E4%BB%96%E5%8F%82%E6%95%B0"><span class="toc-text">进一步讨论其他参数</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#%E7%89%A9%E7%A7%8D-x-s-%E7%9A%84%E5%BD%B1%E5%93%8D"><span class="toc-text">物种$x_s$的影响</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#Hp%E5%80%BC-x-hp-%E3%80%81height%E5%80%BC-x-h-%E3%80%81weight%E5%80%BC-x-w-%E7%9A%84%E5%BD%B1%E5%93%8D"><span class="toc-text">Hp值$x_{hp}$、height值$x_h$、weight值$x_w$的影响</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#regularization%E8%A7%A3%E5%86%B3-overfitting-L2%E6%AD%A3%E5%88%99%E5%8C%96%E8%A7%A3%E5%86%B3%E8%BF%87%E6%8B%9F%E5%90%88%E9%97%AE%E9%A2%98"><span class="toc-text">regularization解决 overfitting (L2正则化解决过拟合问题)</span></a></li></ol></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#conclusion%E6%80%BB%E7%BB%93"><span class="toc-text">conclusion总结</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%85%B3%E4%BA%8Epokemon%E7%9A%84cp%E5%80%BC%E9%A2%84%E6%B5%8B%E7%9A%84%E6%B5%81%E7%A8%8B%E6%80%BB%E7%BB%93%EF%BC%9A"><span class="toc-text">关于pokemon的cp值预测的流程总结：</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%9C%AC%E7%AB%A0%E8%8A%82%E6%80%BB%E7%BB%93%EF%BC%9A"><span class="toc-text">本章节总结：</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%99%84%EF%BC%9ARegularization-L1-L2-%E6%AD%A3%E5%88%99%E5%8C%96%E8%A7%A3%E5%86%B3-overfitting"><span class="toc-text">附：Regularization( L1 L2 正则化解决 overfitting )</span></a></li></ol></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2022/09/14/MybatisNotes/" title="MybatisNotes"><img src="http://imgoss.cnu.cc/2204/27/a4383d0f926b346eb1fe8ff128e5b703.jpg?x-oss-process=style/content" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="MybatisNotes"/></a><div class="content"><a class="title" href="/2022/09/14/MybatisNotes/" title="MybatisNotes">MybatisNotes</a><time datetime="2022-09-14T01:42:07.000Z" title="发表于 2022-09-14 09:42:07">2022-09-14</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/09/09/spiderNotes/" title="spyderNotes"><img src="http://cache.yisu.com/upload/admin/customer_case_img/2019-08-08/1565261709.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="spyderNotes"/></a><div class="content"><a class="title" href="/2022/09/09/spiderNotes/" title="spyderNotes">spyderNotes</a><time datetime="2022-09-09T12:34:08.000Z" title="发表于 2022-09-09 20:34:08">2022-09-09</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/07/14/3%20.%20Gradient%20descent/" title="3. Gradient descent"><img src="http://imgoss.cnu.cc/2204/27/a4383d0f926b346eb1fe8ff128e5b703.jpg?x-oss-process=style/content" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="3. Gradient descent"/></a><div class="content"><a class="title" href="/2022/07/14/3%20.%20Gradient%20descent/" title="3. Gradient descent">3. Gradient descent</a><time datetime="2022-07-14T06:15:56.000Z" title="发表于 2022-07-14 14:15:56">2022-07-14</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/07/13/1.Regression/" title="1. Regression"><img src="http://imgoss.cnu.cc/2204/27/a4383d0f926b346eb1fe8ff128e5b703.jpg?x-oss-process=style/content" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="1. Regression"/></a><div class="content"><a class="title" href="/2022/07/13/1.Regression/" title="1. Regression">1. Regression</a><time datetime="2022-07-13T07:21:44.000Z" title="发表于 2022-07-13 15:21:44">2022-07-13</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/07/13/2.%20Error%20origin/" title="2. Error origin"><img src="http://imgoss.cnu.cc/2204/27/a4383d0f926b346eb1fe8ff128e5b703.jpg?x-oss-process=style/content" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="2. Error origin"/></a><div class="content"><a class="title" href="/2022/07/13/2.%20Error%20origin/" title="2. Error origin">2. Error origin</a><time datetime="2022-07-13T06:45:40.000Z" title="发表于 2022-07-13 14:45:40">2022-07-13</time></div></div></div></div></div></div></main><footer id="footer" style="background: #000000"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2022 By 是甜豆腐脑</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">试着和曾经仰望的人并肩</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>